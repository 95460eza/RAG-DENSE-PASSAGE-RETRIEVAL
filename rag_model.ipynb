{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I- LIBRARY IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The PyTorch package is officially named torch on pip\n",
    "\n",
    "#! pip install transformers datasets torch \n",
    "#! conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia \n",
    "\n",
    "#! pip show torch\n",
    "#! conda install faiss-gpu \n",
    "\n",
    "#! conda install -c conda-forge datasets --update-deps\n",
    "#! pip install --upgrade datasets\n",
    "\n",
    "# !pip install pyarrow\n",
    "\n",
    "# beautifulsoup4 is the name to install via pip\n",
    "# !pip install beautifulsoup4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#psgs_w100.nq.exact, psgs_w100.nq.compressed, psgs_w100.nq.no_index, psgs_w100.multiset.exact, psgs_w100.multiset.compressed,\n",
    "#psgs_w100.multiset.no_index, psgs_w100.nq.exact.no_embeddings, psgs_w100.nq.compressed.no_embeddings, psgs_w100.nq.no_index.no_embeddings,\n",
    "#psgs_w100.multiset.exact.no_embeddings, psgs_w100.multiset.compressed.no_embeddings, psgs_w100.multiset.no_index.no_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 68 μs, sys: 0 ns, total: 68 μs\n",
      "Wall time: 103 μs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/mnt/d/ANALYTICS - Continuous Education/RAG'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import gzip\n",
    "import itertools\n",
    "import json\n",
    "import h5py\n",
    "#import tables # After conda install -c anaconda pytables\n",
    "import gc\n",
    "#import zipfile\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "import time\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# bs4 is the module of beautifulsoup4 that contains the BeautifulSoup class.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#import pyarrow as pa\n",
    "#import pyarrow.feather as feather\n",
    "#import pyarrow.parquet as pq\n",
    "#import pyarrow.ipc as ipc\n",
    "\n",
    "# Hugging Face's Transformers library primarily supports RAG models in PyTorch\n",
    "import torch, torchvision, torchaudio\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#  FAISS MEANS \"Facebook AI Similarity Search\"\n",
    "# TO CRREATE INDECES ON TEXT KNOWLED BASE\n",
    "import faiss\n",
    "\n",
    "# DPR MEANS Dense Passage Retrieval (ENCODE QUERY WITH KNOWLEDGE BASE)\n",
    "from transformers import DPRQuestionEncoderTokenizer, DPRQuestionEncoder, RagRetriever, RagTokenizer, RagTokenForGeneration\n",
    "from transformers import BartTokenizerFast, DPRQuestionEncoderTokenizerFast\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN \"CHROME\" BROWSER IN \"HEADLESS MODE\" PREVENTS IT FROM POING UP WHEN USING SELENIUM LIBRARY \n",
    "chrome_options = Options()\n",
    "# Run in headless mode\n",
    "chrome_options.add_argument(\"--headless\")  \n",
    "# Disable GPU hardware acceleration\n",
    "#chrome_options.add_argument(\"--disable-gpu\") \n",
    "# Bypass OS security model, necessary for headless mode on some systems\n",
    "chrome_options.add_argument(\"--no-sandbox\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset using a valid configuration\n",
    "#dataset = load_dataset(\"wiki_dpr\", \"psgs_w100.nq.exact\", download_config={\"timeout\": 10000})\n",
    "#dataset = load_dataset(\"wiki_dpr\", \"psgs_w100.nq.compressed\")\n",
    "#dataset = load_dataset(\"wiki_dpr\", \"psgs_w100\")\n",
    "#dataset = load_dataset(\"wiki_dpr\", \"psgs_w100.multiset.exact\")\n",
    "\n",
    "# Useful to manipulate the data loaded from the PARQUET format before passing it to the retriever. \n",
    "# Otherwise, you might not further use the dataset object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II- OPTION 1: CASE WHEN DATA IS A \"PARQUET\" FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-A. LOAD AND SAVE  TO DISK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.makedirs(os.path.join(os.getcwd(), 'train'), exist_ok=True)\n",
    "\n",
    "#print(os.path.isdir('/mnt/d/ANALYTICS - Continuous Education/RAG/train'))\n",
    "#If this returns False, it means train exists as a file rather than a directory.\n",
    "\n",
    "#print(os.listdir('/mnt/d/ANALYTICS - Continuous Education/RAG/train'))\n",
    "\n",
    "# designed to delete files NOT DIRECTORIES\n",
    "#os.remove(os.path.join(os.getcwd(), 'train'))\n",
    "\n",
    "#import shutil\n",
    "# IF DIRECTORY HAS CONTENT\n",
    "#shutil.rmtree('/mnt/d/ANALYTICS - Continuous Education/RAG/train')\n",
    "\n",
    "# IF DIRECTORY DOES NOT HAVE CONTENT\n",
    "#os.rmdir('/mnt/d/ANALYTICS - Continuous Education/RAG/train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train-00014-of-00157.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:2\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/versatile_gpu_env/lib/python3.11/site-packages/pandas/io/parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/versatile_gpu_env/lib/python3.11/site-packages/pandas/io/parquet.py:267\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    265\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    275\u001b[0m         path_or_handle,\n\u001b[1;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    280\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/versatile_gpu_env/lib/python3.11/site-packages/pandas/io/parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    130\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/versatile_gpu_env/lib/python3.11/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train-00014-of-00157.parquet'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# PREVIEW the Parquet file into a DataFrame\n",
    "hugginface_knowledgebase_sample_df = pd.read_parquet(\"train-00014-of-00157.parquet\")\n",
    "print(hugginface_knowledgebase_sample_df.shape)\n",
    "hugginface_knowledgebase_sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Unable to find '/mnt/d/ANALYTICS - Continuous Education/RAG/train-00014-of-00157.parquet' with any supported extension ['.csv', '.tsv', '.json', '.jsonl', '.ndjson', '.parquet', '.geoparquet', '.gpq', '.arrow', '.txt', '.tar', '.blp', '.bmp', '.dib', '.bufr', '.cur', '.pcx', '.dcx', '.dds', '.ps', '.eps', '.fit', '.fits', '.fli', '.flc', '.ftc', '.ftu', '.gbr', '.gif', '.grib', '.h5', '.hdf', '.png', '.apng', '.jp2', '.j2k', '.jpc', '.jpf', '.jpx', '.j2c', '.icns', '.ico', '.im', '.iim', '.tif', '.tiff', '.jfif', '.jpe', '.jpg', '.jpeg', '.mpg', '.mpeg', '.msp', '.pcd', '.pxr', '.pbm', '.pgm', '.ppm', '.pnm', '.psd', '.bw', '.rgb', '.rgba', '.sgi', '.ras', '.tga', '.icb', '.vda', '.vst', '.webp', '.wmf', '.emf', '.xbm', '.xpm', '.BLP', '.BMP', '.DIB', '.BUFR', '.CUR', '.PCX', '.DCX', '.DDS', '.PS', '.EPS', '.FIT', '.FITS', '.FLI', '.FLC', '.FTC', '.FTU', '.GBR', '.GIF', '.GRIB', '.H5', '.HDF', '.PNG', '.APNG', '.JP2', '.J2K', '.JPC', '.JPF', '.JPX', '.J2C', '.ICNS', '.ICO', '.IM', '.IIM', '.TIF', '.TIFF', '.JFIF', '.JPE', '.JPG', '.JPEG', '.MPG', '.MPEG', '.MSP', '.PCD', '.PXR', '.PBM', '.PGM', '.PPM', '.PNM', '.PSD', '.BW', '.RGB', '.RGBA', '.SGI', '.RAS', '.TGA', '.ICB', '.VDA', '.VST', '.WEBP', '.WMF', '.EMF', '.XBM', '.XPM', '.aiff', '.au', '.avr', '.caf', '.flac', '.htk', '.svx', '.mat4', '.mat5', '.mpc2k', '.ogg', '.paf', '.pvf', '.raw', '.rf64', '.sd2', '.sds', '.ircam', '.voc', '.w64', '.wav', '.nist', '.wavex', '.wve', '.xi', '.mp3', '.opus', '.AIFF', '.AU', '.AVR', '.CAF', '.FLAC', '.HTK', '.SVX', '.MAT4', '.MAT5', '.MPC2K', '.OGG', '.PAF', '.PVF', '.RAW', '.RF64', '.SD2', '.SDS', '.IRCAM', '.VOC', '.W64', '.WAV', '.NIST', '.WAVEX', '.WVE', '.XI', '.MP3', '.OPUS', '.zip']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# CREATE JSON FILE WITH THE ACTUAL DATA AND ITS METADATA\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dict_containing_also_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetcwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain-00014-of-00157.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_metadata type is: DatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,dict_containing_also_data)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Access the TRAINING DATASET split\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# This should give you the Dataset object\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/versatile_gpu_env/lib/python3.11/site-packages/datasets/load.py:2074\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2069\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2070\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2071\u001b[0m )\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2074\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2087\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2088\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2089\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2091\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/miniconda3/envs/versatile_gpu_env/lib/python3.11/site-packages/datasets/load.py:1795\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1793\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1794\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1795\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1804\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1805\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1807\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1808\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/versatile_gpu_env/lib/python3.11/site-packages/datasets/load.py:1573\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LocalDatasetModuleFactoryWithScript(\n\u001b[1;32m   1565\u001b[0m         combined_path,\n\u001b[1;32m   1566\u001b[0m         download_mode\u001b[38;5;241m=\u001b[39mdownload_mode,\n\u001b[1;32m   1567\u001b[0m         dynamic_modules_path\u001b[38;5;241m=\u001b[39mdynamic_modules_path,\n\u001b[1;32m   1568\u001b[0m         trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[1;32m   1569\u001b[0m     )\u001b[38;5;241m.\u001b[39mget_module()\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(path):\n\u001b[1;32m   1571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLocalDatasetModuleFactoryWithoutScript\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\n\u001b[0;32m-> 1573\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[38;5;66;03m# Try remotely\u001b[39;00m\n\u001b[1;32m   1575\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_relative_path(path) \u001b[38;5;129;01mand\u001b[39;00m path\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/versatile_gpu_env/lib/python3.11/site-packages/datasets/load.py:828\u001b[0m, in \u001b[0;36mLocalDatasetModuleFactoryWithoutScript.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     patterns \u001b[38;5;241m=\u001b[39m get_data_patterns(base_path)\n\u001b[0;32m--> 828\u001b[0m data_files \u001b[38;5;241m=\u001b[39m \u001b[43mDataFilesDict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALL_ALLOWED_EXTENSIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m module_name, default_builder_kwargs \u001b[38;5;241m=\u001b[39m infer_module_for_data_files(\n\u001b[1;32m    834\u001b[0m     data_files\u001b[38;5;241m=\u001b[39mdata_files,\n\u001b[1;32m    835\u001b[0m     path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath,\n\u001b[1;32m    836\u001b[0m )\n\u001b[1;32m    837\u001b[0m data_files \u001b[38;5;241m=\u001b[39m data_files\u001b[38;5;241m.\u001b[39mfilter_extensions(_MODULE_TO_EXTENSIONS[module_name])\n",
      "File \u001b[0;32m~/miniconda3/envs/versatile_gpu_env/lib/python3.11/site-packages/datasets/data_files.py:721\u001b[0m, in \u001b[0;36mDataFilesDict.from_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    716\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m()\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, patterns_for_key \u001b[38;5;129;01min\u001b[39;00m patterns\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    718\u001b[0m     out[key] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    719\u001b[0m         patterns_for_key\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(patterns_for_key, DataFilesList)\n\u001b[0;32m--> 721\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mDataFilesList\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_patterns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatterns_for_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    727\u001b[0m     )\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/versatile_gpu_env/lib/python3.11/site-packages/datasets/data_files.py:624\u001b[0m, in \u001b[0;36mDataFilesList.from_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m patterns:\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         data_files\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m--> 624\u001b[0m             \u001b[43mresolve_pattern\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m                \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m         )\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_magic(pattern):\n",
      "File \u001b[0;32m~/miniconda3/envs/versatile_gpu_env/lib/python3.11/site-packages/datasets/data_files.py:411\u001b[0m, in \u001b[0;36mresolve_pattern\u001b[0;34m(pattern, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m allowed_extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m         error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m with any supported extension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(allowed_extensions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 411\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(error_msg)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Unable to find '/mnt/d/ANALYTICS - Continuous Education/RAG/train-00014-of-00157.parquet' with any supported extension ['.csv', '.tsv', '.json', '.jsonl', '.ndjson', '.parquet', '.geoparquet', '.gpq', '.arrow', '.txt', '.tar', '.blp', '.bmp', '.dib', '.bufr', '.cur', '.pcx', '.dcx', '.dds', '.ps', '.eps', '.fit', '.fits', '.fli', '.flc', '.ftc', '.ftu', '.gbr', '.gif', '.grib', '.h5', '.hdf', '.png', '.apng', '.jp2', '.j2k', '.jpc', '.jpf', '.jpx', '.j2c', '.icns', '.ico', '.im', '.iim', '.tif', '.tiff', '.jfif', '.jpe', '.jpg', '.jpeg', '.mpg', '.mpeg', '.msp', '.pcd', '.pxr', '.pbm', '.pgm', '.ppm', '.pnm', '.psd', '.bw', '.rgb', '.rgba', '.sgi', '.ras', '.tga', '.icb', '.vda', '.vst', '.webp', '.wmf', '.emf', '.xbm', '.xpm', '.BLP', '.BMP', '.DIB', '.BUFR', '.CUR', '.PCX', '.DCX', '.DDS', '.PS', '.EPS', '.FIT', '.FITS', '.FLI', '.FLC', '.FTC', '.FTU', '.GBR', '.GIF', '.GRIB', '.H5', '.HDF', '.PNG', '.APNG', '.JP2', '.J2K', '.JPC', '.JPF', '.JPX', '.J2C', '.ICNS', '.ICO', '.IM', '.IIM', '.TIF', '.TIFF', '.JFIF', '.JPE', '.JPG', '.JPEG', '.MPG', '.MPEG', '.MSP', '.PCD', '.PXR', '.PBM', '.PGM', '.PPM', '.PNM', '.PSD', '.BW', '.RGB', '.RGBA', '.SGI', '.RAS', '.TGA', '.ICB', '.VDA', '.VST', '.WEBP', '.WMF', '.EMF', '.XBM', '.XPM', '.aiff', '.au', '.avr', '.caf', '.flac', '.htk', '.svx', '.mat4', '.mat5', '.mpc2k', '.ogg', '.paf', '.pvf', '.raw', '.rf64', '.sd2', '.sds', '.ircam', '.voc', '.w64', '.wav', '.nist', '.wavex', '.wve', '.xi', '.mp3', '.opus', '.AIFF', '.AU', '.AVR', '.CAF', '.FLAC', '.HTK', '.SVX', '.MAT4', '.MAT5', '.MPC2K', '.OGG', '.PAF', '.PVF', '.RAW', '.RF64', '.SD2', '.SDS', '.IRCAM', '.VOC', '.W64', '.WAV', '.NIST', '.WAVEX', '.WVE', '.XI', '.MP3', '.OPUS', '.zip']"
     ]
    }
   ],
   "source": [
    "# CREATE JSON FILE WITH THE ACTUAL DATA AND ITS METADATA\n",
    "dict_containing_also_data = load_dataset(os.getcwd(), data_files=\"train-00014-of-00157.parquet\")\n",
    "print(\"dataset_metadata type is: DatasetDict\", \"\\n\",dict_containing_also_data)\n",
    "\n",
    "\n",
    "# Access the TRAINING DATASET split\n",
    "# This should give you the Dataset object\n",
    "train_dataset = dict_containing_also_data['train']  \n",
    "print(\"train_dataset type is: Dataset\", \"\\n\",train_dataset)\n",
    "\n",
    "\n",
    "# Save the TRAINING DATASET INTO A FOLDER \n",
    "path_to_folder_with_saved_train_dataset = os.path.join(os.getcwd(), \"train\")\n",
    "print(path_to_folder_with_saved_train_dataset)\n",
    "train_dataset.save_to_disk(path_to_folder_with_saved_train_dataset) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-B. CREATE INDECES OF KNOWLEDGE BASE DOCUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fcacdf15cd0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/koffi/miniconda3/envs/versatile_gpu_env/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/ANALYTICS - Continuous Education/RAG/embeddings_index.index\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CREATE THE CUSTOM INDEX TO BE ABLE TO USE THE \"SAVED\" DATASET AS THE KNOWLEDGE BASE\n",
    "\n",
    "# GRAB 'embeddings' COLUMN is correctly defined\n",
    "embeddings = train_dataset['embeddings']\n",
    "\n",
    "# Convert embeddings COLUMN to a numpy array\n",
    "embeddings_array = np.array(embeddings).astype('float32')\n",
    "\n",
    "# Instantiate a FAISS index Using L2 distance (Euclidean distance)\n",
    "index = faiss.IndexFlatL2(embeddings_array.shape[1])  \n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(embeddings_array)\n",
    "\n",
    "# Save the index to disk for later use\n",
    "index_path = os.path.join(os.getcwd(), \"embeddings_index.index\")\n",
    "print(index_path)\n",
    "faiss.write_index(index, index_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-C. RAG MODEL FOR QUESTIONS ANSWERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# INSTANTIATE A TOKENIZER\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-sequence-nq\")\n",
    "\n",
    "# Create the retriever\n",
    "# USE index_name=\"custom\" WHEN YOU WANT TO PROVIDE YOUR KNOWLEDGE DATA BASE AND BUILD LOCALLY ITS CUSTOM INDEX\n",
    "\n",
    "retriever = RagRetriever.from_pretrained(\"facebook/rag-sequence-nq\",index_name=\"custom\", output_retrieved=False,\n",
    "                                                                    tokenizer = tokenizer,\n",
    "                                                                    passages_path=path_to_folder_with_saved_train_dataset, \n",
    "                                                                    index_path=index_path)\n",
    "\n",
    "\n",
    "# CREATE THE GENERATOR\n",
    "model = RagTokenForGeneration.from_pretrained(\"facebook/rag-sequence-nq\", retriever=retriever)\n",
    "\n",
    "\n",
    "# Define the query/question\n",
    "#question = \"What is the capital of France ?\"\n",
    "question = \"What is the capital of Italy ?\"\n",
    "#question = \"What is the capital city of Canada ?\"\n",
    "#question = \"What is the capital of Germany ?\"\n",
    "# Tokenize the input question\n",
    "input_ids = tokenizer(question, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Get hidden states for the question USING \"question_encoder\" \n",
    "# ALSO Disable gradient calculation for inference\n",
    "with torch.no_grad():  \n",
    "    question_hidden_states = model.question_encoder(input_ids)[0]  \n",
    "\n",
    "# Convert TENSOR to NumPy array BY MOVING IT to CPU and convert to NumPy array (cpu FRAMEWORK)\n",
    "question_hidden_states_np = question_hidden_states.cpu().numpy()\n",
    "\n",
    "#  Use the retriever to retrieve relevant documents\n",
    "docs_dict = retriever(input_ids, question_hidden_states=question_hidden_states_np, return_tensors=\"pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rome\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming you have retrieved embeddings for the documents\n",
    "retrieved_doc_embeds = docs_dict['retrieved_doc_embeds']  # Shape (n_docs, embed_dim)\n",
    "\n",
    "# To match the dimensions of (batch_size, 1, embed_dim) for dot product\n",
    "#question_hidden_states_unsq = question_hidden_states.unsqueeze(1)  # Shape: (batch_size, 1, embed_dim)\n",
    "# Compute dot product similarity\n",
    "#doc_scores = torch.bmm(question_hidden_states_unsq, retrieved_doc_embeds.transpose(1, 2)).squeeze(1)  # Shape: (batch_size, n_docs)\n",
    "\n",
    "\n",
    "# Euclidean distance computation between `question_hidden_states` and `retrieved_doc_embeds`\n",
    "doc_scores = torch.cdist(question_hidden_states, retrieved_doc_embeds, p=2)  # Shape: (batch_size, n_docs)\n",
    "\n",
    "# Calculate doc scores (e.g., cosine similarity) between the question and retrieved embeddings\n",
    "#doc_scores = F.cosine_similarity(question_hidden_states, retrieved_doc_embeds, dim=2)\n",
    "\n",
    "n_docs =docs_dict['context_input_ids'].shape[0]\n",
    "batch_size = input_ids.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "context_input_ids = docs_dict['context_input_ids'].reshape(batch_size * n_docs, -1)\n",
    "#print(context_input_ids.shape)\n",
    "\n",
    "if 'context_attention_mask' not in docs_dict:\n",
    "    docs_dict['context_attention_mask'] = torch.ones_like(context_input_ids)\n",
    "#print(docs_dict['context_attention_mask'].shape)\n",
    "\n",
    "doc_scores = doc_scores.squeeze(1).expand(batch_size, n_docs) \n",
    "#print(input_ids.shape)  # e.g., torch.Size([1, seq_length])\n",
    "#print(context_input_ids.shape)  # e.g., torch.Size([5, 300])\n",
    "#print(doc_scores.shape)  # e.g., torch.Size([1, 5])\n",
    "\n",
    "\n",
    "\n",
    "# Generate the answer conditioned on the retrieved documents\n",
    "outputs = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    #context_input_ids=docs_dict['context_input_ids'],\n",
    "    context_input_ids=docs_dict['context_input_ids'].reshape(batch_size * n_docs, -1),\n",
    "    context_attention_mask=docs_dict['context_attention_mask'],  # Use default or handle if missing\n",
    "    doc_scores = doc_scores,  # Add batch dimension if necessary\n",
    "    n_docs=n_docs\n",
    ")\n",
    "\n",
    "\n",
    "# Decode and print the generated answer\n",
    "generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III-  __**OPTION 2**__ : __\"NO WEB SCRAPPING\"__ TO CREATE TEXT PASSAGES (AKA KNOWLEDGE BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III-A.1. CREATE AS \"TAB-SAPARATED-VALUE (TSV)\" FILE THE TEXT PASSAGES (AKA KNOWLEDGE BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATOR FUNCTION FOR \"CHUNK-LOADING\" DATA FROM A FILE (HERE ZIPPED)\n",
    "# USING \"itertools.islice\"\n",
    "\n",
    "def chunks_generator_for_gz_zipped_file(data_path, chunk_size):\n",
    "\n",
    "    with gzip.open(data_path, 'rt', encoding='utf-8') as f:\n",
    "\n",
    "        # READ ONLY A NUMBER OF ROWS FROM THE SAVED-TO-DISK DATASET THAT IS EQUAL TO \"chunk_size\"\n",
    "        #chunk_count = 0 \n",
    "        for chunk in iter(lambda: list(itertools.islice(f, chunk_size)), []): \n",
    "             \n",
    "            # LIMIT THE NUMBER OF CHUNKS TO READ FROM THE SAVED-TO-DISK DATASET\n",
    "            #if chunk_count >= 10:  \n",
    "            #    break            \n",
    "            \n",
    "            # YIELD ONLY EXIST WITHIN A FUNCTION\n",
    "            yield chunk\n",
    "            #chunk_count += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO EXTRACT \"TEXT PASAGES\" FROM AN  HTML STRING ALREADY EXISTING IN THE DATASET\n",
    "\n",
    "def extract_text_from_wikipedia_html(document_text, row_number, modulo_for_print):    \n",
    "    \n",
    "    paragraphs_as_content_div_find_all_p_is_empty={}\n",
    "    \n",
    "    #print(document_text)\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(document_text, 'html.parser')\n",
    "    content_div = soup.find_all('p')     \n",
    "    #print(\"content_div\", content_div, type(content_div))\n",
    "    \n",
    "    if content_div is None:                  \n",
    "          print(\"Content section 'mw-content-text' is None for Row number:\", row_number)          \n",
    "          text_content = \" \"\n",
    "          return text_content        \n",
    "\n",
    "    cleaned_paragraphs = []\n",
    "\n",
    "     # Loop through the content and extract text from paragraphs\n",
    "    for element in content_div:\n",
    "          #text = element.get_text(strip=True)\n",
    "          text = element.get_text(strip=False)                          \n",
    "          #print(text)\n",
    "          # Only append non-empty text\n",
    "          if len(text.strip()) > 0:                 \n",
    "               cleaned_paragraphs.append(text)    \n",
    "\n",
    "     # Combine all paragraphs into a single string\n",
    "    text_content = ' '.join(cleaned_paragraphs)\n",
    "     # Replace newline characters with a space\n",
    "    text_content = text_content.replace(\"\\n\", \" \").strip()\n",
    "     #text_content = text_content.replace(\"\\t\", \" \")\n",
    "\n",
    "    if not len(text_content.strip()) >0:\n",
    "                              \n",
    "          paragraphs_as_content_div_find_all_p_is_empty[row_number]=( content_div)\n",
    "          #print(row_number,\":\", url, \"\\n\", content_div)\n",
    "          print(\"Row number:\",row_number,\"Paragraphs from find('p') is EMPTY:\")\n",
    "\n",
    "     \n",
    "    if row_number % modulo_for_print == 0:\n",
    "     print(f\"Processed row: {row_number}\") \n",
    "                   \n",
    "    \n",
    "    return text_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO \"EFFICIENTLY\" CREATE IN \"BATCHES\" THE TAB-SEPARATED (TSV) FILE\n",
    "# CONTAINING THE TEXT PASSAGES (AKA KNOWLEDGE BASE OR CONTEXT)\n",
    "\n",
    "def batches_append_to_tsv(data_generator, output_path, batch_size, modulo_for_print):  \n",
    "\n",
    "    batch = []       \n",
    "    row_num = []                \n",
    "\n",
    "    # chunk_created_passages_list =[extract_text_from_wikipedia_html(document_text, \n",
    "    #                                                 text_position_in_chunk+chunk_size*chunk_number, \n",
    "    #                                                 modulo_for_print)  for chunk_number, chunk in enumerate(data_generator)\n",
    "    #                                                                    # ONLY WHEN THE CONDTION BELOW IS MET WILL THE NEXT \"FOR\" BE ENTERED AND RETURN THE FUNCTION\n",
    "    #                                                                    #if chunk_number <= 2  # This acts like a break condition                                                         \n",
    "    #                                                                    for text_position_in_chunk, document_text in enumerate(chunk)                                               \n",
    "    #                                                                                                                                  ]\n",
    "\n",
    "    for chunk_number, chunk in enumerate(data_generator):\n",
    "          \n",
    "          for text_position_in_chunk, document_text in enumerate(chunk):\n",
    "                \n",
    "                chunk_created_passages_list =[extract_text_from_wikipedia_html(document_text, \n",
    "                                                                text_position_in_chunk+chunk_size*chunk_number, \n",
    "                                                                modulo_for_print)\n",
    "                                                ]\n",
    "                \n",
    "                batch = batch + chunk_created_passages_list  \n",
    "                row_num.append(text_position_in_chunk+chunk_size*chunk_number)\n",
    "\n",
    "                     \n",
    "          \n",
    "          if len(batch) > batch_size:\n",
    "\n",
    "            #print(row_num)\n",
    "            #print(len(batch))\n",
    "            \n",
    "            data_created = zip(row_num,batch)\n",
    "            # Filter out tuples where the second element is missing or empty\n",
    "            data_created = [tup for tup in data_created if tup[1]]            \n",
    "\n",
    "            # Convert the list to a DataFrame             \n",
    "            df_batch = pd.DataFrame(data_created) \n",
    "            #df.to_csv(output_path, sep='\\t', mode='a', header=not pd.io.common.file_exists(output_path), index=False)\n",
    "            df_batch.to_csv(output_path, sep='\\t', mode='a', header=None, index=False)\n",
    "            \n",
    "            # Clear the batch and row numbers after writing to the file\n",
    "            row_num.clear()\n",
    "            batch.clear()\n",
    "            chunk_created_passages_list.clear()\n",
    "            del data_created\n",
    "            del df_batch\n",
    "            gc.collect()     \n",
    "          \n",
    "          \n",
    "    if len(batch) > 0:\n",
    "         \n",
    "         #print(row_num)\n",
    "         #print(len(batch))\n",
    "\n",
    "         data_created = zip(row_num,batch)\n",
    "         # Filter out tuples where the second element is missing or empty\n",
    "         data_created = [tup for tup in data_created if tup[1]]            \n",
    "\n",
    "         # Convert the list to a DataFrame             \n",
    "         df_batch = pd.DataFrame(data_created) \n",
    "         #df.to_csv(output_path, sep='\\t', mode='a', header=not pd.io.common.file_exists(output_path), index=False)\n",
    "         df_batch.to_csv(output_path, sep='\\t', mode='a', header=None, index=False)\n",
    "      \n",
    "         # Clear the batch and row numbers after writing to the file\n",
    "         row_num.clear()\n",
    "         batch.clear()\n",
    "         chunk_created_passages_list.clear()\n",
    "         del data_created\n",
    "         del df_batch\n",
    "         gc.collect()     \n",
    "\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row: 0\n",
      "Row number: 3967 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 5000\n",
      "Row number: 6853 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 7466 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 8986 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 9368 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 9934 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 10000\n",
      "Row number: 12938 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 13860 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 15000\n",
      "Row number: 15297 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 15430 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 16590 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 19403 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 19953 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 20000\n",
      "Row number: 21887 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 22796 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 25000\n",
      "Row number: 25166 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 29337 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 29748 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 30000\n",
      "Row number: 30101 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 34588 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 34838 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 35000\n",
      "Row number: 36741 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 40000\n",
      "Row number: 42297 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 42317 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 43205 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 44807 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 45000\n",
      "Row number: 45886 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 50000\n",
      "Row number: 51784 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 52835 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 54830 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 55000\n",
      "Row number: 57454 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 60000\n",
      "Row number: 62097 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 63203 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 64511 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 64515 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 65000\n",
      "Row number: 65477 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 67177 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 67384 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 70000\n",
      "Row number: 70949 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 71695 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 72441 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 73654 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 75000\n",
      "Row number: 75360 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 76347 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 76377 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 76666 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 78552 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 80000\n",
      "Row number: 80337 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 80472 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 83484 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 84889 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 85000\n",
      "Row number: 86521 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 87508 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 90000\n",
      "Row number: 92716 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 93132 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 93596 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 94571 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 95000\n",
      "Processed row: 100000\n",
      "Row number: 102380 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 103046 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 103684 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 104196 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 104343 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 104605 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 105000\n",
      "Row number: 106809 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 108405 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 109355 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 110000\n",
      "Row number: 113043 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 113318 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 115000\n",
      "Row number: 115458 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 118908 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 119580 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 119866 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 120000\n",
      "Row number: 120581 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 125000\n",
      "Row number: 125038 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 126319 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 130000\n",
      "Row number: 131489 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 135000\n",
      "Row number: 137641 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 140000\n",
      "Row number: 140627 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 141127 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 145000\n",
      "Processed row: 150000\n",
      "Row number: 151904 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 155000\n",
      "Row number: 155385 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 157678 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 157855 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 160000\n",
      "Row number: 164881 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 165000\n",
      "Processed row: 170000\n",
      "Row number: 170452 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 171357 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 171536 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 175000\n",
      "Row number: 177309 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 177894 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 178676 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 179661 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 180000\n",
      "Row number: 180047 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 183584 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 185000\n",
      "Row number: 185573 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 185907 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 187138 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 187755 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 189345 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 189775 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 190000\n",
      "Row number: 192349 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 192749 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 195000\n",
      "Row number: 196619 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 199734 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 200000\n",
      "Row number: 200068 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 200675 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 202040 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 204979 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 205000\n",
      "Row number: 205685 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 207267 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 210000\n",
      "Row number: 210534 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 212245 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 215000\n",
      "Row number: 215958 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 219277 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 220000\n",
      "Row number: 221842 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 223081 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 225000\n",
      "Processed row: 230000\n",
      "Row number: 231620 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 233301 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 235000\n",
      "Row number: 238989 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 240000\n",
      "Row number: 240084 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 240105 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 240331 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 240358 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 241241 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 242524 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 245000\n",
      "Row number: 249665 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 250000\n",
      "Row number: 252637 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 253946 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 255000\n",
      "Row number: 255055 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 255175 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 256961 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 259207 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 260000\n",
      "Processed row: 265000\n",
      "Row number: 266022 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 268445 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 269075 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 270000\n",
      "Row number: 270233 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 271180 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 271348 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 272723 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 274061 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 274383 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 275000\n",
      "Row number: 275981 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 280000\n",
      "Row number: 280718 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 281549 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 281583 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 282404 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 282827 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 284496 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 285000\n",
      "Row number: 287855 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 289546 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 290000\n",
      "Row number: 294546 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 294646 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 294666 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 295000\n",
      "Row number: 295334 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 297986 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 299413 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 300000\n",
      "Row number: 300435 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 304741 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 305000\n",
      "CPU times: user 1h 24min 12s, sys: 6.93 s, total: 1h 24min 19s\n",
      "Wall time: 1h 30min 48s\n"
     ]
    }
   ],
   "source": [
    "# RUN FUNCTION ABOVE\n",
    "%%time\n",
    "\n",
    "#file_path_to_google_nq_dataset = os.path.join(os.getcwd(), \"data\",'v1.0-simplified_simplified-nq-train.jsonl.gz')\n",
    "file_path_to_google_nq_dataset = os.path.join(os.getcwd(), \"data\",'google_nq_simplified_dataset_v1.gz') \n",
    "chunk_size =300\n",
    "tsv_file_path1 = os.path.join(os.getcwd(), \"data\",\"text_passages.tsv\")\n",
    "batch_size=4000\n",
    "modulo_for_print=5000\n",
    "\n",
    "batches_append_to_tsv( data_generator = chunks_generator_for_gz_zipped_file(data_path = file_path_to_google_nq_dataset, chunk_size = chunk_size),\n",
    "                       output_path=tsv_file_path1,\n",
    "                       batch_size=batch_size,\n",
    "                       modulo_for_print = modulo_for_print\n",
    "                       \n",
    "                       )\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.6 s, sys: 1.99 s, total: 27.6 s\n",
      "Wall time: 1min 53s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "307216"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tsv_file_path1 = os.path.join(os.getcwd(), \"data\",\"text_passages.tsv\")\n",
    "data_size = 0\n",
    "#for chunk_df in pd.read_csv(file_path, skiprows=range(1, start_row + 1), chunksize=chunk_size):\n",
    "for chunk in pd.read_csv(tsv_file_path1, sep='\\t', names=['id', 'text'], chunksize=100):  \n",
    "    data_size += len(chunk)\n",
    "\n",
    "\n",
    "data_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test= pd.read_csv(tsv_file_path1, sep='\\t', names=['id', 'text'])\n",
    "# print(test.shape)\n",
    "# test\n",
    "# test=None\n",
    "# del test\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III-A.2. EXPLORE BY LOADING CHUNKS FROM SAVED TEXT PASSAGES (IF TOO BIG FOR MEMORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATOR FUNCTION FOR \"CHUNK-LOADING\" DATA FROM A FILE (HERE TSV)\n",
    "\n",
    "def chunks_generator_for_TSV_file_limited(data_path, chunk_size):\n",
    "\n",
    "    chunks_read= []\n",
    "    chunk_count = 0 \n",
    "\n",
    "    # READ ONLY A NUMBER OF ROWS FROM THE SAVED-TO-DISK DATASET THAT IS EQUAL TO \"chunk_size\"\n",
    "    # for chunk_df in pd.read_csv(file_path, skiprows=range(1, start_row + 1), chunksize=chunk_size):\n",
    "    for chunk in pd.read_csv(data_path, sep='\\t', names=['id', 'text'], chunksize=chunk_size):\n",
    "\n",
    "        chunks_read.append(chunk) \n",
    "                  \n",
    "        # LIMIT THE NUMBER OF CHUNKS TO READ FROM THE SAVED-TO-DISK DATASET\n",
    "        if chunk_count >= 50:  \n",
    "                break            \n",
    "        # YIELD ONLY EXIST WITHIN A FUNCTION\n",
    "        yield chunk\n",
    "        chunk_count += 1 \n",
    "    \n",
    "    return chunks_read\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                               text\n",
      "0   0  Email marketing is the act of sending a commer...\n",
      "1   1  Tracy McConnell , better known as `` The Mothe...\n",
      "2   2  Human fertilization is the union of a human eg...\n",
      "3   3  The following is a list of the top National Fo...\n",
      "4   4  The Roanoke Colony ( / \\u02c8ro\\u028a\\u0259\\u0...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_read_df = pd.concat(chunks_generator_for_TSV_file_limited(tsv_file_path1, chunk_size=1), ignore_index=True)\n",
    "print(chunks_read_df.head())\n",
    "\n",
    "# RELEASE MEMORY\n",
    "chunks_read_df = None\n",
    "del chunks_read_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III-A.3. CREATE THE \"INPUTS\" TENSORS AS HD5 FILE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"\\n TRANSFERT of inputs TO Gpu:\")\n",
    "# inputs_tensor = {key: value.to(device) for key, value in inputs.items()}\n",
    "# print(type(inputs_tensor), len(inputs_tensor))\n",
    "# inputs_tensor.keys()\n",
    "# path_to_inputs_tensor =  os.path.join(os.getcwd(), \"data\",\"inputs_tensor.pth\")\n",
    "\n",
    "# TO SAVE AS TENSOR WITH torch.save() USE EXTENSION \".pt\" (STANDS FOR PYTORCH) WHICH IS TYPICALLY TO SAVE (AKA SERIALIZED) A \"SINGLE\" OR A COLLECTION OF \n",
    "# TENSORS WHILE EXTENSION \".pth\" IS TO SAVE MODELS INCLUDING THEIR ARCHITECTURE ALONG THE WEIGHTS\n",
    "# PyTorch does NOT support APPENDING to a .pt or .pth file. THUS YOU MUST load the existing file, add the new data, and then save it again. \n",
    "# \"NOT PREFERRED\" BECAUSE WILL EVENTUALLY RUN OUT OF MEMORY!!!!\n",
    "#torch.save(inputs_tensor, path_to_inputs_tensor)\n",
    "\n",
    "\n",
    "# TO SAVE AS NUMPY\n",
    "# Convert tensors to numpy arrays and save them\n",
    "#inputs_numpy = {key: value.cpu().detach().numpy() for key, value in inputs_tensor.items()}\n",
    "\n",
    "#path_to_inputs_tensor_as_numpy = os.path.join(os.getcwd(), \"data\",\"inputs_tensor.npy\")\n",
    "\n",
    "# numpy np.save() does NOT support appending data to an existing .npy file AS np.save() overwrites the file AT EACH CALL\n",
    "#YOU MUST load the existing file, add the new data, and then save it again.\n",
    "# \"NOT PREFERRED\" BECAUSE WILL EVENTUALLY RUN OUT OF MEMORY!!!!\n",
    "#np.save(path_to_inputs_tensor_as_numpy, inputs_numpy)\n",
    "\n",
    "\n",
    "# PREFERRED IS TO USE HDF5 (Hierarchical Data Format version 5) FORMAT \n",
    "# FOR EFFICIENT INCREMENTAL APPENDING AND SAVING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inputs_tensor_as_numpy(path_to_df, chunk_size, max_length, batch_size, output_path):\n",
    "    \n",
    "    # INSTANTIATE THE pre-trained TOKENIZER TO BE APPLIED THE TEXT PASSAGES (AKA KNOWLEDGE BASE )\n",
    "    tokenizer_passage_encoder = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')  \n",
    "                                                                            \n",
    "    batch_of_inputs = []    \n",
    "\n",
    "    # for chunk_df in pd.read_csv(file_path, skiprows=range(1, start_row + 1), chunksize=chunk_size):                \n",
    "    for chunk_df in pd.read_csv(path_to_df, sep='\\t', names=['id', 'text'], chunksize=chunk_size):          \n",
    "\n",
    "          # CREATE A LIST WITH AS MANY ELEMENTS AS THE DATAFRAME CHUNK\n",
    "          passages_list = list(chunk_df['text'])\n",
    "\n",
    "          #print(\"\\n CPU compute of inputs:\") \n",
    "          # Hugging Face's DPRQuestionEncoderTokenizer \"CANNOT\" USE GPU. Tokenization is a CPU-bound process \n",
    "          # IT WILL RETURN A DICTIONARY CONTAINING PyTorch TENSORS \n",
    "          inputs = tokenizer_passage_encoder(passages_list, return_tensors='pt', padding=True, max_length=max_length, truncation=True)\n",
    "\n",
    "          # MANY libraries (like NumPy) READ/WRITE EASILY FROM HDF5 FILE AND  NumPy arrays use less memory compared to PyTorch tensors\n",
    "          inputs_numpy = {key: value.detach().numpy() for key, value in inputs.items()}\n",
    "          del inputs\n",
    "          #print(inputs_numpy.keys())\n",
    "\n",
    "          batch_of_inputs.append(inputs_numpy)\n",
    "\n",
    "          if len(batch_of_inputs)>batch_size:\n",
    "               \n",
    "               print(len(batch_of_inputs))\n",
    "               \n",
    "               # \"h5py.File(output_path, 'a')\"  OPENS THE FILE IN  \"append\" mode WITHOUT DELETING EXISTING data OR AUTOMATICALLY \n",
    "               # CREATE IF \"at output_path\"\n",
    "               with h5py.File(output_path, 'a') as f:\n",
    "\n",
    "                    # Iterate over each key in the first dictionary to initialize datasets\n",
    "                    for key in batch_of_inputs[0].keys():\n",
    "\n",
    "                         #print(\"key when batch size met\", key)\n",
    "                         # Stack all data for the current key across dictionaries into a single NumPy array\n",
    "                         data_array = np.stack([d[key] for d in batch_of_inputs])\n",
    "\n",
    "                         # Check if the dataset already exists in the file\n",
    "                         if key in f:\n",
    "\n",
    "                              # Resize the dataset to accommodate the new data\n",
    "                              dataset = f[key]\n",
    "                              dataset.resize((dataset.shape[0] + data_array.shape[0]), axis=0)\n",
    "                              dataset[-data_array.shape[0]:] = data_array\n",
    "                         else:\n",
    "                              # Create a new dataset with expandable dimensions\n",
    "                              f.create_dataset(key, data=data_array, maxshape=(None,) + data_array.shape[1:])\n",
    "\n",
    "               # Clear the batch and row numbers after writing to the file\n",
    "               passages_list.clear()\n",
    "               del inputs_numpy\n",
    "               batch_of_inputs.clear()               \n",
    "               del data_array\n",
    "               gc.collect()          \n",
    "       \n",
    "    if len(batch_of_inputs)>0: \n",
    "                         \n",
    "          print(len(batch_of_inputs))\n",
    "\n",
    "          # \"h5py.File(output_path, 'a')\"  OPENS THE FILE IN  \"append\" mode WITHOUT DELETING EXISTING data OR AUTOMATICALLY \n",
    "          # CREATE IF \"at output_path\"\n",
    "          with h5py.File(output_path, 'a') as f:\n",
    "\n",
    "               # Iterate over each key in the first dictionary to initialize datasets\n",
    "               for key in batch_of_inputs[0].keys():\n",
    "\n",
    "                    #print(\"key when batch size NOT met:\", key, )\n",
    "                    # Stack all data for the current key across dictionaries into a single NumPy array\n",
    "                    data_array = np.stack([d[key] for d in batch_of_inputs])\n",
    "\n",
    "                    # Check if the dataset already exists in the file\n",
    "                    if key in f:\n",
    "\n",
    "                         # Resize the dataset to accommodate the new data\n",
    "                         dataset = f[key]\n",
    "                         dataset.resize((dataset.shape[0] + data_array.shape[0]), axis=0)\n",
    "                         dataset[-data_array.shape[0]:] = data_array\n",
    "                    else:\n",
    "                         # Create a new dataset with expandable dimensions\n",
    "                         f.create_dataset(key, data=data_array, maxshape=(None,) + data_array.shape[1:])\n",
    "\n",
    "          # Clear the batch and row numbers after writing to the file\n",
    "          passages_list.clear()\n",
    "          del inputs_numpy\n",
    "          batch_of_inputs.clear()               \n",
    "          del data_array\n",
    "          gc.collect() \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "6\n",
      "key when batch size NOT met: input_ids\n",
      "key when batch size NOT met: token_type_ids\n",
      "key when batch size NOT met: attention_mask\n",
      "CPU times: user 1h 41min 24s, sys: 3.78 s, total: 1h 41min 27s\n",
      "Wall time: 1h 42min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "divisor_value=1688\n",
    "#tsv_file_path1 = os.path.join(os.getcwd(), \"data\",\"text_passages.tsv\")\n",
    "chunk_size_for_inputs_tensor_as_numpy = divisor_value \n",
    "max_length = 256\n",
    "batch_size_for_inputs_tensor_as_numpy = 10 \n",
    "path_to_inputs_tensor_as_numpy = os.path.join(os.getcwd(), \"data\",\" inputs_tensors_saved_as_numpy.h5\")\n",
    "\n",
    "\n",
    "\n",
    "create_inputs_tensor_as_numpy(path_to_df=tsv_file_path1, \n",
    "                                 chunk_size=chunk_size_for_inputs_tensor_as_numpy,\n",
    "                                 max_length = max_length,\n",
    "                                 batch_size=batch_size_for_inputs_tensor_as_numpy,\n",
    "                                 output_path=path_to_inputs_tensor_as_numpy,\n",
    "                                 )\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_mask : (182, 1688, 256)\n",
      "input_ids : (182, 1688, 256)\n",
      "token_type_ids : (182, 1688, 256)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(path_to_inputs_tensor_as_numpy, 'r') as f:\n",
    "    for key in f.keys():\n",
    "        dataset = f[key]\n",
    "        print(f\"{key} : {dataset.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III-A.4. CREATE EMBEDDINGS FROM INPUTS TENSORS AS HD5 FILE\n",
    " * An HDF5 (Hierarchical Data Format version 5) file is a binary file format used to store large amounts of data efficiently and in a STRUCTURED way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_passage_embeddings(file_source_path, chunks_grabbed_from_source_h5_file, batch_size, output_path):    \n",
    "                                                                           \n",
    "    # INSTANTIATE THE pre-trained EMBEDDING MODEL FOR KNOWLEDGE BASE (AKA PASSAGES)\n",
    "    model_passages_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')   \n",
    "    model_passages_encoder = model_passages_encoder.to(device)\n",
    "    \n",
    "    batch_of_embeddings = []    \n",
    "\n",
    "    # Open the HDF5 file\n",
    "    with h5py.File(file_source_path, 'r') as f:          \n",
    "\n",
    "    # Iterate over the keys (assuming the keys are 'input_ids', 'attention_mask', etc.)\n",
    "     for key in f.keys()[0]:\n",
    "\n",
    "          dataset = f[key]\n",
    "          total_chunks_in_source_h5_file= dataset.shape[0]\n",
    "          print(f\"\\n THESE ARE THE TOTAL ROWS (CHUNKS) IN INPUT HD5 FILE : {total_chunks_in_source_h5_file} with dataset shape {dataset.shape}\")\n",
    "          #print(f\"\\n THESE ARE THE keys : {f.keys()}\")\n",
    "          # Load data in chunks\n",
    "          for start in range(0, total_chunks_in_source_h5_file, chunks_grabbed_from_source_h5_file):\n",
    "\n",
    "               end = min(start + chunks_grabbed_from_source_h5_file, total_chunks_in_source_h5_file)\n",
    "               #chunk_data = dataset[start:end]\n",
    "               #print(chunk_data.keys())  \n",
    "               \n",
    "               # GRAB INPUTS SAVED AS NUMPY\n",
    "              \n",
    "               \n",
    "               input_ids = f['input_ids'][start:end]             \n",
    "               attention_mask = f['attention_mask'][start:end]            \n",
    "               token_type_ids = f['token_type_ids'][start:end]  \n",
    "               \n",
    "\n",
    "               # CONVERT INPUTS SAVED AS NUMPY TO TENSORS AND THEN MOVE THEM TO GPU\n",
    "               inputs = {'input_ids': torch.tensor(input_ids.reshape(-1, max_length)).to(device),\n",
    "                         'attention_mask': torch.tensor(attention_mask.reshape(-1, max_length)).to(device),\n",
    "                         'token_type_ids':torch.tensor(token_type_ids.reshape(-1, max_length)).to(device),\n",
    "                         }\n",
    "  \n",
    "               # ON CPU\n",
    "               # inputs = {'input_ids': torch.tensor(input_ids),\n",
    "               #           'attention_mask': torch.tensor(attention_mask),\n",
    "               #           'token_type_ids':torch.tensor(token_type_ids)\n",
    "               #           }\n",
    "  \n",
    "\n",
    "               print(\"START EMBEDDINGS\")\n",
    "               # Perform the forward pass WITH gradients DISABLED for efficiency\n",
    "               with torch.no_grad(): \n",
    "                    embeddings = model_passages_encoder(**inputs).pooler_output   \n",
    "\n",
    "               # Convert to NumPy AND Detach and move to CPU \n",
    "               embeddings_numpy = embeddings.detach().cpu().numpy()    \n",
    "               print(\"SHAPE FOR CHUNK OF EMBEDDED DOCS:\", embeddings_numpy.shape)    \n",
    "\n",
    "               batch_of_embeddings.append(embeddings_numpy)\n",
    "               print(\"EMBEDDINGS FINISHED\")               \n",
    "               print(f\"LENGTH OF batch_of_embeddings {len(batch_of_embeddings)}\")    \n",
    "\n",
    "               if len(batch_of_embeddings)>batch_size:\n",
    "                    \n",
    "                    print(len(batch_of_embeddings))                                  \n",
    "\n",
    "                    # \"h5py.File(output_path, 'a')\" OPENS A IN  \"append\" mode WITHOUT DELETING EXISTING data OR AUTOMATICALLY CREATE IT \"at output_path\"\n",
    "                    # IF IT DOES NOT ALREADY EXIST.                    \n",
    "                    with h5py.File(output_path, 'a') as f2:\n",
    "\n",
    "                         # AN HD5 (Hierarchical Data Format version 5) FILE CAN CONTAIN MULTIPLE DATASETS. WE CALL \"dataset\" THE ONE WE ARE CREATING\n",
    "                         # Check if the dataset already exists OTHERWISE creates it with expandable dimensions\n",
    "                         if \"dataset\" not in f2:\n",
    "\n",
    "                              # WHEN INITIALIZING \"dataset\" ALLOW UNLIMITED ROWS VIA \"None\" (HERE EACH ROW WILL BE A CHUNK). \"dataset\" WILL INCREMENTALLY \n",
    "                              # RECEIVE EMBEDDINGS OF DOCUMENTS IN CHUNKS OF NUMPY ARRAYS HAVING EACH THE NUMBER OF \"COLUMNS\" DEFINED BY: \n",
    "                              # batch_of_embeddings[0].shape[1:] WHERE \"batch_of_embeddings[0]\" IS THE FIRST ARRAY IN THE LIST \"batch_of_embeddings\" AND\n",
    "                              # \".shape[1:]\" IS THE SHAPE OF THE THAT ARRAY EXCEPT THE FIRST DIMENSION. THUS IT IS JUST THE NUMBER OF \"COLUMNS\"\n",
    "                              # NOTE: WE SKIPPED GRABBING THE FIRST DIMENSION (# OF ROWS) BECAUSE WE ALREADY ADDRESSED THAT VIA \"NONE\"\n",
    "                                                          \n",
    "                              maxshape = (None,) + batch_of_embeddings[0].shape[1:]\n",
    "                              # CREATE WITH 0 ROWS AND \"batch_of_embeddings[0].shape[1:]\" NUMBER OF COLUMNS\n",
    "                              dset = f2.create_dataset(\"dataset\", shape=(0,) + batch_of_embeddings[0].shape[1:], \n",
    "                                                      maxshape=maxshape, \n",
    "                                                      # SPECIFY THAT EACH CHUNK APPENDED MUST HAVE AS NUMBER OF ROWS: \"batch_of_embeddings[0].shape[0]\"\n",
    "                                                      # CAN BE ANY NUMBER OF YOUR CHOICE. THE SYSTEM INTERNALLY SPLITS THE NUMBER OF ROW RECEIVED TO CREATE\n",
    "                                                      # THE SPECIFY NUMBER OF ROWS IN EACH CHUNK WITH The HDF5 format ALLOWING partial chunk UNITL MORE DATA\n",
    "                                                      # COMES IN LATER\n",
    "                                                      # chunks=((batch_of_embeddings[0].shape[0]),) + \\\n",
    "                                                      chunks=((batch_of_embeddings[0].shape[0])/chunks_grabbed_from_source_h5_file,) + \\\n",
    "                                                     \n",
    "                                                                                                              batch_of_embeddings[0].shape[1:])\n",
    "\n",
    "                         else:\n",
    "                              # Reopen the existing dataset if it already exists\n",
    "                              dset = f2[\"dataset\"]\n",
    "\n",
    "\n",
    "                         # COULD DO USE \"np.stack([np_arr for np_arr in batch_of_embeddings])\" HERE INSTEAD OF THE LOOP BUT NOT SURE MEMORY CAN HANDLE IT!!!!\n",
    "                         # HOWEVER IT WOULD DO BETTER BECAUSE ONLY A SINGLE \"WRITE\" OPERATION BUT WE LOOSE THE FLEXIBILITY OF ADDING \"SMALLER\" CHUNKS IN LATER\n",
    "                         # RUNS. np.vstack() JUST CONCATENATE ALONG axis 0 (all arrays MUST HAVE the same shape except in this dimension) \n",
    "                         # WHILE np.stack() ALSO ADDS A NEW DIMENSION TO THE OUTPUT: INPUT ARRAYS OF (100, 50) GIVE 10, 100, 50).\n",
    "                         # This WOULD BE useful if you want to keep each INPUT ARRAY AS separate SLICE in the stacked result \n",
    "                         # IT WOULD WORK AS FOLLOWS:\n",
    "                         # stacked_array = np.vstack(batch_of_embeddings)\n",
    "                         # Resize dataset to accommodate the stacked data\n",
    "                         #dset.resize((dset.shape[0] + stacked_array.shape[0]), axis=0)\n",
    "                         # Write the entire stacked array to the end of the dataset\n",
    "                         #dset[-stacked_array.shape[0]:] = stacked_array\n",
    "                         for array in batch_of_embeddings:\n",
    "\n",
    "                              # Resize dataset to accommodate new data\n",
    "                              dset.resize((dset.shape[0] + array.shape[0]), axis=0)\n",
    "                              \n",
    "                              # Write data to the end of the dataset\n",
    "                              dset[-array.shape[0]:] = array\n",
    "\n",
    "                    # Clear the batch and row numbers after writing to the file  \n",
    "                    del embeddings                  \n",
    "                    del embeddings_numpy\n",
    "                    batch_of_embeddings.clear()     \n",
    "                    gc.collect()          \n",
    "       \n",
    "    if len(batch_of_embeddings)>0: \n",
    "                              \n",
    "          print(len(batch_of_embeddings))          \n",
    "\n",
    "          with h5py.File(output_path, 'a') as f2:\n",
    "\n",
    "               # AN HD5 (Hierarchical Data Format version 5) FILE CAN CONTAIN MULTIPLE DATASETS. WE CALL \"dataset\" THE ONE WE ARE CREATING\n",
    "               # Check if the dataset already exists OTHERWISE creates it with expandable dimensions\n",
    "               if \"dataset\" not in f2:\n",
    "\n",
    "                    # WHEN INITIALIZING \"dataset\" ALLOW UNLIMITED ROWS VIA \"None\" (HERE EACH ROW WILL BE A CHUNK). \"dataset\" WILL INCREMENTALLY \n",
    "                    # RECEIVE EMBEDDINGS OF DOCUMENTS IN CHUNKS OF NUMPY ARRAYS HAVING EACH THE NUMBER OF \"COLUMNS\" DEFINED BY: \n",
    "                    # batch_of_embeddings[0].shape[1:] WHERE \"batch_of_embeddings[0]\" IS THE FIRST ARRAY IN THE LIST \"batch_of_embeddings\" AND\n",
    "                    # \".shape[1:]\" IS THE SHAPE OF THE THAT ARRAY EXCEPT THE FIRST DIMENSION. THUS IT IS JUST THE NUMBER OF \"COLUMNS\"\n",
    "                    # NOTE: WE SKIPPED GRABBING THE FIRST DIMENSION (# OF ROWS) BECAUSE WE ALREADY ADDRESSED THAT VIA \"NONE\"\n",
    "                                             \n",
    "                    maxshape = (None,) + batch_of_embeddings[0].shape[1:]\n",
    "                    # CREATE WITH 0 ROWS AND \"batch_of_embeddings[0].shape[1:]\" NUMBER OF COLUMNS\n",
    "                    dset = f2.create_dataset(\"dataset\", shape=(0,) + batch_of_embeddings[0].shape[1:], \n",
    "                                        maxshape=maxshape, \n",
    "                                        # SPECIFY THAT EACH CHUNK APPENDED MUST HAVE AS NUMBER OF ROWS: \"batch_of_embeddings[0].shape[0]\"\n",
    "                                        # CAN BE ANY NUMBER OF YOUR CHOICE. THE SYSTEM INTERNALLY SPLITS THE NUMBER OF ROW RECEIVED TO CREATE\n",
    "                                        # THE SPECIFY NUMBER OF ROWS IN EACH CHUNK WITH The HDF5 format ALLOWING partial chunk UNITL MORE DATA\n",
    "                                        # EVENTUALLY COMES IN LATER\n",
    "                                        # chunks=((batch_of_embeddings[0].shape[0]),) + \\\n",
    "                                        chunks=((batch_of_embeddings[0].shape[0])/chunks_grabbed_from_source_h5_file,) + \\\n",
    "                                        \n",
    "                                                                                                              batch_of_embeddings[0].shape[1:])\n",
    "                    \n",
    "\n",
    "               else:\n",
    "                    # # Reopen the existing dataset if it already exists\n",
    "                    dset  = f2[\"dataset\"]\n",
    "\n",
    "\n",
    "               for ind, array in enumerate(batch_of_embeddings):\n",
    "\n",
    "                    print(\"lAST BATCH: LIST ITEM #:\", ind)\n",
    "                    # Resize dataset to accommodate new data\n",
    "                    dset.resize((dset.shape[0] + array.shape[0]), axis=0)\n",
    "                    \n",
    "                    # Write data to the end of the dataset\n",
    "                    dset[-array.shape[0]:] = array\n",
    "\n",
    "          # Clear the batch and row numbers after writing to the file  \n",
    "          del embeddings        \n",
    "          del embeddings_numpy\n",
    "          batch_of_embeddings.clear() \n",
    "          gc.collect() \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " THESE ARE THE TOTAL ROWS (CHUNKS) IN INPUT HD5 FILE : 182 with dataset shape (182, 1688, 256)\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "\n",
      " THESE ARE THE TOTAL ROWS (CHUNKS) IN INPUT HD5 FILE : 182 with dataset shape (182, 1688, 256)\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "\n",
      " THESE ARE THE TOTAL ROWS (CHUNKS) IN INPUT HD5 FILE : 182 with dataset shape (182, 1688, 256)\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "1\n",
      "lAST BATCH: item #: 0\n",
      "CPU times: user 1d 6h 26min 29s, sys: 22.7 s, total: 1d 6h 26min 51s\n",
      "Wall time: 1d 6h 59min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "path_to_inputs_tensor_as_numpy = os.path.join(os.getcwd(), \"data\",\"inputs_tensors_saved_as_numpy.h5\")\n",
    "divisor_value=1688\n",
    "max_length=256\n",
    "\n",
    "file_path_for_text_passage_embeddings = path_to_inputs_tensor_as_numpy\n",
    "# NUMBER OF CHUNKS IN SOURCE H5 FILE GRABBED AT ONCE\n",
    "# MUST BE A \"DIVISOR\" OF data_size/divisor_value:  [1, 2, 7, 13, 14.0, 26.0, 91.0, 182.0]\n",
    "chunks_grabbed_from_source_h5_file =  2\n",
    "batch_size=3\n",
    "output_path_for_text_passage_embeddings = os.path.join(os.getcwd(), \"data\",\"text_passage_embeddings.h5\")\n",
    "\n",
    "\n",
    "\n",
    "create_text_passage_embeddings(file_source_path=file_path_for_text_passage_embeddings,\n",
    "                               chunks_grabbed_from_source_h5_file=chunks_grabbed_from_source_h5_file,\n",
    "                               batch_size=batch_size,\n",
    "                               output_path=output_path_for_text_passage_embeddings,\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset : (307216, 768)\n"
     ]
    }
   ],
   "source": [
    "#output_path_for_text_passage_embeddings = os.path.join(os.getcwd(), \"data\",\"text_passage_embeddings.h5\")\n",
    "\n",
    "with h5py.File(output_path_for_text_passage_embeddings, 'r') as f:\n",
    "\n",
    "    for key in f.keys():\n",
    "\n",
    "        dataset = f[key]\n",
    "        print(f\"{key} : {dataset.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1787, -0.1265,  0.1265,  ..., -0.1744, -0.2394,  0.0644],\n",
      "        [-0.0223, -0.1575,  0.0675,  ...,  0.1393,  0.1487, -0.1633],\n",
      "        [ 0.1253,  0.0744,  0.0494,  ..., -0.2184, -0.0631,  0.2588],\n",
      "        ...,\n",
      "        [-0.1930,  0.2534,  0.3329,  ..., -0.2555,  0.1054, -0.0842],\n",
      "        [-0.1380,  0.3966,  0.0220,  ...,  0.3717, -0.0123, -0.2291],\n",
      "        [-0.0100, -0.2103, -0.1419,  ...,  0.1013, -0.5435, -0.3156]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III-A.5. CREATE FAISS INDECES FROM TEXT PASSAGES EMBEDDINGS\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda:0')\n",
    "\n",
    "def create_faiss_IndexIVFFlat_index(file_path_for_embeddings_hd5, indeces_path, chunk_size, divisor, number_cluster_for_embeddings=100, \n",
    "                                    sample_embeddings_size=50000):\n",
    "\n",
    "    # Open the HDF5 file\n",
    "    with h5py.File(file_path_for_embeddings_hd5, 'r') as f: \n",
    "\n",
    "    # Iterate over the keys associated with the different datasets inside the HD5 file\n",
    "     for key in f.keys():  \n",
    "\n",
    "        # Grabs iteratively each dataset \n",
    "        dataset = f[key]\n",
    "\n",
    "        # Dimensionality of embeddings\n",
    "        embeddings_vectors_length = dataset.shape[1]\n",
    "\n",
    "        # NUMBER OF VECTORS EMBEDDINGS (ROWS)\n",
    "        total_embeddings = dataset.shape[0]\n",
    "        \n",
    "        # IF EMBEDDINGS ARE \"NORMALIZED\":\n",
    "        # Initialize a quantizer\n",
    "        # quantizer = faiss.IndexFlatIP(d) \n",
    "        # Initialize the IVF index\n",
    "        # index_cpu = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_COSINE)  \n",
    "              \n",
    "        quantizer = faiss.IndexFlatL2(embeddings_vectors_length) \n",
    "        # Initialize the IVF index\n",
    "        # \"number_cluster_for_embeddings\" IS THE Number of clusters INTO WHICH that ALL THE EMBEDDINGS IN THE SOURCE FILE will be partitioned.\n",
    "        index_cpu = faiss.IndexIVFFlat(quantizer, embeddings_vectors_length, number_cluster_for_embeddings, faiss.METRIC_L2)\n",
    "\n",
    "        # \"index.nprobe\" IS typically set after the index has been created and before performing a search\n",
    "        # \"index.nprobe\" SPECIFIES HOW MANY OF THE CLUSTERS MADE USING THE EMBEDDINGS WILL BE INCLUDED FOR A SEARCH, balancing EFFICIENCY and ACCUARCY.                 \n",
    "        # BY Setting nprobe equal to nlist the index will search \"ALL\" clusters in the index TO MAXIMIZE \"RECALL\".\n",
    "        index_cpu.nprobe = number_cluster_for_embeddings\n",
    "\n",
    "        # Transfer CPU index to GPU (device 0)\n",
    "        try:            \n",
    "            #print(\"FAISS-GPU resources are available.\")\n",
    "            gpu_resources = faiss.StandardGpuResources()\n",
    "            index = faiss.index_cpu_to_gpu(gpu_resources, 0, index_cpu)  \n",
    "        except AttributeError:\n",
    "            print(f\" FAISS-GPU resources are not available. Ensure the GPU version of FAISS is installed.\", \"\\n\")\n",
    "        \n",
    "        \n",
    "        # SPECIFY THE SIZE OF EMBEDDINGS SAMPLE TO BE USED FOR \"TRAINING\" the index\n",
    "        sample_embeddings = dataset[:sample_embeddings_size]          \n",
    "        index.train(sample_embeddings)\n",
    "\n",
    "        # TRANSFER BACK THE INDEX TO CPU BEFORE SAVINGS\n",
    "        index_cpu = faiss.index_gpu_to_cpu(index)\n",
    "\n",
    "        # Set index to use disk storage\n",
    "        # If the index size exceeds available MEMORY, FAISS AUTOMATICALLY WILL USE disk-backed storage.        \n",
    "        faiss.write_index(index_cpu, indeces_path)\n",
    "\n",
    "        index = faiss.index_cpu_to_gpu(gpu_resources, 0, index_cpu)\n",
    "\n",
    "        # Add embeddings in chunks\n",
    "        for start in range(0, total_embeddings, chunk_size):\n",
    "            end = min(start + chunk_size, total_embeddings)\n",
    "            embeddings_chunk = np.array(dataset[start:end], dtype='float32')\n",
    "            # Add to disk-backed index\n",
    "            index.add(embeddings_chunk)  \n",
    "\n",
    "            # Periodically save to disk\n",
    "            # Save every 10 chunks\n",
    "            if start % (chunk_size * divisor) == 0: \n",
    "\n",
    "                index_cpu = faiss.index_gpu_to_cpu(index) \n",
    "                # USE \"partial_index\" AVOIDS fragmentation AND unnecessary I/O on THE \"FINAL\" file\n",
    "                faiss.write_index(index_cpu, os.path.join(os.getcwd(),\"data\",\"partial_index.faiss\"))\n",
    "                print(f\"Partial Index saved at chunk starting at row: {start} and ending at row: {end}\")\n",
    "                index = faiss.index_cpu_to_gpu(gpu_resources, 0, index_cpu)\n",
    "\n",
    "        index_cpu = faiss.index_gpu_to_cpu(index) \n",
    "        # Final save to ensure all embeddings are written\n",
    "        faiss.write_index(index_cpu, indeces_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index saved at chunk 0 to 76804\n"
     ]
    }
   ],
   "source": [
    "output_path_for_text_passage_embeddings = os.path.join(os.getcwd(), \"data\",\"text_passage_embeddings.h5\")\n",
    "path_to_save_faiss_indeces= os.path.join(os.getcwd(), \"data\",\"disk_index.faiss\")\n",
    "chunks_of_embeddings = int(data_size/4)\n",
    "divisor = 26\n",
    "number_cluster_for_embeddings = 2048 #Max possible!!!!)\n",
    "sample_embeddings_size = data_size\n",
    "\n",
    "create_faiss_IndexIVFFlat_index(file_path_for_embeddings_hd5=output_path_for_text_passage_embeddings, \n",
    "                                indeces_path=path_to_save_faiss_indeces, \n",
    "                                chunk_size=chunks_of_embeddings,\n",
    "                                divisor = divisor,\n",
    "                                number_cluster_for_embeddings=number_cluster_for_embeddings, \n",
    "                                sample_embeddings_size=sample_embeddings_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III-A.6. RAG MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132868\n",
      "CPU times: user 32.2 s, sys: 1.7 s, total: 33.9 s\n",
      "Wall time: 51.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# FIND MAX LENGHTS ACCROSS ALL TEXT PASSAGES\n",
    "tsv_file_path1 = os.path.join(os.getcwd(), \"data\",\"text_passages.tsv\")\n",
    "\n",
    "#for chunk_df in pd.read_csv(file_path, skiprows=range(1, start_row + 1), chunksize=chunk_size):\n",
    "for chunk in pd.read_csv(tsv_file_path1, sep='\\t', names=['id', 'text'], chunksize=100):  \n",
    "\n",
    "    passages_max_len = 0\n",
    "    column_name = chunk.columns[1]\n",
    "    length_of_texts = [len(row[column_name]) for index, row in chunk.iterrows()]\n",
    "    if passages_max_len < max(length_of_texts):\n",
    "        passages_max_len =  max(length_of_texts)\n",
    "\n",
    "passages_max_len = passages_max_len +10\n",
    "print(passages_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder_containing_text_passages_with_embeddings(text_passages_path, file_path_for_embeddings_hd5, \n",
    "                                                           chunk_size, batch_size, max_len, output_path):\n",
    "\n",
    "\n",
    "    # Open the HDF5 file\n",
    "    with h5py.File(file_path_for_embeddings_hd5, 'r') as f: \n",
    "     \n",
    "\n",
    "    # Iterate over the keys associated with the different datasets inside the HD5 file\n",
    "     for key in f.keys():  \n",
    "\n",
    "        # Grabs iteratively each dataset \n",
    "        dataset = f[key]\n",
    "        total_embeddings = dataset.shape[0]\n",
    "        #print(total_embeddings)\n",
    "\n",
    "        batch_of_dicts = []\n",
    "\n",
    "        #for chunk_df in pd.read_csv(file_path, skiprows=range(1, start_row + 1), chunksize=chunk_size):\n",
    "        for chunk in pd.read_csv(text_passages_path, sep='\\t', names=['id', 'text'], chunksize=chunk_size): \n",
    "            \n",
    "            column_name = chunk.columns[1]\n",
    "            \n",
    "            list_of_dicts_from_chunks = [{column_name: row[column_name], \"title\": 'passage '+ str(index), \"embeddings\": dataset[index]} \n",
    "                                                                                                             for index, row in chunk.iterrows()\n",
    "                                        ] \n",
    "            \n",
    "            batch_of_dicts = batch_of_dicts + list_of_dicts_from_chunks\n",
    "            #print(len(batch_of_dicts))         \n",
    "              \n",
    "\n",
    "            if len(batch_of_dicts)> batch_size:\n",
    "                        \n",
    "                print(len(batch_of_dicts)) \n",
    "\n",
    "                # 'S100' is for a fixed-length ASCII bytes USED FOR STRINGS (HERE 100 BYTES)\n",
    "                # ('f4', (chunk_size, 768)) is for  N-D numpy arrays with shape (chunk_size, 768) of float32 ('f4') type\n",
    "                vector_sizes = batch_of_dicts[0][\"embeddings\"].shape[0]\n",
    "                #print(vector_sizes)\n",
    "\n",
    "                dtype = np.dtype([(column_name, 'S' + str(max_len)),  ('title', 'S' + str(30)),  ('embeddings', ('f4', (1, vector_sizes))) ])\n",
    "\n",
    "                #structured_array = np.array([tuple(d.values()) for d in batch_of_dicts], dtype=dtype)\n",
    "                structured_array = np.array([(d[column_name].encode('utf-8'), d[\"title\"].encode('utf-8'), d[\"embeddings\"]) \n",
    "                                                                                                                      for d in batch_of_dicts], dtype=dtype)\n",
    "                             \n",
    "\n",
    "                # \"h5py.File(output_path, 'a')\" OPENS A IN  \"append\" mode WITHOUT DELETING EXISTING data OR AUTOMATICALLY CREATE IT \"at output_path\"\n",
    "                # IF IT DOES NOT ALREADY EXIST.                    \n",
    "                with h5py.File(output_path, 'a') as f2:\n",
    "\n",
    "                        # AN HD5 (Hierarchical Data Format version 5) FILE CAN CONTAIN MULTIPLE DATASETS. WE CALL \"dataset\" THE ONE WE ARE CREATING\n",
    "                        # Check if the dataset already exists OTHERWISE creates it with expandable dimensions\n",
    "                        if \"dataset\" not in f2:\n",
    "\n",
    "                            # WHEN INITIALIZING \"dataset\" ALLOW UNLIMITED ROWS VIA \"None\" (HERE EACH ROW WILL BE A CHUNK). \"dataset\" WILL INCREMENTALLY \n",
    "                            # RECEIVE EMBEDDINGS OF DOCUMENTS IN CHUNKS OF NUMPY ARRAYS HAVING EACH THE NUMBER OF \"COLUMNS\" DEFINED BY: \n",
    "                            # batch_of_embeddings[0].shape[1:] WHERE \"batch_of_embeddings[0]\" IS THE FIRST ARRAY IN THE LIST \"batch_of_embeddings\" AND\n",
    "                            # \".shape[1:]\" IS THE SHAPE OF THE THAT ARRAY EXCEPT THE FIRST DIMENSION. THUS IT IS JUST THE NUMBER OF \"COLUMNS\"\n",
    "                            # NOTE: WE SKIPPED GRABBING THE FIRST DIMENSION (# OF ROWS) BECAUSE WE ALREADY ADDRESSED THAT VIA \"NONE\"\n",
    "                                                        \n",
    "                            maxshape = (None,) + structured_array.shape[1:]\n",
    "                            \n",
    "                            # CREATE WITH 0 ROWS AND \"batch_of_embeddings[0].shape[1:]\" NUMBER OF COLUMNS\n",
    "                            dset = f2.create_dataset(\"dataset\", data=structured_array, maxshape=maxshape)\n",
    "\n",
    "                            # Add column names as metadata\n",
    "                            dset.attrs['column_names'] = [column_name, 'title', 'embeddings']\n",
    "                                                  \n",
    "                        else:\n",
    "                            # Reopen the existing dataset if it already exists\n",
    "                            dset = f2[\"dataset\"]\n",
    "                            dset.resize((dset.shape[0] + len(batch_of_dicts)), axis=0)\n",
    "                            dset[-len(batch_of_dicts):] = structured_array\n",
    "\n",
    "\n",
    "                # Clear the batch and row numbers after writing to the file         \n",
    "                list_of_dicts_from_chunks.clear()\n",
    "                batch_of_dicts.clear()    \n",
    "                del structured_array \n",
    "                gc.collect()          \n",
    "\n",
    "    if len(batch_of_dicts)>0: \n",
    "                            \n",
    "        print(len(batch_of_dicts))       \n",
    "\n",
    "        # 'S100' is for a fixed-length ASCII bytes USED FOR STRINGS (HERE 100 BYTES)\n",
    "        # ('f4', (chunk_size, 768)) is for  N-D numpy arrays with shape (chunk_size, 768) of float32 ('f4') type\n",
    "        vector_sizes = batch_of_dicts[0][\"embeddings\"].shape[0]\n",
    "        #print(vector_sizes)\n",
    "\n",
    "        dtype = np.dtype([(column_name, 'S' + str(max_len)),  ('title', 'S' + str(30)),  ('embeddings', ('f4', (1, vector_sizes))) ])\n",
    "        \n",
    "        #structured_array = np.array([tuple(d.values()) for d in batch_of_dicts], dtype=dtype)\n",
    "        structured_array = np.array([(d[column_name].encode('utf-8'), d[\"title\"].encode('utf-8'), d[\"embeddings\"]) \n",
    "                                                                                                                      for d in batch_of_dicts], dtype=dtype)\n",
    "\n",
    "        # \"h5py.File(output_path, 'a')\" OPENS A IN  \"append\" mode WITHOUT DELETING EXISTING data OR AUTOMATICALLY CREATE IT \"at output_path\"\n",
    "        # IF IT DOES NOT ALREADY EXIST.                    \n",
    "        with h5py.File(output_path, 'a') as f2:\n",
    "\n",
    "                # AN HD5 (Hierarchical Data Format version 5) FILE CAN CONTAIN MULTIPLE DATASETS. WE CALL \"dataset\" THE ONE WE ARE CREATING\n",
    "                # Check if the dataset already exists OTHERWISE creates it with expandable dimensions\n",
    "                if \"dataset\" not in f2:\n",
    "\n",
    "                    # WHEN INITIALIZING \"dataset\" ALLOW UNLIMITED ROWS VIA \"None\" (HERE EACH ROW WILL BE A CHUNK). \"dataset\" WILL INCREMENTALLY \n",
    "                    # RECEIVE EMBEDDINGS OF DOCUMENTS IN CHUNKS OF NUMPY ARRAYS HAVING EACH THE NUMBER OF \"COLUMNS\" DEFINED BY: \n",
    "                    # batch_of_embeddings[0].shape[1:] WHERE \"batch_of_embeddings[0]\" IS THE FIRST ARRAY IN THE LIST \"batch_of_embeddings\" AND\n",
    "                    # \".shape[1:]\" IS THE SHAPE OF THE THAT ARRAY EXCEPT THE FIRST DIMENSION. THUS IT IS JUST THE NUMBER OF \"COLUMNS\"\n",
    "                    # NOTE: WE SKIPPED GRABBING THE FIRST DIMENSION (# OF ROWS) BECAUSE WE ALREADY ADDRESSED THAT VIA \"NONE\"\n",
    "                                                \n",
    "                    maxshape = (None,) + structured_array.shape[1:]\n",
    "                    \n",
    "                    # CREATE WITH 0 ROWS AND \"batch_of_embeddings[0].shape[1:]\" NUMBER OF COLUMNS\n",
    "                    dset = f2.create_dataset(\"dataset\", data=structured_array, maxshape=maxshape)\n",
    "\n",
    "                    # Add column names as metadata\n",
    "                    dset.attrs['column_names'] = [column_name, 'title', 'embeddings']\n",
    "                                            \n",
    "                else:\n",
    "                    # Reopen the existing dataset if it already exists\n",
    "                    dset = f2[\"dataset\"]\n",
    "                    dset.resize((dset.shape[0] + len(batch_of_dicts)), axis=0)\n",
    "                    dset[-len(batch_of_dicts):] = structured_array\n",
    "\n",
    "\n",
    "        # Clear the batch and row numbers after writing to the file         \n",
    "        list_of_dicts_from_chunks.clear()\n",
    "        batch_of_dicts.clear()    \n",
    "        del structured_array \n",
    "        gc.collect()          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_for_text_passage_embeddings = os.path.join(os.getcwd(), \"data\",\"text_passage_embeddings.h5\")\n",
    "chunk_size_for_text_passages_with_embeddings = int(data_size/364)\n",
    "batch_size_for_text_passages_with_embeddings = chunk_size_for_text_passages_with_embeddings*4\n",
    "\n",
    "output_path_for_text_passage_with_embeddings_added = os.path.join(\"/mnt/h/RAG\",\"text_passages_with_embeddings_added.h5\")\n",
    "#output_path_for_text_passage_with_embeddings_added = os.path.join('/home/koffi', 'text_passages_with_embeddings_added.h5')\n",
    "\n",
    "create_folder_containing_text_passages_with_embeddings(tsv_file_path1, output_path_for_text_passage_embeddings, \n",
    "                                                       chunk_size=chunk_size_for_text_passages_with_embeddings,\n",
    "                                                       batch_size=batch_size_for_text_passages_with_embeddings,\n",
    "                                                       max_len = passages_max_len,\n",
    "                                                       output_path=output_path_for_text_passage_with_embeddings_added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New Text Document.txt']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#os.listdir(os.path.join('/home/koffi', 'artefact_school_of_data'))\n",
    "os.listdir( os.path.join(\"/mnt/h/RAG/passages\"))\n",
    "# sudo mount -t drvfs H: /mnt/h\n",
    "#os.listdir( os.path.join(\"/mnt/h\"))\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43888\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848ad47b26c7453a8c54e249707e19a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/43888 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43888\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceedabbfd5364ac2a56c02658dd161b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/43888 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43888\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee284e1e7dcb454bb4f611f4727da05e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/43888 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43888\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8b83e4b0334c44a4297493bf063acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/43888 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43888\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be6a2e59cd547c1884bab9cd6a02ac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/43888 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43888\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05164411b0fe455c86f3509daf6034b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/43888 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43888\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ee67b8196a4afcb6769eacb799c099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/43888 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset, concatenate_datasets\n",
    "\n",
    "def create_dataset_in_chunks(hdf5_file_path, chunk_size, batch_size, output_path_for_dataset_chunks):\n",
    "\n",
    "    with h5py.File(hdf5_file_path, 'r') as f:\n",
    "        \n",
    "        \n",
    "        # Get the dtype of the dataset\n",
    "        dtype =  f[\"dataset\"].dtype          \n",
    "\n",
    "        total_size = f[\"dataset\"].shape[0]  \n",
    "        #print(total_size)  \n",
    "\n",
    "        chunk_num = 0\n",
    "        for start in range(0, total_size, chunk_size):\n",
    "                              \n",
    "               end = min(start + chunk_size, total_size)\n",
    "               \n",
    "               # Read a batch of data\n",
    "               data_chunk = f[\"dataset\"][start:end]\n",
    "\n",
    "               chunk_num +=1\n",
    "            \n",
    "               list_of_dicts_from_chunk = [] \n",
    "               # Convert the batch to a list of dictionaries\n",
    "               #list_chunk = [{\"data\": item} for item in data_chunk]\n",
    "               for item in data_chunk:\n",
    "                    \n",
    "                    text = item[dtype.names[0]].decode('utf-8')\n",
    "                    title = item[dtype.names[1]].decode('utf-8')\n",
    "                    embeddings = item[dtype.names[2]]\n",
    "\n",
    "                    row_into_dict = {\"text\":text, \"title\":title, \"embeddings\":embeddings}\n",
    "\n",
    "                    list_of_dicts_from_chunk.append(row_into_dict)\n",
    "                    #print(len(list_of_dicts_from_chunk))\n",
    "                    \n",
    "\n",
    "               if len(list_of_dicts_from_chunk) >batch_size:\n",
    "                    \n",
    "                    print(len(list_of_dicts_from_chunk))\n",
    "                    \n",
    "                    # Create a Dataset from the chunk\n",
    "                    chunk_dataset = Dataset.from_list(list_of_dicts_from_chunk)\n",
    "                         \n",
    "                    # Convert the Dataset to a pandas DataFrame by using the 'to_pandas()' method\n",
    "                    #chunk_df = chunk_dataset.to_pandas() \n",
    "\n",
    "                    # SAVE AS PARQUET FILE\n",
    "                    #chunk_df.to_parquet( os.path.join(output_path_for_dataset_chunks, f'chunk_{chunk_num}.parquet'), index=False) \n",
    "                    chunk_dataset.save_to_disk( os.path.join(output_path_for_dataset_chunks, f'chunk_{chunk_num}'))\n",
    "                    #chunk_dataset.save_to_disk( output_path_for_dataset_chunks)   \n",
    "               \n",
    "               list_of_dicts_from_chunk.clear()\n",
    "               chunk_dataset = None\n",
    "               chunk_df = None\n",
    "               del chunk_dataset, chunk_df\n",
    "               gc.collect\n",
    "          \n",
    "        if len(list_of_dicts_from_chunk) >0:\n",
    "                    \n",
    "                    print(len(list_of_dicts_from_chunk))\n",
    "                    \n",
    "                    # Create a Dataset from the chunk\n",
    "                    chunk_dataset = Dataset.from_list(list_of_dicts_from_chunk)\n",
    "                         \n",
    "                    # Convert the Dataset to a pandas DataFrame by using the 'to_pandas()' method\n",
    "                    #chunk_df = chunk_dataset.to_pandas() \n",
    "\n",
    "                    # SAVE AS PARQUET FILE\n",
    "                    #chunk_df.to_parquet( os.path.join(output_path_for_dataset_chunks, f'chunk_{chunk_num}.parquet'), index=False)  \n",
    "                    chunk_dataset.save_to_disk( os.path.join(output_path_for_dataset_chunks, f'chunk_{chunk_num}'))  \n",
    "                    #chunk_dataset.save_to_disk( output_path_for_dataset_chunks)  \n",
    "\n",
    "                    \n",
    "               \n",
    "        list_of_dicts_from_chunk.clear()\n",
    "        chunk_dataset = None\n",
    "        chunk_df = None\n",
    "        del chunk_dataset, chunk_df\n",
    "        gc.collect\n",
    "\n",
    "                              \n",
    "                    \n",
    "output_folder_for_dataset_chunks = os.path.join(\"/mnt/h/RAG/passages\")\n",
    "create_dataset_in_chunks(output_path_for_text_passage_with_embeddings_added, chunk_size=43888,batch_size=43887, \n",
    "                         output_path_for_dataset_chunks = output_folder_for_dataset_chunks)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06eb0ce3e14a429589b9e0fb17879c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/13 shards):   0%|          | 0/307216 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets\n",
    "chunk_dirs = [f\"/mnt/h/RAG/passages/chunk_{i+1}\" for i in range(7)]\n",
    "datasets = [load_from_disk(chunk_dir) for chunk_dir in chunk_dirs]\n",
    "# Concatenate all chunks into a single dataset\n",
    "combined_dataset = concatenate_datasets(datasets)\n",
    "\n",
    "combined_dataset.save_to_disk(output_folder_for_dataset_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "#not needed: CONVERT FILE WITH PASSAGES TO \"PICKLE\" FORMAT!!!!!!!!\n",
    "#pkl_file_path = os.path.join(os.getcwd(), \"data\",\"passages.pkl\")\n",
    "# Save the list of dictionaries to a .pkl file\n",
    "#with open(pkl_file_path, 'wb') as f:\n",
    "#    pickle.dump(passages_as_list_of_dict, f)\n",
    "\n",
    "# Check if the file exists\n",
    "#if not os.path.exists(pkl_file_path):\n",
    "#    raise FileNotFoundError(f\"The file {pkl_file_path} does not exist.\")\n",
    "\n",
    "\n",
    "# INSTANTIATE THE \"RAG\" the TOKENIZER\n",
    "tokenizer_for_retriever = RagTokenizer.from_pretrained(\"facebook/rag-sequence-nq\")\n",
    "\n",
    "# Example question\n",
    "#question = \"What is machine learning?\"\n",
    "#question = \"What is the capital Deutscheland?\"\n",
    "#question = \"Who won the world series in 2020?\"\n",
    "question = \"where is Berlin?\"\n",
    "\n",
    "# Tokenize the input\n",
    "input_ids = tokenizer_for_retriever(question, return_tensors=\"pt\").input_ids\n",
    "\n",
    "\n",
    "\n",
    "# INSTANTIATE the RAG model\n",
    "model_result_generation = RagTokenForGeneration.from_pretrained(\"facebook/rag-sequence-nq\")\n",
    "\n",
    "\n",
    "# Get hidden states for the question USING \"question_encoder\" \n",
    "# ALSO Disable gradient calculation for inference\n",
    "with torch.no_grad():  \n",
    "    question_hidden_states = model_result_generation.question_encoder(input_ids)[0] \n",
    "\n",
    "\n",
    "# Convert TENSOR to NumPy array BY MOVING IT to CPU and convert to NumPy array (cpu FRAMEWORK)\n",
    "question_hidden_states_np = question_hidden_states.cpu().numpy()\n",
    "\n",
    "output_folder_for_dataset_chunks = os.path.join(\"/mnt/h/RAG/passages\")\n",
    "path_to_save_faiss_indeces= os.path.join(os.getcwd(), \"data\",\"disk_index.faiss\")\n",
    "\n",
    "retriever = RagRetriever.from_pretrained(\"facebook/rag-sequence-nq\",\n",
    "                                         index_name=\"custom\", output_retrieved=False,\n",
    "                                         tokenizer = tokenizer_for_retriever,\n",
    "                                         passages_path=output_folder_for_dataset_chunks, \n",
    "                                         index_path=path_to_save_faiss_indeces\n",
    "                                         )\n",
    "\n",
    "\n",
    "#  Use the retriever to retrieve relevant documents\n",
    "docs_dict = retriever(input_ids, question_hidden_states=question_hidden_states_np, return_tensors=\"pt\")\n",
    "\n",
    "# RETRIEVE EMBEDDINGS OF DOCUMENTS (Assuming you have retrieved embeddings for the documents)\n",
    "retrieved_doc_embeds = docs_dict['retrieved_doc_embeds']  # Shape (n_docs, embed_dim)\n",
    "\n",
    "# GET CONTEXT INPUT IDs\n",
    "n_docs =docs_dict['context_input_ids'].shape[0]\n",
    "batch_size = input_ids.shape[0]\n",
    "context_input_ids = docs_dict['context_input_ids'].reshape(batch_size * n_docs, -1)\n",
    "\n",
    "\n",
    "if 'context_attention_mask' not in docs_dict:\n",
    "    docs_dict['context_attention_mask'] = torch.ones_like(context_input_ids)\n",
    "#print(docs_dict['context_attention_mask'].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III-A.6.1. RESPONSE BASED ON COSINE SIMILARITY WITH RETRIEVED DOCUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " berlin\n"
     ]
    }
   ],
   "source": [
    "# Expand question_hidden_states to [1, 5, 1, 768]\n",
    "expanded_question_hidden_states = question_hidden_states.unsqueeze(1).unsqueeze(2).expand(-1, retrieved_doc_embeds.size(1), retrieved_doc_embeds.size(2), -1)\n",
    "\n",
    "# Compute cosine similarity along the last dimension\n",
    "doc_scores_cosine = F.cosine_similarity(expanded_question_hidden_states, retrieved_doc_embeds, dim=-1)\n",
    "\n",
    "# Squeeze out any unnecessary singleton dimensions for further processing\n",
    "doc_scores_cosine = doc_scores_cosine.squeeze(-1)  # Shape should now be [1, 5]\n",
    "\n",
    "\n",
    "# Generate the answer conditioned on retrieved documents\n",
    "outputs_cosine = model_result_generation.generate(\n",
    "    input_ids=input_ids,\n",
    "    context_input_ids=context_input_ids,\n",
    "    doc_scores=doc_scores_cosine,\n",
    "    context_attention_mask=docs_dict['context_attention_mask']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Decode the generated output\n",
    "generated_text_cosine = tokenizer_for_retriever.batch_decode(outputs_cosine, skip_special_tokens=True)[0]\n",
    "print(generated_text_cosine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III-A.6.2. RESPONSE BASED ON EUCLIDEAN DISTANCE WITH RETRIEVED DOCUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " berlin\n"
     ]
    }
   ],
   "source": [
    "# Ensure that retrieved_doc_embeds is squeezed along the third dimension (singleton) before cdist\n",
    "doc_scores_euclidean = torch.cdist(question_hidden_states.unsqueeze(1), retrieved_doc_embeds.squeeze(2), p=2)  # Shape: [1, 5]\n",
    "\n",
    "# Remove the extra singleton dimension if present to get shape [1, 5]\n",
    "doc_scores_euclidean = doc_scores_euclidean.squeeze(1) \n",
    "\n",
    "\n",
    "\n",
    "# Generate the answer conditioned on retrieved documents\n",
    "outputs_euclidean = model_result_generation.generate(\n",
    "                         input_ids=input_ids, \n",
    "                         context_input_ids=context_input_ids, \n",
    "                         doc_scores=doc_scores_euclidean,\n",
    "                         context_attention_mask = docs_dict['context_attention_mask']\n",
    "                         )\n",
    "\n",
    "\n",
    "# Decode the generated output\n",
    "generated_text_euclidean = tokenizer_for_retriever.batch_decode(outputs_euclidean, skip_special_tokens=True)[0]\n",
    "print(generated_text_euclidean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III-A.6.3. RESPONSE BASED ON DOT PRODUCT WITH RETRIEVED DOCUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " berlin\n"
     ]
    }
   ],
   "source": [
    "# Ensure the shapes are correct before batch matrix multiplication\n",
    "question_hidden_states_unsq = question_hidden_states.unsqueeze(1) # Shape: (batch_size, 1, embed_dim)\n",
    "\n",
    "retrieved_doc_embeds_squeezed = retrieved_doc_embeds.squeeze(2)  # Shape: (batch_size, n_docs, embed_dim)\n",
    "\n",
    "\n",
    "# Perform batch matrix multiplication\n",
    "doc_scores_dotprod = torch.bmm(question_hidden_states_unsq, retrieved_doc_embeds_squeezed.transpose(1, 2)).squeeze(1)  # Shape: (batch_size, n_docs)\n",
    "\n",
    "\n",
    "\n",
    "# Generate the answer conditioned on retrieved documents\n",
    "outputs_dotprod = model_result_generation.generate(\n",
    "                         input_ids=input_ids, \n",
    "                         context_input_ids=context_input_ids, \n",
    "                         doc_scores=doc_scores_dotprod,\n",
    "                         context_attention_mask = docs_dict['context_attention_mask']\n",
    "                         )\n",
    "\n",
    "\n",
    "# Decode the generated output\n",
    "generated_text_dotprod = tokenizer_for_retriever.batch_decode(outputs_dotprod, skip_special_tokens=True)[0]\n",
    "print(generated_text_dotprod)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X- APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X.1. - FUNCTION TO EXTRACT MAIN TEXT FROM A WIKIPEDIA PAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_path = os.path.join(os.getcwd(), 'data',\"passages_list_no_nan.zip\")\n",
    "\n",
    "# Open the zip file and extract the target .csv file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    # List all files within the zip to confirm the exact .csv file name if unsure\n",
    "    print(zip_ref.namelist())\n",
    "    \n",
    "    # Read the csv file directly from the zip without extracting it\n",
    "    with zip_ref.open(zip_ref[0]) as file:\n",
    "        mydata = pd.read_csv(file)\n",
    "\n",
    "mydata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_divisors(n):\n",
    "    divisors = []\n",
    "    for i in range(1, int(n**0.5) + 1):\n",
    "        if n % i == 0:  # Check if i is a divisor\n",
    "            divisors.append(i)\n",
    "            if i != n // i:  # Avoid adding the square root twice if n is a perfect square\n",
    "                divisors.append(n // i)\n",
    "    return sorted(divisors)  # Sort the list of divisors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divisors of 307216 : [1, 2, 4, 7, 8, 13, 14, 16, 26, 28, 52, 56, 91, 104, 112, 182, 208, 211, 364, 422, 728, 844, 1456, 1477, 1688, 2743, 2954, 3376, 5486, 5908, 10972, 11816, 19201, 21944, 23632, 38402, 43888, 76804, 153608, 307216]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "number = data_size\n",
    "divisors_of_number1 = find_divisors(number)\n",
    "print(\"Divisors of\", number, \":\", divisors_of_number1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divisors of 38402.0 : [1, 2, 7, 13, 14, 26, 91, 182, 211.0, 422.0, 1477.0, 2743.0, 2954.0, 5486.0, 19201.0, 38402.0]\n"
     ]
    }
   ],
   "source": [
    "#print(data_size/divisor_value)\n",
    "divisor_value = 8\n",
    "divisors_of_number2 = find_divisors(data_size/divisor_value)\n",
    "print(\"Divisors of\", data_size/divisor_value,\n",
    " \":\", divisors_of_number2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO LOAD DATA IN BATCHES FROM AN HD5 FILE\n",
    "import h5py\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class HDF5Dataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.file = h5py.File(file_path, 'r')\n",
    "        self.data = self.file['dataset']  # Adjust to your dataset's key\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]  # Load the data item by index\n",
    "\n",
    "dataset = HDF5Dataset('your_file.h5')\n",
    "dataloader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "for data in dataloader:\n",
    "    # Process your data\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_in_chunks(h5_file_path, dataset_name, chunk_size):\n",
    "    \"\"\"Generator to yield embeddings in chunks from HDF5 file.\"\"\"\n",
    "    \n",
    "    with h5py.File(h5_file_path, 'r') as f:\n",
    "        dataset = f[dataset_name]\n",
    "        total_embeddings = dataset.shape[0]\n",
    "        embedding_dim = dataset.shape[1]\n",
    "\n",
    "        for start in range(0, total_embeddings, chunk_size):\n",
    "            end = min(start + chunk_size, total_embeddings)\n",
    "            embeddings_chunk = dataset[start:end]\n",
    "            yield embeddings_chunk\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_embeddings_to_index(index, h5_file_path, dataset_name, chunk_size):\n",
    "    \"\"\"Load embeddings in chunks and add them to the FAISS index.\"\"\"\n",
    "    for embeddings_chunk in load_embeddings_in_chunks(h5_file_path, dataset_name, chunk_size):\n",
    "        embeddings_chunk = np.array(embeddings_chunk, dtype='float32')  # FAISS requires float32\n",
    "        index.add(embeddings_chunk)  # Add chunk to the FAISS index\n",
    "\n",
    "def search_faiss_index(index, query_vectors, k=5):\n",
    "    \"\"\"Search the FAISS index for the k-nearest neighbors of the query vectors.\"\"\"\n",
    "    query_vectors = np.array(query_vectors, dtype='float32')\n",
    "    distances, indices = index.search(query_vectors, k)\n",
    "    return distances, indices\n",
    "\n",
    "# Parameters\n",
    "h5_file_path = 'path/to/your/embeddings.h5'\n",
    "dataset_name = 'your_dataset_name'  # Dataset within the HDF5 file containing embeddings\n",
    "embedding_dim = 768  # Adjust based on your embeddings' dimensionality\n",
    "chunk_size = 10000  # Adjust chunk size based on available memory\n",
    "index_type = \"Flat\"  # Or use \"IVF\" for larger datasets\n",
    "k = 5  # Number of nearest neighbors to retrieve in search\n",
    "\n",
    "# Initialize and populate FAISS index\n",
    "index = create_faiss_index(embedding_dim, index_type)\n",
    "add_embeddings_to_index(index, h5_file_path, dataset_name, chunk_size)\n",
    "\n",
    "# If using an IVF index, train it with a sample of data before adding embeddings\n",
    "if index_type == \"IVF\" and not index.is_trained:\n",
    "    with h5py.File(h5_file_path, 'r') as f:\n",
    "        training_data = f[dataset_name][:chunk_size]  # Use first chunk for training\n",
    "    index.train(training_data.astype('float32'))\n",
    "\n",
    "# Example search with random query vectors (replace with your own queries)\n",
    "query_vectors = np.random.rand(10, embedding_dim).astype('float32')  # Replace with actual queries\n",
    "distances, indices = search_faiss_index(index, query_vectors, k)\n",
    "\n",
    "# Output results\n",
    "print(\"Nearest neighbors' indices:\", indices)\n",
    "print(\"Nearest neighbors' distances:\", distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# USING \"pip install\" in a CONDA ENVIRONMENT INSTALLED PACKAGES ARE TRACKED VIA \"site-packages directory\" BUT NOT REGISTERED IN \"Conda's package manager\" \n",
    "# THUS \"pip list\" sees them BUT \"conda list\" DOES NOT. IN A Conda-managed environment IT IS BEST TO USE \"conda install -c ....\" \n",
    "#pip install tables\n",
    "#conda install -c anaconda pytables\n",
    "\n",
    "# WILL FAIL BECAUSE pd.read_hdf() expects DATASETS INSIDE AN HDF5 FILE TO BE IN A SPECIFIC FORMAT TO BE ABLE TO BE READ INTO a DataFrame., BUT files \n",
    "# created with \"h5py\" may not have THAT STRUCTURE \n",
    "\n",
    "import tables\n",
    "data_size2 = 0\n",
    "#for chunk_df in pd.read_csv(file_path, skiprows=range(1, start_row + 1), chunksize=chunk_size):\n",
    "for chunk in pd.read_hdf(output_path_for_text_passage_embeddings, key=\"dataset\", iterator=True, chunksize=100):  \n",
    "    data_size2 += len(chunk)\n",
    "\n",
    "data_size2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "versatile_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
