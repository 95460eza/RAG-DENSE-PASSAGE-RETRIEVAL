{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I- LIBRARY INSTALLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The PyTorch package is officially named torch on pip\n",
    "\n",
    "# ! pip install transformers datasets torch \n",
    "# ! conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia \n",
    "\n",
    "# ! pip show torch\n",
    "# ! conda install faiss-gpu \n",
    "\n",
    "# ! conda install -c conda-forge datasets --update-deps\n",
    "# ! pip install --upgrade datasets\n",
    "\n",
    "# !pip install pyarrow\n",
    "\n",
    "# beautifulsoup4 is the name to install via pip\n",
    "# !pip install beautifulsoup4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II- LIBRARY IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.78 s, sys: 260 ms, total: 2.04 s\n",
      "Wall time: 4.38 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/mnt/d/ANALYTICS - Continuous Education/RAG'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import gzip\n",
    "import itertools\n",
    "import json\n",
    "import h5py\n",
    "#import tables # After conda install -c anaconda pytables\n",
    "import gc\n",
    "#import zipfile\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "import time\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# bs4 is the module of beautifulsoup4 that contains the BeautifulSoup class.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#import pyarrow as pa\n",
    "#import pyarrow.feather as feather\n",
    "#import pyarrow.parquet as pq\n",
    "#import pyarrow.ipc as ipc\n",
    "\n",
    "# Hugging Face's Transformers library primarily supports RAG models in PyTorch\n",
    "import torch, torchvision, torchaudio\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#  FAISS MEANS \"Facebook AI Similarity Search\"\n",
    "# TO CRREATE INDECES ON TEXT KNOWLED BASE\n",
    "import faiss\n",
    "\n",
    "# DPR MEANS Dense Passage Retrieval (ENCODE QUERY WITH KNOWLEDGE BASE)\n",
    "from transformers import DPRQuestionEncoderTokenizer, DPRQuestionEncoder, RagRetriever, RagTokenizer, RagTokenForGeneration\n",
    "from transformers import BartTokenizerFast, DPRQuestionEncoderTokenizerFast\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III-A.1. CREATE AS \"TAB-SAPARATED-VALUE (TSV)\" FILE THE TEXT PASSAGES (AKA KNOWLEDGE BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATOR FUNCTION FOR \"CHUNK-LOADING\" DATA FROM A FILE (HERE ZIPPED)\n",
    "# USING \"itertools.islice\"\n",
    "\n",
    "def chunks_generator_for_gz_zipped_file(data_path, chunk_size):\n",
    "\n",
    "    with gzip.open(data_path, 'rt', encoding='utf-8') as f:\n",
    "\n",
    "        # READ ONLY A NUMBER OF ROWS FROM THE SAVED-TO-DISK DATASET THAT IS EQUAL TO \"chunk_size\"\n",
    "        #chunk_count = 0 \n",
    "        for chunk in iter(lambda: list(itertools.islice(f, chunk_size)), []): \n",
    "             \n",
    "            # LIMIT THE NUMBER OF CHUNKS TO READ FROM THE SAVED-TO-DISK DATASET\n",
    "            #if chunk_count >= 10:  \n",
    "            #    break            \n",
    "            \n",
    "            # YIELD ONLY EXIST WITHIN A FUNCTION\n",
    "            yield chunk\n",
    "            #chunk_count += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO EXTRACT \"TEXT PASAGES\" FROM AN  HTML STRING ALREADY EXISTING IN THE DATASET\n",
    "\n",
    "def extract_text_from_wikipedia_html(document_text, row_number, modulo_for_print):    \n",
    "    \n",
    "    paragraphs_as_content_div_find_all_p_is_empty={}\n",
    "    \n",
    "    #print(document_text)\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(document_text, 'html.parser')\n",
    "    content_div = soup.find_all('p')     \n",
    "    #print(\"content_div\", content_div, type(content_div))\n",
    "    \n",
    "    if content_div is None:                  \n",
    "          print(\"Content section 'mw-content-text' is None for Row number:\", row_number)          \n",
    "          text_content = \" \"\n",
    "          return text_content        \n",
    "\n",
    "    cleaned_paragraphs = []\n",
    "\n",
    "     # Loop through the content and extract text from paragraphs\n",
    "    for element in content_div:\n",
    "          #text = element.get_text(strip=True)\n",
    "          text = element.get_text(strip=False)                          \n",
    "          #print(text)\n",
    "          # Only append non-empty text\n",
    "          if len(text.strip()) > 0:                 \n",
    "               cleaned_paragraphs.append(text)    \n",
    "\n",
    "     # Combine all paragraphs into a single string\n",
    "    text_content = ' '.join(cleaned_paragraphs)\n",
    "     # Replace newline characters with a space\n",
    "    text_content = text_content.replace(\"\\n\", \" \").strip()\n",
    "     #text_content = text_content.replace(\"\\t\", \" \")\n",
    "\n",
    "    if not len(text_content.strip()) >0:\n",
    "                              \n",
    "          paragraphs_as_content_div_find_all_p_is_empty[row_number]=( content_div)\n",
    "          #print(row_number,\":\", url, \"\\n\", content_div)\n",
    "          print(\"Row number:\",row_number,\"Paragraphs from find('p') is EMPTY:\")\n",
    "\n",
    "     \n",
    "    if row_number % modulo_for_print == 0:\n",
    "     print(f\"Processed row: {row_number}\") \n",
    "                   \n",
    "    \n",
    "    return text_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO \"EFFICIENTLY\" CREATE IN \"BATCHES\" THE TAB-SEPARATED (TSV) FILE\n",
    "# CONTAINING THE TEXT PASSAGES (AKA KNOWLEDGE BASE OR CONTEXT)\n",
    "\n",
    "def batches_append_to_tsv(data_generator, output_path, batch_size, modulo_for_print):  \n",
    "\n",
    "    batch = []       \n",
    "    row_num = []                \n",
    "\n",
    "    # chunk_created_passages_list =[extract_text_from_wikipedia_html(document_text, \n",
    "    #                                                 text_position_in_chunk+chunk_size*chunk_number, \n",
    "    #                                                 modulo_for_print)  for chunk_number, chunk in enumerate(data_generator)\n",
    "    #                                                                    # ONLY WHEN THE CONDTION BELOW IS MET WILL THE NEXT \"FOR\" BE ENTERED AND RETURN THE FUNCTION\n",
    "    #                                                                    #if chunk_number <= 2  # This acts like a break condition                                                         \n",
    "    #                                                                    for text_position_in_chunk, document_text in enumerate(chunk)                                               \n",
    "    #                                                                                                                                  ]\n",
    "\n",
    "    for chunk_number, chunk in enumerate(data_generator):\n",
    "          \n",
    "          for text_position_in_chunk, document_text in enumerate(chunk):\n",
    "                \n",
    "                chunk_created_passages_list =[extract_text_from_wikipedia_html(document_text, \n",
    "                                                                text_position_in_chunk+chunk_size*chunk_number, \n",
    "                                                                modulo_for_print)\n",
    "                                                ]\n",
    "                \n",
    "                batch = batch + chunk_created_passages_list  \n",
    "                row_num.append(text_position_in_chunk+chunk_size*chunk_number)\n",
    "\n",
    "                     \n",
    "          \n",
    "          if len(batch) > batch_size:\n",
    "\n",
    "            #print(row_num)\n",
    "            #print(len(batch))\n",
    "            \n",
    "            data_created = zip(row_num,batch)\n",
    "            # Filter out tuples where the second element is missing or empty\n",
    "            data_created = [tup for tup in data_created if tup[1]]            \n",
    "\n",
    "            # Convert the list to a DataFrame             \n",
    "            df_batch = pd.DataFrame(data_created) \n",
    "            #df.to_csv(output_path, sep='\\t', mode='a', header=not pd.io.common.file_exists(output_path), index=False)\n",
    "            df_batch.to_csv(output_path, sep='\\t', mode='a', header=None, index=False)\n",
    "            \n",
    "            # Clear the batch and row numbers after writing to the file\n",
    "            row_num.clear()\n",
    "            batch.clear()\n",
    "            chunk_created_passages_list.clear()\n",
    "            del data_created\n",
    "            del df_batch\n",
    "            gc.collect()     \n",
    "          \n",
    "          \n",
    "    if len(batch) > 0:\n",
    "         \n",
    "         #print(row_num)\n",
    "         #print(len(batch))\n",
    "\n",
    "         data_created = zip(row_num,batch)\n",
    "         # Filter out tuples where the second element is missing or empty\n",
    "         data_created = [tup for tup in data_created if tup[1]]            \n",
    "\n",
    "         # Convert the list to a DataFrame             \n",
    "         df_batch = pd.DataFrame(data_created) \n",
    "         #df.to_csv(output_path, sep='\\t', mode='a', header=not pd.io.common.file_exists(output_path), index=False)\n",
    "         df_batch.to_csv(output_path, sep='\\t', mode='a', header=None, index=False)\n",
    "      \n",
    "         # Clear the batch and row numbers after writing to the file\n",
    "         row_num.clear()\n",
    "         batch.clear()\n",
    "         chunk_created_passages_list.clear()\n",
    "         del data_created\n",
    "         del df_batch\n",
    "         gc.collect()     \n",
    "\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row: 0\n",
      "Row number: 3967 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 5000\n",
      "Row number: 6853 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 7466 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 8986 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 9368 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 9934 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 10000\n",
      "Row number: 12938 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 13860 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 15000\n",
      "Row number: 15297 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 15430 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 16590 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 19403 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 19953 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 20000\n",
      "Row number: 21887 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 22796 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 25000\n",
      "Row number: 25166 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 29337 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 29748 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 30000\n",
      "Row number: 30101 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 34588 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 34838 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 35000\n",
      "Row number: 36741 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 40000\n",
      "Row number: 42297 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 42317 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 43205 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 44807 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 45000\n",
      "Row number: 45886 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 50000\n",
      "Row number: 51784 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 52835 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 54830 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 55000\n",
      "Row number: 57454 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 60000\n",
      "Row number: 62097 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 63203 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 64511 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 64515 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 65000\n",
      "Row number: 65477 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 67177 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 67384 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 70000\n",
      "Row number: 70949 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 71695 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 72441 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 73654 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 75000\n",
      "Row number: 75360 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 76347 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 76377 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 76666 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 78552 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 80000\n",
      "Row number: 80337 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 80472 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 83484 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 84889 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 85000\n",
      "Row number: 86521 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 87508 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 90000\n",
      "Row number: 92716 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 93132 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 93596 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 94571 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 95000\n",
      "Processed row: 100000\n",
      "Row number: 102380 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 103046 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 103684 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 104196 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 104343 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 104605 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 105000\n",
      "Row number: 106809 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 108405 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 109355 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 110000\n",
      "Row number: 113043 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 113318 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 115000\n",
      "Row number: 115458 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 118908 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 119580 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 119866 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 120000\n",
      "Row number: 120581 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 125000\n",
      "Row number: 125038 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 126319 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 130000\n",
      "Row number: 131489 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 135000\n",
      "Row number: 137641 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 140000\n",
      "Row number: 140627 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 141127 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 145000\n",
      "Processed row: 150000\n",
      "Row number: 151904 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 155000\n",
      "Row number: 155385 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 157678 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 157855 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 160000\n",
      "Row number: 164881 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 165000\n",
      "Processed row: 170000\n",
      "Row number: 170452 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 171357 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 171536 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 175000\n",
      "Row number: 177309 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 177894 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 178676 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 179661 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 180000\n",
      "Row number: 180047 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 183584 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 185000\n",
      "Row number: 185573 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 185907 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 187138 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 187755 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 189345 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 189775 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 190000\n",
      "Row number: 192349 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 192749 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 195000\n",
      "Row number: 196619 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 199734 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 200000\n",
      "Row number: 200068 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 200675 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 202040 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 204979 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 205000\n",
      "Row number: 205685 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 207267 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 210000\n",
      "Row number: 210534 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 212245 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 215000\n",
      "Row number: 215958 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 219277 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 220000\n",
      "Row number: 221842 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 223081 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 225000\n",
      "Processed row: 230000\n",
      "Row number: 231620 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 233301 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 235000\n",
      "Row number: 238989 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 240000\n",
      "Row number: 240084 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 240105 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 240331 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 240358 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 241241 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 242524 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 245000\n",
      "Row number: 249665 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 250000\n",
      "Row number: 252637 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 253946 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 255000\n",
      "Row number: 255055 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 255175 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 256961 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 259207 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 260000\n",
      "Processed row: 265000\n",
      "Row number: 266022 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 268445 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 269075 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 270000\n",
      "Row number: 270233 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 271180 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 271348 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 272723 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 274061 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 274383 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 275000\n",
      "Row number: 275981 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 280000\n",
      "Row number: 280718 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 281549 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 281583 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 282404 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 282827 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 284496 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 285000\n",
      "Row number: 287855 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 289546 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 290000\n",
      "Row number: 294546 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 294646 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 294666 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 295000\n",
      "Row number: 295334 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 297986 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 299413 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 300000\n",
      "Row number: 300435 Paragraphs from find('p') is EMPTY:\n",
      "Row number: 304741 Paragraphs from find('p') is EMPTY:\n",
      "Processed row: 305000\n",
      "CPU times: user 1h 24min 12s, sys: 6.93 s, total: 1h 24min 19s\n",
      "Wall time: 1h 30min 48s\n"
     ]
    }
   ],
   "source": [
    "# RUN FUNCTION ABOVE\n",
    "%%time\n",
    "\n",
    "#file_path_to_google_nq_dataset = os.path.join(os.getcwd(), \"data\",'v1.0-simplified_simplified-nq-train.jsonl.gz')\n",
    "file_path_to_google_nq_dataset = os.path.join(os.getcwd(), \"data\",'google_nq_simplified_dataset_v1.gz') \n",
    "chunk_size =300\n",
    "tsv_file_path1 = os.path.join(os.getcwd(), \"data\",\"text_passages.tsv\")\n",
    "batch_size=4000\n",
    "modulo_for_print=5000\n",
    "\n",
    "batches_append_to_tsv( data_generator = chunks_generator_for_gz_zipped_file(data_path = file_path_to_google_nq_dataset, chunk_size = chunk_size),\n",
    "                       output_path=tsv_file_path1,\n",
    "                       batch_size=batch_size,\n",
    "                       modulo_for_print = modulo_for_print\n",
    "                       \n",
    "                       )\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.6 s, sys: 1.99 s, total: 27.6 s\n",
      "Wall time: 1min 53s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "307216"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tsv_file_path1 = os.path.join(os.getcwd(), \"data\",\"text_passages.tsv\")\n",
    "data_size = 0\n",
    "#for chunk_df in pd.read_csv(file_path, skiprows=range(1, start_row + 1), chunksize=chunk_size):\n",
    "for chunk in pd.read_csv(tsv_file_path1, sep='\\t', names=['id', 'text'], chunksize=100):  \n",
    "    data_size += len(chunk)\n",
    "\n",
    "\n",
    "data_size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III-A.2. EXPLORE BY LOADING CHUNKS FROM SAVED TEXT PASSAGES (IF TOO BIG FOR MEMORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATOR FUNCTION FOR \"CHUNK-LOADING\" DATA FROM A FILE (HERE TSV)\n",
    "\n",
    "def chunks_generator_for_TSV_file_limited(data_path, chunk_size):\n",
    "\n",
    "    chunks_read= []\n",
    "    chunk_count = 0 \n",
    "\n",
    "    # READ ONLY A NUMBER OF ROWS FROM THE SAVED-TO-DISK DATASET THAT IS EQUAL TO \"chunk_size\"\n",
    "    # for chunk_df in pd.read_csv(file_path, skiprows=range(1, start_row + 1), chunksize=chunk_size):\n",
    "    for chunk in pd.read_csv(data_path, sep='\\t', names=['id', 'text'], chunksize=chunk_size):\n",
    "\n",
    "        chunks_read.append(chunk) \n",
    "                  \n",
    "        # LIMIT THE NUMBER OF CHUNKS TO READ FROM THE SAVED-TO-DISK DATASET\n",
    "        if chunk_count >= 50:  \n",
    "                break            \n",
    "        # YIELD ONLY EXIST WITHIN A FUNCTION\n",
    "        yield chunk\n",
    "        chunk_count += 1 \n",
    "    \n",
    "    return chunks_read\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                               text\n",
      "0   0  Email marketing is the act of sending a commer...\n",
      "1   1  Tracy McConnell , better known as `` The Mothe...\n",
      "2   2  Human fertilization is the union of a human eg...\n",
      "3   3  The following is a list of the top National Fo...\n",
      "4   4  The Roanoke Colony ( / \\u02c8ro\\u028a\\u0259\\u0...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_read_df = pd.concat(chunks_generator_for_TSV_file_limited(tsv_file_path1, chunk_size=1), ignore_index=True)\n",
    "print(chunks_read_df.head())\n",
    "\n",
    "# RELEASE MEMORY\n",
    "chunks_read_df = None\n",
    "del chunks_read_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III-A.3. CREATE THE \"INPUTS\" TENSORS AS HD5 FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"\\n TRANSFERT of inputs TO Gpu:\")\n",
    "# inputs_tensor = {key: value.to(device) for key, value in inputs.items()}\n",
    "# print(type(inputs_tensor), len(inputs_tensor))\n",
    "# inputs_tensor.keys()\n",
    "# path_to_inputs_tensor =  os.path.join(os.getcwd(), \"data\",\"inputs_tensor.pth\")\n",
    "\n",
    "# TO SAVE AS TENSOR WITH torch.save() USE EXTENSION \".pt\" (STANDS FOR PYTORCH) WHICH IS TYPICALLY TO SAVE (AKA SERIALIZED) A \"SINGLE\" OR A COLLECTION OF \n",
    "# TENSORS WHILE EXTENSION \".pth\" IS TO SAVE MODELS INCLUDING THEIR ARCHITECTURE ALONG THE WEIGHTS\n",
    "# PyTorch does NOT support APPENDING to a .pt or .pth file. THUS YOU MUST load the existing file, add the new data, and then save it again. \n",
    "# \"NOT PREFERRED\" BECAUSE WILL EVENTUALLY RUN OUT OF MEMORY!!!!\n",
    "#torch.save(inputs_tensor, path_to_inputs_tensor)\n",
    "\n",
    "\n",
    "# TO SAVE AS NUMPY\n",
    "# Convert tensors to numpy arrays and save them\n",
    "#inputs_numpy = {key: value.cpu().detach().numpy() for key, value in inputs_tensor.items()}\n",
    "\n",
    "#path_to_inputs_tensor_as_numpy = os.path.join(os.getcwd(), \"data\",\"inputs_tensor.npy\")\n",
    "\n",
    "# numpy np.save() does NOT support appending data to an existing .npy file AS np.save() overwrites the file AT EACH CALL\n",
    "#YOU MUST load the existing file, add the new data, and then save it again.\n",
    "# \"NOT PREFERRED\" BECAUSE WILL EVENTUALLY RUN OUT OF MEMORY!!!!\n",
    "#np.save(path_to_inputs_tensor_as_numpy, inputs_numpy)\n",
    "\n",
    "\n",
    "# PREFERRED IS TO USE HDF5 (Hierarchical Data Format version 5) FORMAT \n",
    "# FOR EFFICIENT INCREMENTAL APPENDING AND SAVING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inputs_tensor_as_numpy(path_to_df, chunk_size, max_length, batch_size, output_path):\n",
    "    \n",
    "    # INSTANTIATE THE pre-trained TOKENIZER TO BE APPLIED THE TEXT PASSAGES (AKA KNOWLEDGE BASE )\n",
    "    tokenizer_passage_encoder = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')  \n",
    "                                                                            \n",
    "    batch_of_inputs = []    \n",
    "\n",
    "    # for chunk_df in pd.read_csv(file_path, skiprows=range(1, start_row + 1), chunksize=chunk_size):                \n",
    "    for chunk_df in pd.read_csv(path_to_df, sep='\\t', names=['id', 'text'], chunksize=chunk_size):          \n",
    "\n",
    "          # CREATE A LIST WITH AS MANY ELEMENTS AS THE DATAFRAME CHUNK\n",
    "          passages_list = list(chunk_df['text'])\n",
    "\n",
    "          #print(\"\\n CPU compute of inputs:\") \n",
    "          # Hugging Face's DPRQuestionEncoderTokenizer \"CANNOT\" USE GPU. Tokenization is a CPU-bound process \n",
    "          # IT WILL RETURN A DICTIONARY CONTAINING PyTorch TENSORS \n",
    "          inputs = tokenizer_passage_encoder(passages_list, return_tensors='pt', padding=True, max_length=max_length, truncation=True)\n",
    "\n",
    "          # MANY libraries (like NumPy) READ/WRITE EASILY FROM HDF5 FILE AND  NumPy arrays use less memory compared to PyTorch tensors\n",
    "          inputs_numpy = {key: value.detach().numpy() for key, value in inputs.items()}\n",
    "          del inputs\n",
    "          #print(inputs_numpy.keys())\n",
    "\n",
    "          batch_of_inputs.append(inputs_numpy)\n",
    "\n",
    "          if len(batch_of_inputs)>batch_size:\n",
    "               \n",
    "               print(len(batch_of_inputs))\n",
    "               \n",
    "               # \"h5py.File(output_path, 'a')\"  OPENS THE FILE IN  \"append\" mode WITHOUT DELETING EXISTING data OR AUTOMATICALLY \n",
    "               # CREATE IF \"at output_path\"\n",
    "               with h5py.File(output_path, 'a') as f:\n",
    "\n",
    "                    # Iterate over each key in the first dictionary to initialize datasets\n",
    "                    for key in batch_of_inputs[0].keys():\n",
    "\n",
    "                         #print(\"key when batch size met\", key)\n",
    "                         # Stack all data for the current key across dictionaries into a single NumPy array\n",
    "                         data_array = np.stack([d[key] for d in batch_of_inputs])\n",
    "\n",
    "                         # Check if the dataset already exists in the file\n",
    "                         if key in f:\n",
    "\n",
    "                              # Resize the dataset to accommodate the new data\n",
    "                              dataset = f[key]\n",
    "                              dataset.resize((dataset.shape[0] + data_array.shape[0]), axis=0)\n",
    "                              dataset[-data_array.shape[0]:] = data_array\n",
    "                         else:\n",
    "                              # Create a new dataset with expandable dimensions\n",
    "                              f.create_dataset(key, data=data_array, maxshape=(None,) + data_array.shape[1:])\n",
    "\n",
    "               # Clear the batch and row numbers after writing to the file\n",
    "               passages_list.clear()\n",
    "               del inputs_numpy\n",
    "               batch_of_inputs.clear()               \n",
    "               del data_array\n",
    "               gc.collect()          \n",
    "       \n",
    "    if len(batch_of_inputs)>0: \n",
    "                         \n",
    "          print(len(batch_of_inputs))\n",
    "\n",
    "          # \"h5py.File(output_path, 'a')\"  OPENS THE FILE IN  \"append\" mode WITHOUT DELETING EXISTING data OR AUTOMATICALLY \n",
    "          # CREATE IF \"at output_path\"\n",
    "          with h5py.File(output_path, 'a') as f:\n",
    "\n",
    "               # Iterate over each key in the first dictionary to initialize datasets\n",
    "               for key in batch_of_inputs[0].keys():\n",
    "\n",
    "                    #print(\"key when batch size NOT met:\", key, )\n",
    "                    # Stack all data for the current key across dictionaries into a single NumPy array\n",
    "                    data_array = np.stack([d[key] for d in batch_of_inputs])\n",
    "\n",
    "                    # Check if the dataset already exists in the file\n",
    "                    if key in f:\n",
    "\n",
    "                         # Resize the dataset to accommodate the new data\n",
    "                         dataset = f[key]\n",
    "                         dataset.resize((dataset.shape[0] + data_array.shape[0]), axis=0)\n",
    "                         dataset[-data_array.shape[0]:] = data_array\n",
    "                    else:\n",
    "                         # Create a new dataset with expandable dimensions\n",
    "                         f.create_dataset(key, data=data_array, maxshape=(None,) + data_array.shape[1:])\n",
    "\n",
    "          # Clear the batch and row numbers after writing to the file\n",
    "          passages_list.clear()\n",
    "          del inputs_numpy\n",
    "          batch_of_inputs.clear()               \n",
    "          del data_array\n",
    "          gc.collect() \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "11\n",
      "key when batch size met input_ids\n",
      "key when batch size met token_type_ids\n",
      "key when batch size met attention_mask\n",
      "6\n",
      "key when batch size NOT met: input_ids\n",
      "key when batch size NOT met: token_type_ids\n",
      "key when batch size NOT met: attention_mask\n",
      "CPU times: user 1h 41min 24s, sys: 3.78 s, total: 1h 41min 27s\n",
      "Wall time: 1h 42min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "divisor_value=1688\n",
    "#tsv_file_path1 = os.path.join(os.getcwd(), \"data\",\"text_passages.tsv\")\n",
    "chunk_size_for_inputs_tensor_as_numpy = divisor_value \n",
    "max_length = 256\n",
    "batch_size_for_inputs_tensor_as_numpy = 10 \n",
    "path_to_inputs_tensor_as_numpy = os.path.join(os.getcwd(), \"data\",\" inputs_tensors_saved_as_numpy.h5\")\n",
    "\n",
    "\n",
    "\n",
    "create_inputs_tensor_as_numpy(path_to_df=tsv_file_path1, \n",
    "                                 chunk_size=chunk_size_for_inputs_tensor_as_numpy,\n",
    "                                 max_length = max_length,\n",
    "                                 batch_size=batch_size_for_inputs_tensor_as_numpy,\n",
    "                                 output_path=path_to_inputs_tensor_as_numpy,\n",
    "                                 )\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_mask : (182, 1688, 256)\n",
      "input_ids : (182, 1688, 256)\n",
      "token_type_ids : (182, 1688, 256)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(path_to_inputs_tensor_as_numpy, 'r') as f:\n",
    "    for key in f.keys():\n",
    "        dataset = f[key]\n",
    "        print(f\"{key} : {dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III-A.4. CREATE EMBEDDINGS FROM INPUTS TENSORS AS HD5 FILE\n",
    " * An HDF5 (Hierarchical Data Format version 5) file is a binary file format used to store large amounts of data efficiently and in a STRUCTURED way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_passage_embeddings(file_source_path, chunks_grabbed_from_source_h5_file, batch_size, output_path):    \n",
    "                                                                           \n",
    "    # INSTANTIATE THE pre-trained EMBEDDING MODEL FOR KNOWLEDGE BASE (AKA PASSAGES)\n",
    "    model_passages_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')   \n",
    "    model_passages_encoder = model_passages_encoder.to(device)\n",
    "    \n",
    "    batch_of_embeddings = []    \n",
    "\n",
    "    # Open the HDF5 file\n",
    "    with h5py.File(file_source_path, 'r') as f:          \n",
    "\n",
    "    # Iterate over the keys (assuming the keys are 'input_ids', 'attention_mask', etc.)\n",
    "     for key in f.keys()[0]:\n",
    "\n",
    "          dataset = f[key]\n",
    "          total_chunks_in_source_h5_file= dataset.shape[0]\n",
    "          print(f\"\\n THESE ARE THE TOTAL ROWS (CHUNKS) IN INPUT HD5 FILE : {total_chunks_in_source_h5_file} with dataset shape {dataset.shape}\")\n",
    "          #print(f\"\\n THESE ARE THE keys : {f.keys()}\")\n",
    "          # Load data in chunks\n",
    "          for start in range(0, total_chunks_in_source_h5_file, chunks_grabbed_from_source_h5_file):\n",
    "\n",
    "               end = min(start + chunks_grabbed_from_source_h5_file, total_chunks_in_source_h5_file)\n",
    "               #chunk_data = dataset[start:end]\n",
    "               #print(chunk_data.keys())  \n",
    "               \n",
    "               # GRAB INPUTS SAVED AS NUMPY\n",
    "              \n",
    "               \n",
    "               input_ids = f['input_ids'][start:end]             \n",
    "               attention_mask = f['attention_mask'][start:end]            \n",
    "               token_type_ids = f['token_type_ids'][start:end]  \n",
    "               \n",
    "\n",
    "               # CONVERT INPUTS SAVED AS NUMPY TO TENSORS AND THEN MOVE THEM TO GPU\n",
    "               inputs = {'input_ids': torch.tensor(input_ids.reshape(-1, max_length)).to(device),\n",
    "                         'attention_mask': torch.tensor(attention_mask.reshape(-1, max_length)).to(device),\n",
    "                         'token_type_ids':torch.tensor(token_type_ids.reshape(-1, max_length)).to(device),\n",
    "                         }\n",
    "  \n",
    "               # ON CPU\n",
    "               # inputs = {'input_ids': torch.tensor(input_ids),\n",
    "               #           'attention_mask': torch.tensor(attention_mask),\n",
    "               #           'token_type_ids':torch.tensor(token_type_ids)\n",
    "               #           }\n",
    "  \n",
    "\n",
    "               print(\"START EMBEDDINGS\")\n",
    "               # Perform the forward pass WITH gradients DISABLED for efficiency\n",
    "               with torch.no_grad(): \n",
    "                    embeddings = model_passages_encoder(**inputs).pooler_output   \n",
    "\n",
    "               # Convert to NumPy AND Detach and move to CPU \n",
    "               embeddings_numpy = embeddings.detach().cpu().numpy()    \n",
    "               print(\"SHAPE FOR CHUNK OF EMBEDDED DOCS:\", embeddings_numpy.shape)    \n",
    "\n",
    "               batch_of_embeddings.append(embeddings_numpy)\n",
    "               print(\"EMBEDDINGS FINISHED\")               \n",
    "               print(f\"LENGTH OF batch_of_embeddings {len(batch_of_embeddings)}\")    \n",
    "\n",
    "               if len(batch_of_embeddings)>batch_size:\n",
    "                    \n",
    "                    print(len(batch_of_embeddings))                                  \n",
    "\n",
    "                    # \"h5py.File(output_path, 'a')\" OPENS A IN  \"append\" mode WITHOUT DELETING EXISTING data OR AUTOMATICALLY CREATE IT \"at output_path\"\n",
    "                    # IF IT DOES NOT ALREADY EXIST.                    \n",
    "                    with h5py.File(output_path, 'a') as f2:\n",
    "\n",
    "                         # AN HD5 (Hierarchical Data Format version 5) FILE CAN CONTAIN MULTIPLE DATASETS. WE CALL \"dataset\" THE ONE WE ARE CREATING\n",
    "                         # Check if the dataset already exists OTHERWISE creates it with expandable dimensions\n",
    "                         if \"dataset\" not in f2:\n",
    "\n",
    "                              # WHEN INITIALIZING \"dataset\" ALLOW UNLIMITED ROWS VIA \"None\" (HERE EACH ROW WILL BE A CHUNK). \"dataset\" WILL INCREMENTALLY \n",
    "                              # RECEIVE EMBEDDINGS OF DOCUMENTS IN CHUNKS OF NUMPY ARRAYS HAVING EACH THE NUMBER OF \"COLUMNS\" DEFINED BY: \n",
    "                              # batch_of_embeddings[0].shape[1:] WHERE \"batch_of_embeddings[0]\" IS THE FIRST ARRAY IN THE LIST \"batch_of_embeddings\" AND\n",
    "                              # \".shape[1:]\" IS THE SHAPE OF THE THAT ARRAY EXCEPT THE FIRST DIMENSION. THUS IT IS JUST THE NUMBER OF \"COLUMNS\"\n",
    "                              # NOTE: WE SKIPPED GRABBING THE FIRST DIMENSION (# OF ROWS) BECAUSE WE ALREADY ADDRESSED THAT VIA \"NONE\"\n",
    "                                                          \n",
    "                              maxshape = (None,) + batch_of_embeddings[0].shape[1:]\n",
    "                              # CREATE WITH 0 ROWS AND \"batch_of_embeddings[0].shape[1:]\" NUMBER OF COLUMNS\n",
    "                              dset = f2.create_dataset(\"dataset\", shape=(0,) + batch_of_embeddings[0].shape[1:], \n",
    "                                                      maxshape=maxshape, \n",
    "                                                      # SPECIFY THAT EACH CHUNK APPENDED MUST HAVE AS NUMBER OF ROWS: \"batch_of_embeddings[0].shape[0]\"\n",
    "                                                      # CAN BE ANY NUMBER OF YOUR CHOICE. THE SYSTEM INTERNALLY SPLITS THE NUMBER OF ROW RECEIVED TO CREATE\n",
    "                                                      # THE SPECIFY NUMBER OF ROWS IN EACH CHUNK WITH The HDF5 format ALLOWING partial chunk UNITL MORE DATA\n",
    "                                                      # COMES IN LATER\n",
    "                                                      # chunks=((batch_of_embeddings[0].shape[0]),) + \\\n",
    "                                                      chunks=((batch_of_embeddings[0].shape[0])/chunks_grabbed_from_source_h5_file,) + \\\n",
    "                                                     \n",
    "                                                                                                              batch_of_embeddings[0].shape[1:])\n",
    "\n",
    "                         else:\n",
    "                              # Reopen the existing dataset if it already exists\n",
    "                              dset = f2[\"dataset\"]\n",
    "\n",
    "\n",
    "                         # COULD DO USE \"np.stack([np_arr for np_arr in batch_of_embeddings])\" HERE INSTEAD OF THE LOOP BUT NOT SURE MEMORY CAN HANDLE IT!!!!\n",
    "                         # HOWEVER IT WOULD DO BETTER BECAUSE ONLY A SINGLE \"WRITE\" OPERATION BUT WE LOOSE THE FLEXIBILITY OF ADDING \"SMALLER\" CHUNKS IN LATER\n",
    "                         # RUNS. np.vstack() JUST CONCATENATE ALONG axis 0 (all arrays MUST HAVE the same shape except in this dimension) \n",
    "                         # WHILE np.stack() ALSO ADDS A NEW DIMENSION TO THE OUTPUT: INPUT ARRAYS OF (100, 50) GIVE 10, 100, 50).\n",
    "                         # This WOULD BE useful if you want to keep each INPUT ARRAY AS separate SLICE in the stacked result \n",
    "                         # IT WOULD WORK AS FOLLOWS:\n",
    "                         # stacked_array = np.vstack(batch_of_embeddings)\n",
    "                         # Resize dataset to accommodate the stacked data\n",
    "                         #dset.resize((dset.shape[0] + stacked_array.shape[0]), axis=0)\n",
    "                         # Write the entire stacked array to the end of the dataset\n",
    "                         #dset[-stacked_array.shape[0]:] = stacked_array\n",
    "                         for array in batch_of_embeddings:\n",
    "\n",
    "                              # Resize dataset to accommodate new data\n",
    "                              dset.resize((dset.shape[0] + array.shape[0]), axis=0)\n",
    "                              \n",
    "                              # Write data to the end of the dataset\n",
    "                              dset[-array.shape[0]:] = array\n",
    "\n",
    "                    # Clear the batch and row numbers after writing to the file  \n",
    "                    del embeddings                  \n",
    "                    del embeddings_numpy\n",
    "                    batch_of_embeddings.clear()     \n",
    "                    gc.collect()          \n",
    "       \n",
    "    if len(batch_of_embeddings)>0: \n",
    "                              \n",
    "          print(len(batch_of_embeddings))          \n",
    "\n",
    "          with h5py.File(output_path, 'a') as f2:\n",
    "\n",
    "               # AN HD5 (Hierarchical Data Format version 5) FILE CAN CONTAIN MULTIPLE DATASETS. WE CALL \"dataset\" THE ONE WE ARE CREATING\n",
    "               # Check if the dataset already exists OTHERWISE creates it with expandable dimensions\n",
    "               if \"dataset\" not in f2:\n",
    "\n",
    "                    # WHEN INITIALIZING \"dataset\" ALLOW UNLIMITED ROWS VIA \"None\" (HERE EACH ROW WILL BE A CHUNK). \"dataset\" WILL INCREMENTALLY \n",
    "                    # RECEIVE EMBEDDINGS OF DOCUMENTS IN CHUNKS OF NUMPY ARRAYS HAVING EACH THE NUMBER OF \"COLUMNS\" DEFINED BY: \n",
    "                    # batch_of_embeddings[0].shape[1:] WHERE \"batch_of_embeddings[0]\" IS THE FIRST ARRAY IN THE LIST \"batch_of_embeddings\" AND\n",
    "                    # \".shape[1:]\" IS THE SHAPE OF THE THAT ARRAY EXCEPT THE FIRST DIMENSION. THUS IT IS JUST THE NUMBER OF \"COLUMNS\"\n",
    "                    # NOTE: WE SKIPPED GRABBING THE FIRST DIMENSION (# OF ROWS) BECAUSE WE ALREADY ADDRESSED THAT VIA \"NONE\"\n",
    "                                             \n",
    "                    maxshape = (None,) + batch_of_embeddings[0].shape[1:]\n",
    "                    # CREATE WITH 0 ROWS AND \"batch_of_embeddings[0].shape[1:]\" NUMBER OF COLUMNS\n",
    "                    dset = f2.create_dataset(\"dataset\", shape=(0,) + batch_of_embeddings[0].shape[1:], \n",
    "                                        maxshape=maxshape, \n",
    "                                        # SPECIFY THAT EACH CHUNK APPENDED MUST HAVE AS NUMBER OF ROWS: \"batch_of_embeddings[0].shape[0]\"\n",
    "                                        # CAN BE ANY NUMBER OF YOUR CHOICE. THE SYSTEM INTERNALLY SPLITS THE NUMBER OF ROW RECEIVED TO CREATE\n",
    "                                        # THE SPECIFY NUMBER OF ROWS IN EACH CHUNK WITH The HDF5 format ALLOWING partial chunk UNITL MORE DATA\n",
    "                                        # EVENTUALLY COMES IN LATER\n",
    "                                        # chunks=((batch_of_embeddings[0].shape[0]),) + \\\n",
    "                                        chunks=((batch_of_embeddings[0].shape[0])/chunks_grabbed_from_source_h5_file,) + \\\n",
    "                                        \n",
    "                                                                                                              batch_of_embeddings[0].shape[1:])\n",
    "                    \n",
    "\n",
    "               else:\n",
    "                    # # Reopen the existing dataset if it already exists\n",
    "                    dset  = f2[\"dataset\"]\n",
    "\n",
    "\n",
    "               for ind, array in enumerate(batch_of_embeddings):\n",
    "\n",
    "                    print(\"lAST BATCH: LIST ITEM #:\", ind)\n",
    "                    # Resize dataset to accommodate new data\n",
    "                    dset.resize((dset.shape[0] + array.shape[0]), axis=0)\n",
    "                    \n",
    "                    # Write data to the end of the dataset\n",
    "                    dset[-array.shape[0]:] = array\n",
    "\n",
    "          # Clear the batch and row numbers after writing to the file  \n",
    "          del embeddings        \n",
    "          del embeddings_numpy\n",
    "          batch_of_embeddings.clear() \n",
    "          gc.collect() \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " THESE ARE THE TOTAL ROWS (CHUNKS) IN INPUT HD5 FILE : 182 with dataset shape (182, 1688, 256)\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "\n",
      " THESE ARE THE TOTAL ROWS (CHUNKS) IN INPUT HD5 FILE : 182 with dataset shape (182, 1688, 256)\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "\n",
      " THESE ARE THE TOTAL ROWS (CHUNKS) IN INPUT HD5 FILE : 182 with dataset shape (182, 1688, 256)\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 2\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 3\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 4\n",
      "4\n",
      "START EMBEDDINGS\n",
      "SHAPE FOR CHUNK OF EMBEDDED DOCS: (3376, 768)\n",
      "EMBEDDINGS FINISHED\n",
      "LENGTH OF batch_of_embeddings 1\n",
      "1\n",
      "lAST BATCH: item #: 0\n",
      "CPU times: user 1d 6h 26min 29s, sys: 22.7 s, total: 1d 6h 26min 51s\n",
      "Wall time: 1d 6h 59min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "path_to_inputs_tensor_as_numpy = os.path.join(os.getcwd(), \"data\",\"inputs_tensors_saved_as_numpy.h5\")\n",
    "divisor_value=1688\n",
    "max_length=256\n",
    "\n",
    "file_path_for_text_passage_embeddings = path_to_inputs_tensor_as_numpy\n",
    "# NUMBER OF CHUNKS IN SOURCE H5 FILE GRABBED AT ONCE\n",
    "# MUST BE A \"DIVISOR\" OF data_size/divisor_value:  [1, 2, 7, 13, 14.0, 26.0, 91.0, 182.0]\n",
    "chunks_grabbed_from_source_h5_file =  2\n",
    "batch_size=3\n",
    "output_path_for_text_passage_embeddings = os.path.join(os.getcwd(), \"data\",\"text_passage_embeddings.h5\")\n",
    "\n",
    "\n",
    "\n",
    "create_text_passage_embeddings(file_source_path=file_path_for_text_passage_embeddings,\n",
    "                               chunks_grabbed_from_source_h5_file=chunks_grabbed_from_source_h5_file,\n",
    "                               batch_size=batch_size,\n",
    "                               output_path=output_path_for_text_passage_embeddings,\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset : (307216, 768)\n"
     ]
    }
   ],
   "source": [
    "#output_path_for_text_passage_embeddings = os.path.join(os.getcwd(), \"data\",\"text_passage_embeddings.h5\")\n",
    "\n",
    "with h5py.File(output_path_for_text_passage_embeddings, 'r') as f:\n",
    "\n",
    "    for key in f.keys():\n",
    "\n",
    "        dataset = f[key]\n",
    "        print(f\"{key} : {dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III-A.5. CREATE FAISS INDECES FROM TEXT PASSAGES EMBEDDINGS\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda:0')\n",
    "\n",
    "def create_faiss_IndexIVFFlat_index(file_path_for_embeddings_hd5, indeces_path, chunk_size, divisor, number_cluster_for_embeddings=100, \n",
    "                                    sample_embeddings_size=50000):\n",
    "\n",
    "    # Open the HDF5 file\n",
    "    with h5py.File(file_path_for_embeddings_hd5, 'r') as f: \n",
    "\n",
    "    # Iterate over the keys associated with the different datasets inside the HD5 file\n",
    "     for key in f.keys():  \n",
    "\n",
    "        # Grabs iteratively each dataset \n",
    "        dataset = f[key]\n",
    "\n",
    "        # Dimensionality of embeddings\n",
    "        embeddings_vectors_length = dataset.shape[1]\n",
    "\n",
    "        # NUMBER OF VECTORS EMBEDDINGS (ROWS)\n",
    "        total_embeddings = dataset.shape[0]\n",
    "        \n",
    "        # IF EMBEDDINGS ARE \"NORMALIZED\":\n",
    "        # Initialize a quantizer\n",
    "        # quantizer = faiss.IndexFlatIP(d) \n",
    "        # Initialize the IVF index\n",
    "        # index_cpu = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_COSINE)  \n",
    "              \n",
    "        quantizer = faiss.IndexFlatL2(embeddings_vectors_length) \n",
    "        # Initialize the IVF index\n",
    "        # \"number_cluster_for_embeddings\" IS THE Number of clusters INTO WHICH that ALL THE EMBEDDINGS IN THE SOURCE FILE will be partitioned.\n",
    "        index_cpu = faiss.IndexIVFFlat(quantizer, embeddings_vectors_length, number_cluster_for_embeddings, faiss.METRIC_L2)\n",
    "\n",
    "        # \"index.nprobe\" IS typically set after the index has been created and before performing a search\n",
    "        # \"index.nprobe\" SPECIFIES HOW MANY OF THE CLUSTERS MADE USING THE EMBEDDINGS WILL BE INCLUDED FOR A SEARCH, balancing EFFICIENCY and ACCUARCY.                 \n",
    "        # BY Setting nprobe equal to nlist the index will search \"ALL\" clusters in the index TO MAXIMIZE \"RECALL\".\n",
    "        index_cpu.nprobe = number_cluster_for_embeddings\n",
    "\n",
    "        # Transfer CPU index to GPU (device 0)\n",
    "        try:            \n",
    "            #print(\"FAISS-GPU resources are available.\")\n",
    "            gpu_resources = faiss.StandardGpuResources()\n",
    "            index = faiss.index_cpu_to_gpu(gpu_resources, 0, index_cpu)  \n",
    "        except AttributeError:\n",
    "            print(f\" FAISS-GPU resources are not available. Ensure the GPU version of FAISS is installed.\", \"\\n\")\n",
    "        \n",
    "        \n",
    "        # SPECIFY THE SIZE OF EMBEDDINGS SAMPLE TO BE USED FOR \"TRAINING\" the index\n",
    "        sample_embeddings = dataset[:sample_embeddings_size]          \n",
    "        index.train(sample_embeddings)\n",
    "\n",
    "        # TRANSFER BACK THE INDEX TO CPU BEFORE SAVINGS\n",
    "        index_cpu = faiss.index_gpu_to_cpu(index)\n",
    "\n",
    "        # Set index to use disk storage\n",
    "        # If the index size exceeds available MEMORY, FAISS AUTOMATICALLY WILL USE disk-backed storage.        \n",
    "        faiss.write_index(index_cpu, indeces_path)\n",
    "\n",
    "        index = faiss.index_cpu_to_gpu(gpu_resources, 0, index_cpu)\n",
    "\n",
    "        # Add embeddings in chunks\n",
    "        for start in range(0, total_embeddings, chunk_size):\n",
    "            end = min(start + chunk_size, total_embeddings)\n",
    "            embeddings_chunk = np.array(dataset[start:end], dtype='float32')\n",
    "            # Add to disk-backed index\n",
    "            index.add(embeddings_chunk)  \n",
    "\n",
    "            # Periodically save to disk\n",
    "            # Save every 10 chunks\n",
    "            if start % (chunk_size * divisor) == 0: \n",
    "\n",
    "                index_cpu = faiss.index_gpu_to_cpu(index) \n",
    "                # USE \"partial_index\" AVOIDS fragmentation AND unnecessary I/O on THE \"FINAL\" file\n",
    "                faiss.write_index(index_cpu, os.path.join(os.getcwd(),\"data\",\"partial_index.faiss\"))\n",
    "                print(f\"Partial Index saved at chunk starting at row: {start} and ending at row: {end}\")\n",
    "                index = faiss.index_cpu_to_gpu(gpu_resources, 0, index_cpu)\n",
    "\n",
    "        index_cpu = faiss.index_gpu_to_cpu(index) \n",
    "        # Final save to ensure all embeddings are written\n",
    "        faiss.write_index(index_cpu, indeces_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index saved at chunk 0 to 76804\n"
     ]
    }
   ],
   "source": [
    "output_path_for_text_passage_embeddings = os.path.join(os.getcwd(), \"data\",\"text_passage_embeddings.h5\")\n",
    "path_to_save_faiss_indeces= os.path.join(os.getcwd(), \"data\",\"disk_index.faiss\")\n",
    "chunks_of_embeddings = int(data_size/4)\n",
    "divisor = 26\n",
    "number_cluster_for_embeddings = 2048 #Max possible!!!!)\n",
    "sample_embeddings_size = data_size\n",
    "\n",
    "create_faiss_IndexIVFFlat_index(file_path_for_embeddings_hd5=output_path_for_text_passage_embeddings, \n",
    "                                indeces_path=path_to_save_faiss_indeces, \n",
    "                                chunk_size=chunks_of_embeddings,\n",
    "                                divisor = divisor,\n",
    "                                number_cluster_for_embeddings=number_cluster_for_embeddings, \n",
    "                                sample_embeddings_size=sample_embeddings_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III-A.6. RAG MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132868\n",
      "CPU times: user 32.2 s, sys: 1.7 s, total: 33.9 s\n",
      "Wall time: 51.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# FIND MAX LENGHTS ACCROSS ALL TEXT PASSAGES\n",
    "tsv_file_path1 = os.path.join(os.getcwd(), \"data\",\"text_passages.tsv\")\n",
    "\n",
    "#for chunk_df in pd.read_csv(file_path, skiprows=range(1, start_row + 1), chunksize=chunk_size):\n",
    "for chunk in pd.read_csv(tsv_file_path1, sep='\\t', names=['id', 'text'], chunksize=100):  \n",
    "\n",
    "    passages_max_len = 0\n",
    "    column_name = chunk.columns[1]\n",
    "    length_of_texts = [len(row[column_name]) for index, row in chunk.iterrows()]\n",
    "    if passages_max_len < max(length_of_texts):\n",
    "        passages_max_len =  max(length_of_texts)\n",
    "\n",
    "passages_max_len = passages_max_len +10\n",
    "print(passages_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder_containing_text_passages_with_embeddings(text_passages_path, file_path_for_embeddings_hd5, \n",
    "                                                           chunk_size, batch_size, max_len, output_path):\n",
    "\n",
    "\n",
    "    # Open the HDF5 file\n",
    "    with h5py.File(file_path_for_embeddings_hd5, 'r') as f: \n",
    "     \n",
    "\n",
    "    # Iterate over the keys associated with the different datasets inside the HD5 file\n",
    "     for key in f.keys():  \n",
    "\n",
    "        # Grabs iteratively each dataset \n",
    "        dataset = f[key]\n",
    "        total_embeddings = dataset.shape[0]\n",
    "        #print(total_embeddings)\n",
    "\n",
    "        batch_of_dicts = []\n",
    "\n",
    "        #for chunk_df in pd.read_csv(file_path, skiprows=range(1, start_row + 1), chunksize=chunk_size):\n",
    "        for chunk in pd.read_csv(text_passages_path, sep='\\t', names=['id', 'text'], chunksize=chunk_size): \n",
    "            \n",
    "            column_name = chunk.columns[1]\n",
    "            \n",
    "            list_of_dicts_from_chunks = [{column_name: row[column_name], \"title\": 'passage '+ str(index), \"embeddings\": dataset[index]} \n",
    "                                                                                                             for index, row in chunk.iterrows()\n",
    "                                        ] \n",
    "            \n",
    "            batch_of_dicts = batch_of_dicts + list_of_dicts_from_chunks\n",
    "            #print(len(batch_of_dicts))         \n",
    "              \n",
    "\n",
    "            if len(batch_of_dicts)> batch_size:\n",
    "                        \n",
    "                print(len(batch_of_dicts)) \n",
    "\n",
    "                # 'S100' is for a fixed-length ASCII bytes USED FOR STRINGS (HERE 100 BYTES)\n",
    "                # ('f4', (chunk_size, 768)) is for  N-D numpy arrays with shape (chunk_size, 768) of float32 ('f4') type\n",
    "                vector_sizes = batch_of_dicts[0][\"embeddings\"].shape[0]\n",
    "                #print(vector_sizes)\n",
    "\n",
    "                dtype = np.dtype([(column_name, 'S' + str(max_len)),  ('title', 'S' + str(30)),  ('embeddings', ('f4', (1, vector_sizes))) ])\n",
    "\n",
    "                #structured_array = np.array([tuple(d.values()) for d in batch_of_dicts], dtype=dtype)\n",
    "                structured_array = np.array([(d[column_name].encode('utf-8'), d[\"title\"].encode('utf-8'), d[\"embeddings\"]) \n",
    "                                                                                                                      for d in batch_of_dicts], dtype=dtype)\n",
    "                             \n",
    "\n",
    "                # \"h5py.File(output_path, 'a')\" OPENS A IN  \"append\" mode WITHOUT DELETING EXISTING data OR AUTOMATICALLY CREATE IT \"at output_path\"\n",
    "                # IF IT DOES NOT ALREADY EXIST.                    \n",
    "                with h5py.File(output_path, 'a') as f2:\n",
    "\n",
    "                        # AN HD5 (Hierarchical Data Format version 5) FILE CAN CONTAIN MULTIPLE DATASETS. WE CALL \"dataset\" THE ONE WE ARE CREATING\n",
    "                        # Check if the dataset already exists OTHERWISE creates it with expandable dimensions\n",
    "                        if \"dataset\" not in f2:\n",
    "\n",
    "                            # WHEN INITIALIZING \"dataset\" ALLOW UNLIMITED ROWS VIA \"None\" (HERE EACH ROW WILL BE A CHUNK). \"dataset\" WILL INCREMENTALLY \n",
    "                            # RECEIVE EMBEDDINGS OF DOCUMENTS IN CHUNKS OF NUMPY ARRAYS HAVING EACH THE NUMBER OF \"COLUMNS\" DEFINED BY: \n",
    "                            # batch_of_embeddings[0].shape[1:] WHERE \"batch_of_embeddings[0]\" IS THE FIRST ARRAY IN THE LIST \"batch_of_embeddings\" AND\n",
    "                            # \".shape[1:]\" IS THE SHAPE OF THE THAT ARRAY EXCEPT THE FIRST DIMENSION. THUS IT IS JUST THE NUMBER OF \"COLUMNS\"\n",
    "                            # NOTE: WE SKIPPED GRABBING THE FIRST DIMENSION (# OF ROWS) BECAUSE WE ALREADY ADDRESSED THAT VIA \"NONE\"\n",
    "                                                        \n",
    "                            maxshape = (None,) + structured_array.shape[1:]\n",
    "                            \n",
    "                            # CREATE WITH 0 ROWS AND \"batch_of_embeddings[0].shape[1:]\" NUMBER OF COLUMNS\n",
    "                            dset = f2.create_dataset(\"dataset\", data=structured_array, maxshape=maxshape)\n",
    "\n",
    "                            # Add column names as metadata\n",
    "                            dset.attrs['column_names'] = [column_name, 'title', 'embeddings']\n",
    "                                                  \n",
    "                        else:\n",
    "                            # Reopen the existing dataset if it already exists\n",
    "                            dset = f2[\"dataset\"]\n",
    "                            dset.resize((dset.shape[0] + len(batch_of_dicts)), axis=0)\n",
    "                            dset[-len(batch_of_dicts):] = structured_array\n",
    "\n",
    "\n",
    "                # Clear the batch and row numbers after writing to the file         \n",
    "                list_of_dicts_from_chunks.clear()\n",
    "                batch_of_dicts.clear()    \n",
    "                del structured_array \n",
    "                gc.collect()          \n",
    "\n",
    "    if len(batch_of_dicts)>0: \n",
    "                            \n",
    "        print(len(batch_of_dicts))       \n",
    "\n",
    "        # 'S100' is for a fixed-length ASCII bytes USED FOR STRINGS (HERE 100 BYTES)\n",
    "        # ('f4', (chunk_size, 768)) is for  N-D numpy arrays with shape (chunk_size, 768) of float32 ('f4') type\n",
    "        vector_sizes = batch_of_dicts[0][\"embeddings\"].shape[0]\n",
    "        #print(vector_sizes)\n",
    "\n",
    "        dtype = np.dtype([(column_name, 'S' + str(max_len)),  ('title', 'S' + str(30)),  ('embeddings', ('f4', (1, vector_sizes))) ])\n",
    "        \n",
    "        #structured_array = np.array([tuple(d.values()) for d in batch_of_dicts], dtype=dtype)\n",
    "        structured_array = np.array([(d[column_name].encode('utf-8'), d[\"title\"].encode('utf-8'), d[\"embeddings\"]) \n",
    "                                                                                                                      for d in batch_of_dicts], dtype=dtype)\n",
    "\n",
    "        # \"h5py.File(output_path, 'a')\" OPENS A IN  \"append\" mode WITHOUT DELETING EXISTING data OR AUTOMATICALLY CREATE IT \"at output_path\"\n",
    "        # IF IT DOES NOT ALREADY EXIST.                    \n",
    "        with h5py.File(output_path, 'a') as f2:\n",
    "\n",
    "                # AN HD5 (Hierarchical Data Format version 5) FILE CAN CONTAIN MULTIPLE DATASETS. WE CALL \"dataset\" THE ONE WE ARE CREATING\n",
    "                # Check if the dataset already exists OTHERWISE creates it with expandable dimensions\n",
    "                if \"dataset\" not in f2:\n",
    "\n",
    "                    # WHEN INITIALIZING \"dataset\" ALLOW UNLIMITED ROWS VIA \"None\" (HERE EACH ROW WILL BE A CHUNK). \"dataset\" WILL INCREMENTALLY \n",
    "                    # RECEIVE EMBEDDINGS OF DOCUMENTS IN CHUNKS OF NUMPY ARRAYS HAVING EACH THE NUMBER OF \"COLUMNS\" DEFINED BY: \n",
    "                    # batch_of_embeddings[0].shape[1:] WHERE \"batch_of_embeddings[0]\" IS THE FIRST ARRAY IN THE LIST \"batch_of_embeddings\" AND\n",
    "                    # \".shape[1:]\" IS THE SHAPE OF THE THAT ARRAY EXCEPT THE FIRST DIMENSION. THUS IT IS JUST THE NUMBER OF \"COLUMNS\"\n",
    "                    # NOTE: WE SKIPPED GRABBING THE FIRST DIMENSION (# OF ROWS) BECAUSE WE ALREADY ADDRESSED THAT VIA \"NONE\"\n",
    "                                                \n",
    "                    maxshape = (None,) + structured_array.shape[1:]\n",
    "                    \n",
    "                    # CREATE WITH 0 ROWS AND \"batch_of_embeddings[0].shape[1:]\" NUMBER OF COLUMNS\n",
    "                    dset = f2.create_dataset(\"dataset\", data=structured_array, maxshape=maxshape)\n",
    "\n",
    "                    # Add column names as metadata\n",
    "                    dset.attrs['column_names'] = [column_name, 'title', 'embeddings']\n",
    "                                            \n",
    "                else:\n",
    "                    # Reopen the existing dataset if it already exists\n",
    "                    dset = f2[\"dataset\"]\n",
    "                    dset.resize((dset.shape[0] + len(batch_of_dicts)), axis=0)\n",
    "                    dset[-len(batch_of_dicts):] = structured_array\n",
    "\n",
    "\n",
    "        # Clear the batch and row numbers after writing to the file         \n",
    "        list_of_dicts_from_chunks.clear()\n",
    "        batch_of_dicts.clear()    \n",
    "        del structured_array \n",
    "        gc.collect()          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_for_text_passage_embeddings = os.path.join(os.getcwd(), \"data\",\"text_passage_embeddings.h5\")\n",
    "chunk_size_for_text_passages_with_embeddings = int(data_size/364)\n",
    "batch_size_for_text_passages_with_embeddings = chunk_size_for_text_passages_with_embeddings*4\n",
    "\n",
    "output_path_for_text_passage_with_embeddings_added = os.path.join(\"/mnt/h/RAG\",\"text_passages_with_embeddings_added.h5\")\n",
    "#output_path_for_text_passage_with_embeddings_added = os.path.join('/home/koffi', 'text_passages_with_embeddings_added.h5')\n",
    "\n",
    "create_folder_containing_text_passages_with_embeddings(tsv_file_path1, output_path_for_text_passage_embeddings, \n",
    "                                                       chunk_size=chunk_size_for_text_passages_with_embeddings,\n",
    "                                                       batch_size=batch_size_for_text_passages_with_embeddings,\n",
    "                                                       max_len = passages_max_len,\n",
    "                                                       output_path=output_path_for_text_passage_with_embeddings_added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43888\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848ad47b26c7453a8c54e249707e19a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/43888 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43888\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceedabbfd5364ac2a56c02658dd161b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/43888 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43888\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee284e1e7dcb454bb4f611f4727da05e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/43888 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43888\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8b83e4b0334c44a4297493bf063acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/43888 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43888\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be6a2e59cd547c1884bab9cd6a02ac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/43888 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43888\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05164411b0fe455c86f3509daf6034b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/43888 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43888\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ee67b8196a4afcb6769eacb799c099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/43888 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset, concatenate_datasets\n",
    "\n",
    "def create_dataset_in_chunks(hdf5_file_path, chunk_size, batch_size, output_path_for_dataset_chunks):\n",
    "\n",
    "    with h5py.File(hdf5_file_path, 'r') as f:\n",
    "        \n",
    "        \n",
    "        # Get the dtype of the dataset\n",
    "        dtype =  f[\"dataset\"].dtype          \n",
    "\n",
    "        total_size = f[\"dataset\"].shape[0]  \n",
    "        #print(total_size)  \n",
    "\n",
    "        chunk_num = 0\n",
    "        for start in range(0, total_size, chunk_size):\n",
    "                              \n",
    "               end = min(start + chunk_size, total_size)\n",
    "               \n",
    "               # Read a batch of data\n",
    "               data_chunk = f[\"dataset\"][start:end]\n",
    "\n",
    "               chunk_num +=1\n",
    "            \n",
    "               list_of_dicts_from_chunk = [] \n",
    "               # Convert the batch to a list of dictionaries\n",
    "               #list_chunk = [{\"data\": item} for item in data_chunk]\n",
    "               for item in data_chunk:\n",
    "                    \n",
    "                    text = item[dtype.names[0]].decode('utf-8')\n",
    "                    title = item[dtype.names[1]].decode('utf-8')\n",
    "                    embeddings = item[dtype.names[2]]\n",
    "\n",
    "                    row_into_dict = {\"text\":text, \"title\":title, \"embeddings\":embeddings}\n",
    "\n",
    "                    list_of_dicts_from_chunk.append(row_into_dict)\n",
    "                    #print(len(list_of_dicts_from_chunk))\n",
    "                    \n",
    "\n",
    "               if len(list_of_dicts_from_chunk) >batch_size:\n",
    "                    \n",
    "                    print(len(list_of_dicts_from_chunk))\n",
    "                    \n",
    "                    # Create a Dataset from the chunk\n",
    "                    chunk_dataset = Dataset.from_list(list_of_dicts_from_chunk)\n",
    "                         \n",
    "                    # Convert the Dataset to a pandas DataFrame by using the 'to_pandas()' method\n",
    "                    #chunk_df = chunk_dataset.to_pandas() \n",
    "\n",
    "                    # SAVE AS PARQUET FILE\n",
    "                    #chunk_df.to_parquet( os.path.join(output_path_for_dataset_chunks, f'chunk_{chunk_num}.parquet'), index=False) \n",
    "                    chunk_dataset.save_to_disk( os.path.join(output_path_for_dataset_chunks, f'chunk_{chunk_num}'))\n",
    "                    #chunk_dataset.save_to_disk( output_path_for_dataset_chunks)   \n",
    "               \n",
    "               list_of_dicts_from_chunk.clear()\n",
    "               chunk_dataset = None\n",
    "               chunk_df = None\n",
    "               del chunk_dataset, chunk_df\n",
    "               gc.collect\n",
    "          \n",
    "        if len(list_of_dicts_from_chunk) >0:\n",
    "                    \n",
    "                    print(len(list_of_dicts_from_chunk))\n",
    "                    \n",
    "                    # Create a Dataset from the chunk\n",
    "                    chunk_dataset = Dataset.from_list(list_of_dicts_from_chunk)\n",
    "                         \n",
    "                    # Convert the Dataset to a pandas DataFrame by using the 'to_pandas()' method\n",
    "                    #chunk_df = chunk_dataset.to_pandas() \n",
    "\n",
    "                    # SAVE AS PARQUET FILE\n",
    "                    #chunk_df.to_parquet( os.path.join(output_path_for_dataset_chunks, f'chunk_{chunk_num}.parquet'), index=False)  \n",
    "                    chunk_dataset.save_to_disk( os.path.join(output_path_for_dataset_chunks, f'chunk_{chunk_num}'))  \n",
    "                    #chunk_dataset.save_to_disk( output_path_for_dataset_chunks)  \n",
    "\n",
    "                    \n",
    "               \n",
    "        list_of_dicts_from_chunk.clear()\n",
    "        chunk_dataset = None\n",
    "        chunk_df = None\n",
    "        del chunk_dataset, chunk_df\n",
    "        gc.collect\n",
    "\n",
    "                              \n",
    "                    \n",
    "output_folder_for_dataset_chunks = os.path.join(\"/mnt/h/RAG/passages\")\n",
    "create_dataset_in_chunks(output_path_for_text_passage_with_embeddings_added, chunk_size=43888,batch_size=43887, \n",
    "                         output_path_for_dataset_chunks = output_folder_for_dataset_chunks)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06eb0ce3e14a429589b9e0fb17879c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/13 shards):   0%|          | 0/307216 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets\n",
    "chunk_dirs = [f\"/mnt/h/RAG/passages/chunk_{i+1}\" for i in range(7)]\n",
    "datasets = [load_from_disk(chunk_dir) for chunk_dir in chunk_dirs]\n",
    "# Concatenate all chunks into a single dataset\n",
    "combined_dataset = concatenate_datasets(datasets)\n",
    "\n",
    "combined_dataset.save_to_disk(output_folder_for_dataset_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "#not needed: CONVERT FILE WITH PASSAGES TO \"PICKLE\" FORMAT!!!!!!!!\n",
    "#pkl_file_path = os.path.join(os.getcwd(), \"data\",\"passages.pkl\")\n",
    "# Save the list of dictionaries to a .pkl file\n",
    "#with open(pkl_file_path, 'wb') as f:\n",
    "#    pickle.dump(passages_as_list_of_dict, f)\n",
    "\n",
    "# Check if the file exists\n",
    "#if not os.path.exists(pkl_file_path):\n",
    "#    raise FileNotFoundError(f\"The file {pkl_file_path} does not exist.\")\n",
    "\n",
    "\n",
    "# INSTANTIATE THE \"RAG\" the TOKENIZER\n",
    "tokenizer_for_retriever = RagTokenizer.from_pretrained(\"facebook/rag-sequence-nq\")\n",
    "\n",
    "# Example question\n",
    "#question = \"What is machine learning?\"\n",
    "#question = \"What is the capital Deutscheland?\"\n",
    "question = \"Who won the world series in 2020?\"\n",
    "#question = \"what is Berlin?\"\n",
    "\n",
    "# Tokenize the input\n",
    "input_ids = tokenizer_for_retriever(question, return_tensors=\"pt\").input_ids\n",
    "\n",
    "\n",
    "\n",
    "# INSTANTIATE the RAG model\n",
    "model_result_generation = RagTokenForGeneration.from_pretrained(\"facebook/rag-sequence-nq\")\n",
    "\n",
    "\n",
    "# Get hidden states for the question USING \"question_encoder\" \n",
    "# ALSO Disable gradient calculation for inference\n",
    "with torch.no_grad():  \n",
    "    question_hidden_states = model_result_generation.question_encoder(input_ids)[0] \n",
    "\n",
    "\n",
    "# Convert TENSOR to NumPy array BY MOVING IT to CPU and convert to NumPy array (cpu FRAMEWORK)\n",
    "question_hidden_states_np = question_hidden_states.cpu().numpy()\n",
    "\n",
    "output_folder_for_dataset_chunks = os.path.join(\"/mnt/h/RAG/passages\")\n",
    "path_to_save_faiss_indeces= os.path.join(os.getcwd(), \"data\",\"disk_index.faiss\")\n",
    "\n",
    "retriever = RagRetriever.from_pretrained(\"facebook/rag-sequence-nq\",\n",
    "                                         index_name=\"custom\", output_retrieved=False,\n",
    "                                         tokenizer = tokenizer_for_retriever,\n",
    "                                         passages_path=output_folder_for_dataset_chunks, \n",
    "                                         index_path=path_to_save_faiss_indeces\n",
    "                                         )\n",
    "\n",
    "\n",
    "#  Use the retriever to retrieve relevant documents\n",
    "docs_dict = retriever(input_ids, question_hidden_states=question_hidden_states_np, return_tensors=\"pt\")\n",
    "\n",
    "# RETRIEVE EMBEDDINGS OF DOCUMENTS (Assuming you have retrieved embeddings for the documents)\n",
    "retrieved_doc_embeds = docs_dict['retrieved_doc_embeds']  # Shape (n_docs, embed_dim)\n",
    "\n",
    "# GET CONTEXT INPUT IDs\n",
    "n_docs =docs_dict['context_input_ids'].shape[0]\n",
    "batch_size = input_ids.shape[0]\n",
    "context_input_ids = docs_dict['context_input_ids'].reshape(batch_size * n_docs, -1)\n",
    "\n",
    "\n",
    "if 'context_attention_mask' not in docs_dict:\n",
    "    docs_dict['context_attention_mask'] = torch.ones_like(context_input_ids)\n",
    "#print(docs_dict['context_attention_mask'].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III-A.6.1. RESPONSE BASED ON COSINE SIMILARITY WITH RETRIEVED DOCUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " los angeles , california\n"
     ]
    }
   ],
   "source": [
    "# Expand question_hidden_states to [1, 5, 1, 768]\n",
    "expanded_question_hidden_states = question_hidden_states.unsqueeze(1).unsqueeze(2).expand(-1, retrieved_doc_embeds.size(1), retrieved_doc_embeds.size(2), -1)\n",
    "\n",
    "# Compute cosine similarity along the last dimension\n",
    "doc_scores_cosine = F.cosine_similarity(expanded_question_hidden_states, retrieved_doc_embeds, dim=-1)\n",
    "\n",
    "# Squeeze out any unnecessary singleton dimensions for further processing\n",
    "doc_scores_cosine = doc_scores_cosine.squeeze(-1)  # Shape should now be [1, 5]\n",
    "\n",
    "\n",
    "# Generate the answer conditioned on retrieved documents\n",
    "outputs_cosine = model_result_generation.generate(\n",
    "    input_ids=input_ids,\n",
    "    context_input_ids=context_input_ids,\n",
    "    doc_scores=doc_scores_cosine,\n",
    "    context_attention_mask=docs_dict['context_attention_mask']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Decode the generated output\n",
    "generated_text_cosine = tokenizer_for_retriever.batch_decode(outputs_cosine, skip_special_tokens=True)[0]\n",
    "print(generated_text_cosine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III-A.6.2. RESPONSE BASED ON EUCLIDEAN DISTANCE WITH RETRIEVED DOCUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " los angeles dodgers\n"
     ]
    }
   ],
   "source": [
    "# Ensure that retrieved_doc_embeds is squeezed along the third dimension (singleton) before cdist\n",
    "doc_scores_euclidean = torch.cdist(question_hidden_states.unsqueeze(1), retrieved_doc_embeds.squeeze(2), p=2)  # Shape: [1, 5]\n",
    "\n",
    "# Remove the extra singleton dimension if present to get shape [1, 5]\n",
    "doc_scores_euclidean = doc_scores_euclidean.squeeze(1) \n",
    "\n",
    "\n",
    "\n",
    "# Generate the answer conditioned on retrieved documents\n",
    "outputs_euclidean = model_result_generation.generate(\n",
    "                         input_ids=input_ids, \n",
    "                         context_input_ids=context_input_ids, \n",
    "                         doc_scores=doc_scores_euclidean,\n",
    "                         context_attention_mask = docs_dict['context_attention_mask']\n",
    "                         )\n",
    "\n",
    "\n",
    "# Decode the generated output\n",
    "generated_text_euclidean = tokenizer_for_retriever.batch_decode(outputs_euclidean, skip_special_tokens=True)[0]\n",
    "print(generated_text_euclidean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III-A.6.3. RESPONSE BASED ON DOT PRODUCT WITH RETRIEVED DOCUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " los angeles , california\n"
     ]
    }
   ],
   "source": [
    "# Ensure the shapes are correct before batch matrix multiplication\n",
    "question_hidden_states_unsq = question_hidden_states.unsqueeze(1) # Shape: (batch_size, 1, embed_dim)\n",
    "\n",
    "retrieved_doc_embeds_squeezed = retrieved_doc_embeds.squeeze(2)  # Shape: (batch_size, n_docs, embed_dim)\n",
    "\n",
    "\n",
    "# Perform batch matrix multiplication\n",
    "doc_scores_dotprod = torch.bmm(question_hidden_states_unsq, retrieved_doc_embeds_squeezed.transpose(1, 2)).squeeze(1)  # Shape: (batch_size, n_docs)\n",
    "\n",
    "\n",
    "\n",
    "# Generate the answer conditioned on retrieved documents\n",
    "outputs_dotprod = model_result_generation.generate(\n",
    "                         input_ids=input_ids, \n",
    "                         context_input_ids=context_input_ids, \n",
    "                         doc_scores=doc_scores_dotprod,\n",
    "                         context_attention_mask = docs_dict['context_attention_mask']\n",
    "                         )\n",
    "\n",
    "\n",
    "# Decode the generated output\n",
    "generated_text_dotprod = tokenizer_for_retriever.batch_decode(outputs_dotprod, skip_special_tokens=True)[0]\n",
    "print(generated_text_dotprod)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X- APPENDIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO LOAD DATA IN BATCHES FROM AN HD5 FILE\n",
    "import h5py\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class HDF5Dataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.file = h5py.File(file_path, 'r')\n",
    "        self.data = self.file['dataset']  # Adjust to your dataset's key\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]  # Load the data item by index\n",
    "\n",
    "dataset = HDF5Dataset('your_file.h5')\n",
    "dataloader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "for data in dataloader:\n",
    "    # Process your data\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_in_chunks(h5_file_path, dataset_name, chunk_size):\n",
    "    \"\"\"Generator to yield embeddings in chunks from HDF5 file.\"\"\"\n",
    "    \n",
    "    with h5py.File(h5_file_path, 'r') as f:\n",
    "        dataset = f[dataset_name]\n",
    "        total_embeddings = dataset.shape[0]\n",
    "        embedding_dim = dataset.shape[1]\n",
    "\n",
    "        for start in range(0, total_embeddings, chunk_size):\n",
    "            end = min(start + chunk_size, total_embeddings)\n",
    "            embeddings_chunk = dataset[start:end]\n",
    "            yield embeddings_chunk\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_embeddings_to_index(index, h5_file_path, dataset_name, chunk_size):\n",
    "    \"\"\"Load embeddings in chunks and add them to the FAISS index.\"\"\"\n",
    "    for embeddings_chunk in load_embeddings_in_chunks(h5_file_path, dataset_name, chunk_size):\n",
    "        embeddings_chunk = np.array(embeddings_chunk, dtype='float32')  # FAISS requires float32\n",
    "        index.add(embeddings_chunk)  # Add chunk to the FAISS index\n",
    "\n",
    "def search_faiss_index(index, query_vectors, k=5):\n",
    "    \"\"\"Search the FAISS index for the k-nearest neighbors of the query vectors.\"\"\"\n",
    "    query_vectors = np.array(query_vectors, dtype='float32')\n",
    "    distances, indices = index.search(query_vectors, k)\n",
    "    return distances, indices\n",
    "\n",
    "# Parameters\n",
    "h5_file_path = 'path/to/your/embeddings.h5'\n",
    "dataset_name = 'your_dataset_name'  # Dataset within the HDF5 file containing embeddings\n",
    "embedding_dim = 768  # Adjust based on your embeddings' dimensionality\n",
    "chunk_size = 10000  # Adjust chunk size based on available memory\n",
    "index_type = \"Flat\"  # Or use \"IVF\" for larger datasets\n",
    "k = 5  # Number of nearest neighbors to retrieve in search\n",
    "\n",
    "# Initialize and populate FAISS index\n",
    "index = create_faiss_index(embedding_dim, index_type)\n",
    "add_embeddings_to_index(index, h5_file_path, dataset_name, chunk_size)\n",
    "\n",
    "# If using an IVF index, train it with a sample of data before adding embeddings\n",
    "if index_type == \"IVF\" and not index.is_trained:\n",
    "    with h5py.File(h5_file_path, 'r') as f:\n",
    "        training_data = f[dataset_name][:chunk_size]  # Use first chunk for training\n",
    "    index.train(training_data.astype('float32'))\n",
    "\n",
    "# Example search with random query vectors (replace with your own queries)\n",
    "query_vectors = np.random.rand(10, embedding_dim).astype('float32')  # Replace with actual queries\n",
    "distances, indices = search_faiss_index(index, query_vectors, k)\n",
    "\n",
    "# Output results\n",
    "print(\"Nearest neighbors' indices:\", indices)\n",
    "print(\"Nearest neighbors' distances:\", distances)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "versatile_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
