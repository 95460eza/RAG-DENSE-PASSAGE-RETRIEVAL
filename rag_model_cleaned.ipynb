{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [I- LIBRARY INSTALLS](#toc1_)    \n",
    "- [II- LIBRARY IMPORTS](#toc2_)    \n",
    "  - [III - DATA PRE-PROCESSING](#toc2_1_)    \n",
    "      - [III- 1. CREATE THE TEXT PASSAGES (AKA KNOWLEDGE BASE) AS A \"TAB-SAPARATED-VALUE\" (TSV) FILE](#toc2_1_1_1_)    \n",
    "      - [III- 2. BREAK DOWN \"ORIGINAL\" TEXT PASSAGES (AKA KNOWLEDGE BASE) INTO \"SHORTER\" LENGTH ONES](#toc2_1_1_2_)    \n",
    "      - [III- 3. EXPLORE DATASET BY LOADING \"CHUNKS\" FROM SAVED TEXT PASSAGES (IF TOO BIG FOR MEMORY)](#toc2_1_1_3_)    \n",
    "    - [III- 4. CREATE THE \"INPUTS\" TENSORS (AKA TOKENS) OF THE KNWOLEDGE BASE DOCUMENTS AS HD5 FILE](#toc2_1_2_)    \n",
    "    - [III- 5. CREATE EMBEDDINGS FROM INPUTS TENSORS AS HD5 FILE](#toc2_1_3_)    \n",
    "    - [III- 6. CREATE FAISS INDECES FROM TEXT PASSAGES EMBEDDINGS](#toc2_1_4_)    \n",
    "    - [III- 7. EXAMPLE: FINDING \"BY HAND\" SIMILAR DOCUMENTS TO A QUESTION](#toc2_1_5_)    \n",
    "      - [III- 7.1 EMBED THE USER \"PROMPT/QUESTION\" INTO A VECTOR](#toc2_1_5_1_)    \n",
    "      - [III- 7.2 RETRIEVE RELEVANT DOCUMENTS FOR THE USER \"PROMPT/QUESTION\"](#toc2_1_5_2_)    \n",
    "      - [III- 7.3 CHECK THE RELEVANT DOCUMENTS RETRIEVED FOR THE USER \"PROMPT/QUESTION\"](#toc2_1_5_3_)    \n",
    "      - [III- 7.4 COMPARE BART LLM RESPONSE WITH AND WIHTOUT CONTEXT](#toc2_1_5_4_)    \n",
    "    - [IV- ORGANIZE TEXT PASSAGES AND THEIR EMBEDDINGS IN THE FORMAT NEEDED BY THE RAG \"MODEL\"](#toc2_1_6_)    \n",
    "      - [IV- 11.1 JOIN TEXT PASSAGES AND THEIR EMBEDDINGS IN THE SAME FILE](#toc2_1_6_1_)    \n",
    "      - [III- 11.2 SPLIT INTO CHUNKS OF APACHE \"ARROW\" FORMAT FILES THE TEXT PASSAGES AND THEIR EMBEDDINGS](#toc2_1_6_2_)    \n",
    "  - [IV- \"FACEBOOK\" RAG \"MODEL\"](#toc2_2_)    \n",
    "    - [IV- 1. THE RAG \"RETRIEVER\"](#toc2_2_1_)    \n",
    "    - [IV- 2. THE RAG \"GENERATOR\"](#toc2_2_2_)    \n",
    "      - [IV- 2.1 RESPONSE BASED ON COSINE SIMILARITY WITH RETRIEVED DOCUMENTS](#toc2_2_2_1_)    \n",
    "    - [IV- 2.2. RESPONSE BASED ON EUCLIDEAN DISTANCE WITH RETRIEVED DOCUMENTS](#toc2_2_3_)    \n",
    "    - [IV- 2.3. RESPONSE BASED ON DOT PRODUCT WITH RETRIEVED DOCUMENTS](#toc2_2_4_)    \n",
    "- [X- APPENDIX](#toc3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[I- LIBRARY INSTALLS](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The PyTorch package is officially named torch on pip\n",
    "\n",
    "# ! pip install transformers datasets torch \n",
    "# ! conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia \n",
    "\n",
    "# ! pip show torch\n",
    "# ! conda install faiss-gpu \n",
    "\n",
    "# ! conda install -c conda-forge datasets --update-deps\n",
    "# ! pip install --upgrade datasets\n",
    "\n",
    "# !pip install pyarrow\n",
    "\n",
    "# beautifulsoup4 is the name to install via pip\n",
    "# !pip install beautifulsoup4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[II- LIBRARY IMPORTS](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 08:27:44.419255: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-17 08:27:44.986509: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-17 08:27:44.986533: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-17 08:27:44.989168: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-17 08:27:45.231028: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-17 08:27:46.278599: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /mnt/d/ANALYTICS - Continuous Education/RAG\n",
      "\n",
      "Total RAM available 'NOW': 67.36 GB\n",
      "\n",
      "CPU times: user 3.79 s, sys: 984 ms, total: 4.77 s\n",
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Automatically detect and use all available cores\n",
    "import multiprocessing\n",
    "import os\n",
    "import psutil\n",
    "import shutil\n",
    "\n",
    "import gzip\n",
    "import itertools\n",
    "# Dask processes data in parallel without HAVING TO MANUALLY CODE FOR LOADING IN CHUNKS\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask_cuda import LocalCUDACluster\n",
    "import dask_cudf  # GPU-accelerated DataFrames (replaces dask.dataframe)\n",
    "# DASK ALLOWS \"PARALLELIZED\" processing OF DATA IN MULTIPLE \"CHUNKS\" simultaneously ON SEVERAL CPU cores (OR GPU WIRH DASK-CUDA)\n",
    "#from concurrent.futures import ProcessPoolExecutor\n",
    "import cudf\n",
    "import numba\n",
    "from numba import cuda\n",
    "\n",
    "import json\n",
    "import h5py\n",
    "#import tables # After conda install -c anaconda pytables\n",
    "import gc\n",
    "#import zipfile\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "import time\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# bs4 is the module of beautifulsoup4 that contains the BeautifulSoup class.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#import pyarrow as pa\n",
    "#import pyarrow.feather as feather\n",
    "#import pyarrow.parquet as pq\n",
    "#import pyarrow.ipc as ipc\n",
    "\n",
    "# Hugging Face's Transformers library primarily supports RAG models in PyTorch\n",
    "import torch, torchvision, torchaudio\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#  FAISS MEANS \"Facebook AI Similarity Search\"\n",
    "# TO CRREATE INDECES ON TEXT KNOWLED BASE\n",
    "import faiss\n",
    "\n",
    "# DPR MEANS Dense Passage Retrieval (ENCODE QUERY WITH KNOWLEDGE BASE)\n",
    "from transformers import AutoTokenizer, DPRQuestionEncoderTokenizer, DPRContextEncoderTokenizer, DPRQuestionEncoder, DPRContextEncoder,\\\n",
    "                        BartTokenizer, BartForConditionalGeneration, RagRetriever, RagTokenizer, RagTokenForGeneration \n",
    "from transformers import BartTokenizerFast, DPRQuestionEncoderTokenizerFast\n",
    "\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets, load_from_disk\n",
    "\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "\n",
    "print(f\"\\nTotal RAM available 'NOW': {psutil.virtual_memory().total / 1e9:.2f} GB\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_storage_on_disk = '/mnt/f/RAG DATASETS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[III - DATA PRE-PROCESSING](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_1_1_'></a>[III- 1. CREATE THE TEXT PASSAGES (AKA KNOWLEDGE BASE) AS A \"TAB-SAPARATED-VALUE\" (TSV) FILE](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO EXTRACT \"TEXT PASAGES\" FROM AN HTML STRING \"COLUMN\" THAT MUST \"ALREADY\" EXIST IN THE DATASET\n",
    "\n",
    "def extract_text_from_wikipedia_html(document_text, row_number, modulo_for_print):    \n",
    "    \n",
    "    paragraphs_as_content_div_find_all_p_is_empty={}\n",
    "    \n",
    "    #print(document_text)\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(document_text, 'html.parser')\n",
    "    content_div = soup.find_all('p')     \n",
    "    #print(\"content_div\", content_div, type(content_div))\n",
    "    \n",
    "    if content_div is None:                  \n",
    "          print(\"Content section 'mw-content-text' is None for Row number:\", row_number)          \n",
    "          text_content = \" \"\n",
    "          return text_content        \n",
    "\n",
    "    cleaned_paragraphs = []\n",
    "\n",
    "     # Loop through the content and extract text from paragraphs\n",
    "    for element in content_div:\n",
    "          #text = element.get_text(strip=True)\n",
    "          text = element.get_text(strip=False)                          \n",
    "          #print(text)\n",
    "          # Only append non-empty text\n",
    "          if len(text.strip()) > 0:                 \n",
    "               cleaned_paragraphs.append(text)    \n",
    "\n",
    "     # Combine all paragraphs into a single string\n",
    "    text_content = ' '.join(cleaned_paragraphs)\n",
    "     # Replace newline characters with a space\n",
    "    text_content = text_content.replace(\"\\n\", \" \").strip()\n",
    "     #text_content = text_content.replace(\"\\t\", \" \")\n",
    "\n",
    "    if not len(text_content.strip()) >0:\n",
    "                              \n",
    "          paragraphs_as_content_div_find_all_p_is_empty[row_number]=( content_div)\n",
    "          #print(row_number,\":\", url, \"\\n\", content_div)\n",
    "          print(\"Row number:\",row_number,\"Paragraphs from find('p') is EMPTY:\")\n",
    "\n",
    "     \n",
    "    if row_number % modulo_for_print == 0:\n",
    "     print(f\"Processed row: {row_number}\") \n",
    "                   \n",
    "    \n",
    "    return text_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO \"EFFICIENTLY\" CREATE IN \"BATCHES\" THE TAB-SEPARATED (TSV) FILE THAT WILL CONTAIN THE TEXT PASSAGES (AKA KNOWLEDGE BASE OR CONTEXT)\n",
    "\n",
    "def batches_append_to_tsv(data_generator, output_path, batch_size, modulo_for_print):  \n",
    "\n",
    "    batch = []       \n",
    "    row_num = []                \n",
    "\n",
    "    # OPTION 1: USE LIST \"COMPREHENSION\"\n",
    "    # \"chunk_size\" WILL BE PROVIDED VIA \"data_generator\" LATER\n",
    "    # YOU COULD check if it exists before using it VIA chunk_size = getattr(data_generator, \"chunk_size\", None)\n",
    "    # chunk_created_passages_list =[extract_text_from_wikipedia_html(document_text, text_position_in_chunk+ chunk_size*chunk_number, modulo_for_print) \n",
    "    #                                                 # 1st \"FOR\" LOOP \n",
    "    #                                                 for chunk_number, chunk in enumerate(data_generator) \n",
    "    #                                                 # ONLY WHEN THE CONDTION BELOW IS MET WILL THE NEXT \"FOR\" LOOP BE ENTERED AND \"THUS\" RETURN THE FUNCTION\n",
    "    #                                                 #if chunk_number <= 2  # This acts like a break condition                                                         \n",
    "    #                                                 for text_position_in_chunk, document_text in enumerate(chunk)                                               \n",
    "    #                                  ]\n",
    "\n",
    "    # OPTION 2: JUST USE \"FOR\" LOOPS\n",
    "    for chunk_number, chunk in enumerate(data_generator):\n",
    "          \n",
    "          for text_position_in_chunk, document_text in enumerate(chunk):\n",
    "                # \"chunk_size\" WILL BE PROVIDED VIA \"data_generator\" LATER\n",
    "                # YOU COULD check if it exists before using it VIA chunk_size = getattr(data_generator, \"chunk_size\", None)\n",
    "                chunk_created_passages_list =[extract_text_from_wikipedia_html(document_text, text_position_in_chunk+ chunk_size*chunk_number, \n",
    "                                                                               modulo_for_print)\n",
    "                                                ]\n",
    "                \n",
    "                batch = batch + chunk_created_passages_list  \n",
    "                row_num.append(text_position_in_chunk + chunk_size*chunk_number)\n",
    "\n",
    "                     \n",
    "          \n",
    "          if len(batch) > batch_size:\n",
    "\n",
    "            #print(row_num)\n",
    "            #print(len(batch))\n",
    "            \n",
    "            data_created = zip(row_num,batch)\n",
    "            # Filter out tuples where the second element is missing or empty\n",
    "            data_created = [tup for tup in data_created if tup[1]]            \n",
    "\n",
    "            # Convert the list to a DataFrame             \n",
    "            df_batch = pd.DataFrame(data_created) \n",
    "            #df.to_csv(output_path, sep='\\t', mode='a', header=not pd.io.common.file_exists(output_path), index=False)\n",
    "            df_batch.to_csv(output_path, sep='\\t', mode='a', header=None, index=False)\n",
    "            \n",
    "            # Clear the batch and row numbers after writing to the file\n",
    "            row_num.clear()\n",
    "            batch.clear()\n",
    "            chunk_created_passages_list.clear()\n",
    "            del data_created\n",
    "            del df_batch\n",
    "            gc.collect()     \n",
    "          \n",
    "          \n",
    "    if len(batch) > 0:\n",
    "         \n",
    "         #print(row_num)\n",
    "         #print(len(batch))\n",
    "\n",
    "         data_created = zip(row_num,batch)\n",
    "         # Filter out tuples where the second element is missing or empty\n",
    "         data_created = [tup for tup in data_created if tup[1]]            \n",
    "\n",
    "         # Convert the list to a DataFrame             \n",
    "         df_batch = pd.DataFrame(data_created) \n",
    "         #df.to_csv(output_path, sep='\\t', mode='a', header=not pd.io.common.file_exists(output_path), index=False)\n",
    "         df_batch.to_csv(output_path, sep='\\t', mode='a', header=None, index=False)\n",
    "      \n",
    "         # Clear the batch and row numbers after writing to the file\n",
    "         row_num.clear()\n",
    "         batch.clear()\n",
    "         chunk_created_passages_list.clear()\n",
    "         del data_created\n",
    "         del df_batch\n",
    "         gc.collect()     \n",
    "\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATOR FUNCTION FOR \"LOADING\" DATA IN CHUNKS FROM A SAVED FILE (HERE ZIPPED)\n",
    "# USING \"itertools.islice\"\n",
    "\n",
    "def chunks_generator_for_gz_zipped_file(data_path, chunk_size):\n",
    "\n",
    "    with gzip.open(data_path, 'rt', encoding='utf-8') as f:\n",
    "\n",
    "        # READ ONLY A NUMBER OF ROWS FROM THE SAVED-TO-DISK DATASET THAT IS EQUAL TO \"chunk_size\"\n",
    "        #chunk_count = 0 \n",
    "        for chunk in iter(lambda: list(itertools.islice(f, chunk_size)), []): \n",
    "             \n",
    "            # LIMIT THE NUMBER OF CHUNKS TO READ FROM THE SAVED-TO-DISK DATASET\n",
    "            #if chunk_count >= 10:  \n",
    "            #    break            \n",
    "            \n",
    "            # YIELD ONLY EXIST WITHIN A FUNCTION\n",
    "            yield chunk\n",
    "            #chunk_count += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET FILE IS (4.4GB): \"v1.0-simplified_simplified-nq-train.jsonl.gz\". \"I\" RENAMED IT \"google_nq_simplified_dataset_v1.gz\" FOUND AT:\n",
    "# http://storage.cloud.google.com/natural_questions/v1.0-simplified/simplified-nq-train.jsonl.gz\n",
    "\n",
    "file_path_to_google_nq_dataset = os.path.join(datasets_storage_on_disk, \"data\",'google_nq_simplified_dataset_v1.gz') \n",
    "\n",
    "tsv_file_path_original = os.path.join(datasets_storage_on_disk, \"data\",\"text_passages_original.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# RUN THE \"batches_append_to_tsv\" FUNCTION ABOVE\n",
    "\n",
    "\n",
    "chunk_size =300\n",
    "batch_size=4000\n",
    "modulo_for_print=5000\n",
    "\n",
    "batches_append_to_tsv( data_generator = chunks_generator_for_gz_zipped_file(data_path = file_path_to_google_nq_dataset, chunk_size = chunk_size),\n",
    "                       output_path=tsv_file_path_original,\n",
    "                       batch_size=batch_size,\n",
    "                       modulo_for_print = modulo_for_print\n",
    "                       \n",
    "                       )\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19 s, sys: 3.17 s, total: 22.2 s\n",
      "Wall time: 1min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "307216"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data_size_initial = 0\n",
    "#for chunk_df in pd.read_csv(file_path, skiprows=range(1, start_row + 1), chunksize=chunk_size):\n",
    "for chunk in pd.read_csv(tsv_file_path_original, sep='\\t', names=['id', 'text'], chunksize=50000):  \n",
    "    data_size_initial += len(chunk)\n",
    "\n",
    "data_size_initial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_1_2_'></a>[III- 2. BREAK DOWN \"ORIGINAL\" TEXT PASSAGES (AKA KNOWLEDGE BASE) INTO \"SHORTER\" LENGTH ONES](#toc0_)\n",
    "\n",
    "* CONSERVE MEANING THROUGH BOTH OVERLAPPING AND CONTEXTUAL ENCODING/DECODING BEFORE (\"VIA \"AutoTokenizer\") CHUNCKING\n",
    "\n",
    "\n",
    "* Most transformer-based models Have a maximum input size of 512 tokens (chunk_size). With chunk_size=256, stride=128, each chunk is Small enough to fit in the model and Large enough to capture meaningful context\n",
    "\n",
    "* FOR RAG, \"STANDARD\" Dense Passage Retrieval (DPR) MODELS process passages of 100-300 tokens to balance efficiency and retrieval quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Most transformer-based models Have a maximum input size of 512 tokens (chunk_size). With chunk_size=256, stride=128, each chunk is Small enough to fit \n",
    "# in the model and Large enough to capture meaningful context\n",
    "# FOR RAG, \"STANDARD\" Dense Passage Retrieval (DPR) MODELS process passages of 100-300 tokens to balance efficiency and retrieval quality.\n",
    "\n",
    "def sliding_window_chunking(texts, model_name=\"bert-base-uncased\", chunk_size=256, stride=128):\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Ensure 'texts' is a list. If it's a string, convert it into a list with one element.\n",
    "    if isinstance(texts, str):  \n",
    "        texts = [texts]\n",
    "        \n",
    "    all_chunks = []  # TO Store all document chunks\n",
    "    \n",
    "    for text in texts:  # Loop through each document\n",
    "\n",
    "        # Tokenize text into IDs\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)  # No CLS/SEP tokens\n",
    "        #print(\"token length\", len(tokens), \"text:\",text )\n",
    "\n",
    "        # Create overlapping chunks. If you split text into fixed chunks of 256 tokens with no overlap, you risk losing contextual meaning at chunk \n",
    "        # boundaries. For \"stride=128\" the first half of each chunk is the end (overlaps with) of the previous chunk\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(tokens):\n",
    "            chunk = tokens[start : start + chunk_size]\n",
    "            all_chunks.append(tokenizer.decode(chunk))  # Convert back to text\n",
    "            #print(\"chunk\", tokenizer.decode(chunk))\n",
    "            start += stride  # Move window forward\n",
    "    \n",
    "    #print(\"all_chunks LENGTH: \", len(all_chunks), type(all_chunks),\"\\n\", all_chunks)\n",
    "    \n",
    "    return all_chunks\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT USING \"DASK-CUDF\" TO PROCESS ON GPU BECAUSE WITH A DASK-CUDF \"DATAFRAME\", \".map()\" EXPECTS A NUMPY OR NUMBA-COMPATIBLE FUNCTION FOR THE FUNCTION\n",
    "# \"sliding_window_chunking()\" IN: \"df = df.assign(new_col=df[\"col1\"].map(sliding_window_chunking, meta=('new_col', 'object')))\" BELOW:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def break_original_documents_into_smaller_ones(in_path, out_path):\n",
    "\n",
    "\n",
    "    # Configure Dask \"Client\" to use all available cores and memory efficiently\n",
    "    # If you don’t initialize a client, Dask runs in \"single-threaded\" mode (local scheduler) then Computations still run in parallel, but they are limited to\n",
    "    # the threads OF A \"SINGLE\" PROCESSOR. WITH \"client\", Dask switches to \"DISTRIBUTED COMPUTING\" WITH MULTIPLE WORKER PROCESSES, NOT just threads\n",
    "\n",
    "    \n",
    "    #client.close()\n",
    "    #cluster.close()\n",
    "\n",
    "    # Initializing the Client with \"client = Client()\" automatically creates a local cluster under the hood. no need for cluster = LocalCluster()!!\n",
    "    client = Client(\n",
    "    n_workers= multiprocessing.cpu_count(),                # Use all CPU cores as workers\n",
    "    threads_per_worker=1,             # Use 1 thread per worker (prevents oversubscription)\n",
    "    #memory_limit=\"96GB\"                # Use ~75% of 128GB RAM to avoid memory pressure. IF OMITTED WILL USE \"ALL\" AVAILABLE RAM!!!\n",
    "                        )\n",
    "    \n",
    "    print(f\"\\n{client}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    # Read the file into a Dask DataFrame, assigning explicit column names\n",
    "    # \"dd.read_csv()\" loads data \"LAZILY\" and in parallel. It does not load the entire file into memory at once like Pandas' pd.read_csv\n",
    "    df = dd.read_csv(in_path, sep=\"\\t\", header=None, dtype=str, names=[\"col0\", \"col1\"])  # Give names to columns\n",
    "\n",
    "\n",
    "    # Apply my_function to the second column\n",
    "    df = df.assign(new_col=df[\"col1\"].map(sliding_window_chunking, meta=('new_col', 'object')))\n",
    "\n",
    "\n",
    "    # Explode the new column to create new rows\n",
    "    df = df.explode(\"new_col\")\n",
    "\n",
    "\n",
    "    # Save the result as a new TSV file\n",
    "    df[\"new_col\"].to_csv(out_path, sep=\"\\t\", header=False, index=True, single_file=True)\n",
    "\n",
    "    #print(f\"\\nProcessing complete. NEW TSV file created.\")\n",
    "\n",
    "    return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_file_path1 = os.path.join(datasets_storage_on_disk, \"data\", \"text_passages_reduced_length.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "\n",
    "break_original_documents_into_smaller_ones(in_path=tsv_file_path_original, out_path=tsv_file_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.8 s, sys: 1.19 s, total: 44 s\n",
      "Wall time: 2min 10s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8825328"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data_size = 0\n",
    "#for chunk_df in pd.read_csv(file_path, skiprows=range(1, start_row + 1), chunksize=chunk_size):\n",
    "for chunk in pd.read_csv(tsv_file_path1, sep='\\t', names=['id', 'text'], chunksize=50000):  \n",
    "    data_size += len(chunk)\n",
    "\n",
    "data_size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_1_3_'></a>[III- 3. EXPLORE DATASET BY LOADING \"CHUNKS\" FROM SAVED TEXT PASSAGES (IF TOO BIG FOR MEMORY)](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATOR FUNCTION FOR \"CHUNK-LOADING\" DATA FROM A FILE (HERE TSV)\n",
    "\n",
    "def chunks_generator_for_TSV_file_limited(data_path, chunk_size):\n",
    "\n",
    "    chunks_read= []\n",
    "    chunk_count = 0 \n",
    "\n",
    "    # READ ONLY A NUMBER OF ROWS FROM THE SAVED-TO-DISK DATASET THAT IS EQUAL TO \"chunk_size\"\n",
    "    # for chunk_df in pd.read_csv(file_path, skiprows=range(1, start_row + 1), chunksize=chunk_size):\n",
    "    for chunk in pd.read_csv(data_path, sep='\\t', names=['id', 'text'], chunksize=chunk_size):\n",
    "\n",
    "        chunks_read.append(chunk) \n",
    "                  \n",
    "        # LIMIT THE NUMBER OF CHUNKS TO READ FROM THE SAVED-TO-DISK DATASET\n",
    "        if chunk_count >= 50:  \n",
    "                break            \n",
    "        # THE YIELD COMMAND ONLY EXISTS WITHIN A FUNCTION\n",
    "        yield chunk\n",
    "        chunk_count += 1 \n",
    "    \n",
    "    return chunks_read\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 2)\n",
      "   id                                               text\n",
      "0   0  email marketing is the act of sending a commer...\n",
      "1   1  customers to purchase something immediately, a...\n",
      "2   2  blocking out content from emails with filters ...\n",
      "3   3  real time, and to monitor how effective their ...\n",
      "4   4  with a few other narrow definitions of transac...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_read_df = pd.concat(chunks_generator_for_TSV_file_limited(tsv_file_path1, chunk_size=1), ignore_index=True)\n",
    "print(chunks_read_df.shape)\n",
    "print(chunks_read_df.head())\n",
    "\n",
    "sample_chunks = os.path.join(datasets_storage_on_disk, \"data\",\"sample_chunks_\"+str(chunks_read_df.shape[0])+\".tsv\")\n",
    "\n",
    "chunks_read_df.to_csv(sample_chunks, sep='\\t', mode='a', header=True, index=False)\n",
    "\n",
    "# RELEASE MEMORY\n",
    "chunks_read_df = None\n",
    "del chunks_read_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_2_'></a>[III- 4. CREATE THE \"INPUTS\" TENSORS (AKA TOKENS) OF THE KNWOLEDGE BASE DOCUMENTS AS HD5 FILE](#toc0_)\n",
    "\n",
    "* An HDF5 (Hierarchical Data Format version 5) file is a \"BINARY\" file FORMAT used to store large amounts of data efficiently and in a STRUCTURED way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"\\n TRANSFERT of inputs TO Gpu:\")\n",
    "# inputs_tensor = {key: value.to(device) for key, value in inputs.items()}\n",
    "# print(type(inputs_tensor), len(inputs_tensor))\n",
    "# inputs_tensor.keys()\n",
    "# path_to_inputs_tensor =  os.path.join(datasets_storage_on_disk, \"data\",\"inputs_tensor.pth\")\n",
    "\n",
    "# TO SAVE AS TENSOR WITH torch.save() USE EXTENSION \".pt\" (STANDS FOR PYTORCH) WHICH IS TYPICALLY TO SAVE (AKA SERIALIZED) A \"SINGLE\" OR A COLLECTION OF \n",
    "# TENSORS WHILE EXTENSION \".pth\" IS TO SAVE MODELS INCLUDING THEIR ARCHITECTURE ALONG THE WEIGHTS\n",
    "# PyTorch does NOT support APPENDING to a .pt or .pth file. THUS YOU MUST load the existing file, add the new data, and then save it again. \n",
    "# \"NOT PREFERRED\" BECAUSE WILL EVENTUALLY RUN OUT OF MEMORY!!!!\n",
    "#torch.save(inputs_tensor, path_to_inputs_tensor)\n",
    "\n",
    "\n",
    "# TO SAVE AS NUMPY\n",
    "# Convert tensors to numpy arrays and save them\n",
    "#inputs_numpy = {key: value.cpu().detach().numpy() for key, value in inputs_tensor.items()}\n",
    "\n",
    "#path_to_inputs_tensor_as_numpy = os.path.join(datasets_storage_on_disk, \"data\",\"inputs_tensor.npy\")\n",
    "\n",
    "# numpy np.save() does NOT support appending data to an existing .npy file AS np.save() overwrites the file AT EACH CALL\n",
    "#YOU MUST load the existing file, add the new data, and then save it again.\n",
    "# \"NOT PREFERRED\" BECAUSE WILL EVENTUALLY RUN OUT OF MEMORY!!!!\n",
    "#np.save(path_to_inputs_tensor_as_numpy, inputs_numpy)\n",
    "\n",
    "\n",
    "# PREFERRED IS TO USE HDF5 (Hierarchical Data Format version 5) FORMAT \n",
    "# FOR EFFICIENT INCREMENTAL APPENDING AND SAVING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inputs_tensor_as_numpy(path_to_df, chunk_size, max_length, batch_size, output_path):\n",
    "    \n",
    "    # INSTANTIATE THE pre-trained TOKENIZER TO BE APPLIED THE TEXT PASSAGES (AKA KNOWLEDGE BASE )\n",
    "    trained_model_used_in_context_tokenizer = 'facebook/dpr-ctx_encoder-single-nq-base'\n",
    "\n",
    "    #tokenizer_passage_encoder = AutoTokenizer.from_pretrained(trained_model_used_in_context_tokenizer) \n",
    "    #tokenizer_passage_encoder = DPRQuestionEncoderTokenizer.from_pretrained(trained_model_used_in_context_tokenizer)     \n",
    "    # REGARDING WARNING AT EXECUTION TIME: \"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from\"\n",
    "    # Functionally, both DPRQuestionEncoderTokenizer and DPRContextEncoderTokenizer tokenizers are identical, so it doesn’t impact model performance. \n",
    "    tokenizer_passage_encoder = DPRContextEncoderTokenizer.from_pretrained(trained_model_used_in_context_tokenizer) \n",
    "\n",
    "    \n",
    "                                                                            \n",
    "    batch_of_inputs = []    \n",
    "\n",
    "    # for chunk_df in pd.read_csv(file_path, skiprows=range(1, start_row + 1), chunksize=chunk_size):                \n",
    "    for chunk_df in pd.read_csv(path_to_df, sep='\\t', names=['id', 'text'], chunksize=chunk_size):          \n",
    "\n",
    "          # CREATE A LIST WITH AS MANY ELEMENTS AS THE DATAFRAME CHUNK\n",
    "          passages_list = list(chunk_df['text'])\n",
    "          #passages_ids = list(chunk_df['id'])\n",
    "\n",
    "\n",
    "          #print(\"\\n CPU compute of inputs:\") \n",
    "          # Hugging Face's DPRContextEncoderTokenizer \"CANNOT\" USE GPU Because Tokenizations are purely CPU-based operations. \n",
    "          # IT WILL RETURN A DICTIONARY CONTAINING PyTorch TENSORS \n",
    "          inputs = tokenizer_passage_encoder(passages_list, return_tensors='pt', padding=True, max_length=max_length, truncation=True)\n",
    "\n",
    "          # MANY libraries (like NumPy) READ/WRITE EASILY FROM \"HDF5\" STORAGE FILE AND NumPy arrays USE \"LESS\" MEMORY THAN PYTORCH TENSORS, THUS THE \n",
    "          # CONVERSION BELOW\n",
    "          inputs_numpy = {key: value.detach().numpy() for key, value in inputs.items()}\n",
    "          del inputs\n",
    "          #print(inputs_numpy.keys())\n",
    "\n",
    "          batch_of_inputs.append(inputs_numpy)\n",
    "\n",
    "          if len(batch_of_inputs)>batch_size:\n",
    "               \n",
    "               print(len(batch_of_inputs))\n",
    "               \n",
    "               # \"h5py.File(output_path, 'a')\"  OPENS THE FILE IN  \"append\" mode WITHOUT DELETING EXISTING data OR AUTOMATICALLY \n",
    "               # CREATE IF \"at output_path\"\n",
    "               with h5py.File(output_path, 'a') as f:\n",
    "\n",
    "                    # Iterate over each key in the first dictionary to initialize datasets\n",
    "                    for key in batch_of_inputs[0].keys():\n",
    "\n",
    "                         #print(\"key when batch size met\", key)\n",
    "                         # Stack all data for the current key across dictionaries into a single NumPy array\n",
    "                         print(key)\n",
    "                         #data_array = np.stack([d[key] for d in batch_of_inputs])\n",
    "                         data_array = np.concatenate([d[key] for d in batch_of_inputs], axis=0)  # Axis 0 stacks row-wise\n",
    "\n",
    "\n",
    "                         # Check if the dataset already exists in the file\n",
    "                         if key in f:\n",
    "\n",
    "                              # Resize the dataset to accommodate the new data\n",
    "                              dataset = f[key]\n",
    "                              dataset.resize((dataset.shape[0] + data_array.shape[0]), axis=0)\n",
    "                              dataset[-data_array.shape[0]:] = data_array\n",
    "                         else:\n",
    "                              # Create a new dataset with expandable dimensions\n",
    "                              f.create_dataset(key, data=data_array, maxshape=(None,) + data_array.shape[1:])\n",
    "\n",
    "               # Clear the batch and row numbers after writing to the file\n",
    "               passages_list.clear()\n",
    "               del inputs_numpy\n",
    "               batch_of_inputs.clear()               \n",
    "               del data_array\n",
    "               gc.collect()          \n",
    "       \n",
    "    if len(batch_of_inputs)>0: \n",
    "                         \n",
    "          print(len(batch_of_inputs))\n",
    "\n",
    "          # \"h5py.File(output_path, 'a')\"  OPENS THE FILE IN  \"append\" mode WITHOUT DELETING EXISTING data OR AUTOMATICALLY \n",
    "          # CREATE IF \"at output_path\"\n",
    "          with h5py.File(output_path, 'a') as f:\n",
    "\n",
    "               # Iterate over each key in the first dictionary to initialize datasets\n",
    "               for key in batch_of_inputs[0].keys():\n",
    "\n",
    "                    #print(\"key when batch size NOT met:\", key, )\n",
    "                    # Stack all data for the current key across dictionaries into a single NumPy array\n",
    "                    print(key)\n",
    "                    #data_array = np.stack([d[key] for d in batch_of_inputs])\n",
    "                    data_array = np.concatenate([d[key] for d in batch_of_inputs], axis=0)  # Axis 0 stacks row-wise\n",
    "\n",
    "                    # Check if the dataset already exists in the file\n",
    "                    if key in f:\n",
    "\n",
    "                         # Resize the dataset to accommodate the new data\n",
    "                         dataset = f[key]\n",
    "                         dataset.resize((dataset.shape[0] + data_array.shape[0]), axis=0)\n",
    "                         dataset[-data_array.shape[0]:] = data_array\n",
    "                    else:\n",
    "                         # Create a new dataset with expandable dimensions\n",
    "                         f.create_dataset(key, data=data_array, maxshape=(None,) + data_array.shape[1:])\n",
    "\n",
    "          # Clear the batch and row numbers after writing to the file\n",
    "          passages_list.clear()\n",
    "          del inputs_numpy\n",
    "          batch_of_inputs.clear()               \n",
    "          del data_array\n",
    "          gc.collect() \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "divisor_value=15816\n",
    "chunk_size_for_inputs_tensor_as_numpy = divisor_value \n",
    "max_length = 256\n",
    "batch_size_for_inputs_tensor_as_numpy = 10 \n",
    "path_to_inputs_tensor_as_numpy = os.path.join(datasets_storage_on_disk, \"data\",\"inputs_tensors_saved_as_numpy_NEW.h5\")\n",
    "#path_to_inputs_tensor_as_numpy = os.path.join(datasets_storage_on_disk, \"data\",\"inputs_tensors_saved_as_numpy.h5\")\n",
    "\n",
    "\n",
    "\n",
    "create_inputs_tensor_as_numpy(path_to_df=tsv_file_path1, \n",
    "                                 chunk_size=chunk_size_for_inputs_tensor_as_numpy,\n",
    "                                 max_length = max_length,\n",
    "                                 batch_size=batch_size_for_inputs_tensor_as_numpy,\n",
    "                                 output_path=path_to_inputs_tensor_as_numpy,\n",
    "                                 )\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_inputs_tensor_as_numpy = os.path.join(datasets_storage_on_disk, \"data\",\"inputs_tensors_saved_as_numpy_NEW.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_mask : (8825328, 256)\n",
      "input_ids : (8825328, 256)\n",
      "token_type_ids : (8825328, 256)\n",
      "\n",
      "TOTAL TENSORS CREATED (AKA TOKENIZED PASSAGES): 8825328\n",
      "\n",
      "TOTAL PASSAGES ('DATA SIZE') SUBMITTED: 8825328\n",
      "\n",
      "MISSING TENSORS FOR PASSAGES: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with h5py.File(path_to_inputs_tensor_as_numpy, 'r') as f:\n",
    "    for key in f.keys():\n",
    "        dataset = f[key]\n",
    "        print(f\"{key} : {dataset.shape}\")\n",
    "\n",
    "\n",
    "input_tensors_created = dataset.shape[0]\n",
    "\n",
    "print(f\"\\nTOTAL TENSORS CREATED (AKA TOKENIZED PASSAGES): {input_tensors_created}\")\n",
    "print(f\"\\nTOTAL PASSAGES ('DATA SIZE') SUBMITTED: {data_size}\")\n",
    "print(f\"\\nMISSING TENSORS FOR PASSAGES: {data_size-input_tensors_created}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_3_'></a>[III- 5. CREATE EMBEDDINGS FROM INPUTS TENSORS AS HD5 FILE](#toc0_)\n",
    " * An HDF5 (Hierarchical Data Format version 5) file is a binary file format used to store large amounts of data efficiently and in a STRUCTURED way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_passage_embeddings(file_source_path, chunks_grabbed_from_source_h5_file, batch_size, output_path):    \n",
    "                                                                           \n",
    "    trained_model_used_in_context_encoder = 'facebook/dpr-ctx_encoder-single-nq-base'\n",
    "    # INSTANTIATE THE pre-trained EMBEDDING MODEL FOR KNOWLEDGE BASE (AKA PASSAGES)\n",
    "    #model_passages_encoder = DPRQuestionEncoder.from_pretrained(trained_model_used_in_context_encoder)   \n",
    "    model_passages_encoder = DPRContextEncoder.from_pretrained(trained_model_used_in_context_encoder)   \n",
    "\n",
    "    model_passages_encoder = model_passages_encoder.to(device)\n",
    "    \n",
    "    batch_of_embeddings = []    \n",
    "\n",
    "    # Open the HDF5 file\n",
    "    with h5py.File(file_source_path, 'r') as f:          \n",
    "\n",
    "    # Iterate over the keys (assuming the keys are 'input_ids', 'attention_mask', etc.)\n",
    "    # DOES \"NOT\" REALLY LOOP BECAUSE \"f.keys()[0]\" IS JUST A LIST OF 1 ELEMENTS!!!! BAD PROGRAMMING BUT GIVES CORRECT RESULT\n",
    "     for key in list(f.keys())[:1]:\n",
    "          \n",
    "          \n",
    "          dataset = f[key]\n",
    "          total_chunks_in_source_h5_file= dataset.shape[0]\n",
    "          print(f\"\\n THESE ARE THE TOTAL ROWS (CHUNKS) IN INPUT HD5 FILE : {total_chunks_in_source_h5_file} with dataset shape {dataset.shape}\")\n",
    "          #print(f\"\\n THESE ARE THE keys : {f.keys()}\")\n",
    "          # Load data in chunks\n",
    "          for start in range(0, total_chunks_in_source_h5_file, chunks_grabbed_from_source_h5_file):\n",
    "\n",
    "               end = min(start + chunks_grabbed_from_source_h5_file, total_chunks_in_source_h5_file)\n",
    "               #chunk_data = dataset[start:end]\n",
    "               #print(chunk_data.keys())  \n",
    "               \n",
    "               # GRAB INPUTS SAVED AS NUMPY\n",
    "              \n",
    "               \n",
    "               input_ids = f['input_ids'][start:end]             \n",
    "               attention_mask = f['attention_mask'][start:end]            \n",
    "               token_type_ids = f['token_type_ids'][start:end]  \n",
    "               \n",
    "\n",
    "               # CONVERT INPUTS SAVED AS NUMPY TO TENSORS AND THEN MOVE THEM TO GPU BEFORE PERFORMING THE EMBEDDING ON GPU\n",
    "               inputs = {'input_ids': torch.tensor(input_ids.reshape(-1, max_length)).to(device),\n",
    "                         'attention_mask': torch.tensor(attention_mask.reshape(-1, max_length)).to(device),\n",
    "                         'token_type_ids':torch.tensor(token_type_ids.reshape(-1, max_length)).to(device),\n",
    "                         }\n",
    "  \n",
    "               # ON CPU\n",
    "               # inputs = {'input_ids': torch.tensor(input_ids),\n",
    "               #           'attention_mask': torch.tensor(attention_mask),\n",
    "               #           'token_type_ids':torch.tensor(token_type_ids)\n",
    "               #           }\n",
    "  \n",
    "\n",
    "               print(\"START EMBEDDINGS\")\n",
    "               # Perform the forward pass WITH gradients DISABLED for efficiency\n",
    "               with torch.no_grad(): \n",
    "                    embeddings = model_passages_encoder(**inputs).pooler_output   \n",
    "                    # TO SHOW THE \"DIMENSION\" OF THE EMBEDDINGS VECTORS IMPLEMENTED BY \"THIS\" MODEL: \"DPRContextEncoder.from_pretrained(trained_model_used_in_context_encoder)\"\n",
    "                    # FROM EXPERIENCE USING THIS MODEL THE VECTOR HAS DIMENSION 768. WILL BE CONFIRMED FURTHER DOWN\n",
    "                    # print(embeddings.shape)\n",
    "\n",
    "               # Convert to NumPy AFTER Detaching FROM GPU COMPUTATIONAL GRAPH AND MOVING to CPU \n",
    "               # Some models (especially those optimized for GPUs) use float16 for performance BUT OTHER LIBRARIES expect float32 (e.g., FAISS, Scikit-learn, ANN)\n",
    "               # FOR FURTHER PROCESSING THE OUTPUT. THUS, EXPLICITLY \"CASTING\" prevents issues \n",
    "               # embeddings_numpy = embeddings.detach().cpu().numpy().astype(\"float32\")\n",
    "               embeddings_numpy = embeddings.detach().cpu().numpy()    \n",
    "                   \n",
    "               print(\"SHAPE FOR CHUNK OF EMBEDDED DOCS:\", embeddings_numpy.shape)    \n",
    "\n",
    "               batch_of_embeddings.append(embeddings_numpy)\n",
    "               print(\"EMBEDDINGS FINISHED\")               \n",
    "               print(f\"LENGTH OF batch_of_embeddings {len(batch_of_embeddings)}\")    \n",
    "\n",
    "               if len(batch_of_embeddings)>batch_size:\n",
    "                    \n",
    "                    print(len(batch_of_embeddings))                                  \n",
    "\n",
    "                    # \"h5py.File(output_path, 'a')\" OPENS A IN  \"append\" mode WITHOUT DELETING EXISTING data OR AUTOMATICALLY CREATE IT \"at output_path\"\n",
    "                    # IF IT DOES NOT ALREADY EXIST.                    \n",
    "                    with h5py.File(output_path, 'a') as f2:\n",
    "\n",
    "                         # AN HD5 (Hierarchical Data Format version 5) FILE CAN CONTAIN MULTIPLE DATASETS. WE CALL \"dataset\" THE ONE WE ARE CREATING\n",
    "                         # Check if the dataset already exists OTHERWISE creates it with expandable dimensions\n",
    "                         if \"dataset\" not in f2:\n",
    "\n",
    "                              # WHEN INITIALIZING \"dataset\" ALLOW UNLIMITED ROWS VIA \"None\" (HERE EACH ROW WILL BE A CHUNK). \"dataset\" WILL INCREMENTALLY \n",
    "                              # RECEIVE EMBEDDINGS OF DOCUMENTS IN CHUNKS OF NUMPY ARRAYS HAVING EACH THE NUMBER OF \"COLUMNS\" DEFINED BY: \n",
    "                              # batch_of_embeddings[0].shape[1:] WHERE \"batch_of_embeddings[0]\" IS THE FIRST ARRAY IN THE LIST \"batch_of_embeddings\" AND\n",
    "                              # \".shape[1:]\" IS THE SHAPE OF THE THAT ARRAY EXCEPT THE FIRST DIMENSION. THUS IT IS JUST THE NUMBER OF \"COLUMNS\"\n",
    "                              # NOTE: WE SKIPPED GRABBING THE FIRST DIMENSION (# OF ROWS) BECAUSE WE ALREADY ADDRESSED THAT VIA \"NONE\"\n",
    "                                                          \n",
    "                              maxshape = (None,) + batch_of_embeddings[0].shape[1:]\n",
    "                              # CREATE WITH 0 ROWS AND \"batch_of_embeddings[0].shape[1:]\" NUMBER OF COLUMNS\n",
    "                              dset = f2.create_dataset(\"dataset\", shape=(0,) + batch_of_embeddings[0].shape[1:], \n",
    "                                                      maxshape=maxshape, \n",
    "                                                      # SPECIFY THAT EACH CHUNK APPENDED MUST HAVE AS NUMBER OF ROWS: \"batch_of_embeddings[0].shape[0]\"\n",
    "                                                      # CAN BE ANY NUMBER OF YOUR CHOICE. THE SYSTEM INTERNALLY SPLITS THE NUMBER OF ROW RECEIVED TO CREATE\n",
    "                                                      # THE SPECIFY NUMBER OF ROWS IN EACH CHUNK WITH The HDF5 format ALLOWING partial chunk UNITL MORE DATA\n",
    "                                                      # COMES IN LATER\n",
    "                                                      # chunks=((batch_of_embeddings[0].shape[0]),) + \\\n",
    "                                                      chunks=((batch_of_embeddings[0].shape[0])/chunks_grabbed_from_source_h5_file,) + \\\n",
    "                                                     \n",
    "                                                                                                              batch_of_embeddings[0].shape[1:])\n",
    "\n",
    "                         else:\n",
    "                              # Reopen the existing dataset if it already exists\n",
    "                              dset = f2[\"dataset\"]\n",
    "\n",
    "\n",
    "                         # COULD DO USE \"np.stack([np_arr for np_arr in batch_of_embeddings])\" HERE INSTEAD OF THE LOOP BUT NOT SURE MEMORY CAN HANDLE IT!!!!\n",
    "                         # HOWEVER IT WOULD DO BETTER BECAUSE ONLY A SINGLE \"WRITE\" OPERATION BUT WE LOOSE THE FLEXIBILITY OF ADDING \"SMALLER\" CHUNKS IN LATER\n",
    "                         # RUNS. np.vstack() JUST CONCATENATE ALONG axis 0 (all arrays MUST HAVE the same shape except in this dimension) \n",
    "                         # WHILE np.stack() ALSO ADDS A NEW DIMENSION TO THE OUTPUT: INPUT ARRAYS OF (100, 50) GIVE 10, 100, 50).\n",
    "                         # This WOULD BE useful if you want to keep each INPUT ARRAY AS separate SLICE in the stacked result \n",
    "                         # IT WOULD WORK AS FOLLOWS:\n",
    "                         # stacked_array = np.vstack(batch_of_embeddings)\n",
    "                         # Resize dataset to accommodate the stacked data\n",
    "                         #dset.resize((dset.shape[0] + stacked_array.shape[0]), axis=0)\n",
    "                         # Write the entire stacked array to the end of the dataset\n",
    "                         #dset[-stacked_array.shape[0]:] = stacked_array\n",
    "                         for array in batch_of_embeddings:\n",
    "\n",
    "                              # Resize dataset to accommodate new data\n",
    "                              dset.resize((dset.shape[0] + array.shape[0]), axis=0)\n",
    "                              \n",
    "                              # Write data to the end of the dataset\n",
    "                              dset[-array.shape[0]:] = array\n",
    "\n",
    "                    # Clear the batch and row numbers after writing to the file  \n",
    "                    del embeddings                  \n",
    "                    del embeddings_numpy\n",
    "                    batch_of_embeddings.clear()     \n",
    "                    gc.collect()          \n",
    "       \n",
    "    if len(batch_of_embeddings)>0: \n",
    "                              \n",
    "          print(len(batch_of_embeddings))          \n",
    "\n",
    "          with h5py.File(output_path, 'a') as f2:\n",
    "\n",
    "               # AN HD5 (Hierarchical Data Format version 5) FILE CAN CONTAIN MULTIPLE DATASETS. WE CALL \"dataset\" THE ONE WE ARE CREATING\n",
    "               # Check if the dataset already exists OTHERWISE creates it with expandable dimensions\n",
    "               if \"dataset\" not in f2:\n",
    "\n",
    "                    # WHEN INITIALIZING \"dataset\" ALLOW UNLIMITED ROWS VIA \"None\" (HERE EACH ROW WILL BE A CHUNK). \"dataset\" WILL INCREMENTALLY \n",
    "                    # RECEIVE EMBEDDINGS OF DOCUMENTS IN CHUNKS OF NUMPY ARRAYS HAVING EACH THE NUMBER OF \"COLUMNS\" DEFINED BY: \n",
    "                    # batch_of_embeddings[0].shape[1:] WHERE \"batch_of_embeddings[0]\" IS THE FIRST ARRAY IN THE LIST \"batch_of_embeddings\" AND\n",
    "                    # \".shape[1:]\" IS THE SHAPE OF THE THAT ARRAY EXCEPT THE FIRST DIMENSION. THUS IT IS JUST THE NUMBER OF \"COLUMNS\"\n",
    "                    # NOTE: WE SKIPPED GRABBING THE FIRST DIMENSION (# OF ROWS) BECAUSE WE ALREADY ADDRESSED THAT VIA \"NONE\"\n",
    "                                             \n",
    "                    maxshape = (None,) + batch_of_embeddings[0].shape[1:]\n",
    "                    # CREATE WITH 0 ROWS AND \"batch_of_embeddings[0].shape[1:]\" NUMBER OF COLUMNS\n",
    "                    dset = f2.create_dataset(\"dataset\", shape=(0,) + batch_of_embeddings[0].shape[1:], \n",
    "                                        maxshape=maxshape, \n",
    "                                        # SPECIFY THAT EACH CHUNK APPENDED MUST HAVE AS NUMBER OF ROWS: \"batch_of_embeddings[0].shape[0]\"\n",
    "                                        # CAN BE ANY NUMBER OF YOUR CHOICE. THE SYSTEM INTERNALLY SPLITS THE NUMBER OF ROW RECEIVED TO CREATE\n",
    "                                        # THE SPECIFY NUMBER OF ROWS IN EACH CHUNK WITH The HDF5 format ALLOWING partial chunk UNITL MORE DATA\n",
    "                                        # EVENTUALLY COMES IN LATER\n",
    "                                        # chunks=((batch_of_embeddings[0].shape[0]),) + \\\n",
    "                                        chunks=((batch_of_embeddings[0].shape[0])/chunks_grabbed_from_source_h5_file,) + \\\n",
    "                                        \n",
    "                                                                                                              batch_of_embeddings[0].shape[1:])\n",
    "                    \n",
    "\n",
    "               else:\n",
    "                    # # Reopen the existing dataset if it already exists\n",
    "                    dset  = f2[\"dataset\"]\n",
    "\n",
    "\n",
    "               for ind, array in enumerate(batch_of_embeddings):\n",
    "\n",
    "                    print(\"lAST BATCH: LIST ITEM #:\", ind)\n",
    "                    # Resize dataset to accommodate new data\n",
    "                    dset.resize((dset.shape[0] + array.shape[0]), axis=0)\n",
    "                    \n",
    "                    # Write data to the end of the dataset\n",
    "                    dset[-array.shape[0]:] = array\n",
    "\n",
    "          # Clear the batch and row numbers after writing to the file  \n",
    "          del embeddings        \n",
    "          del embeddings_numpy\n",
    "          batch_of_embeddings.clear() \n",
    "          gc.collect() \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# NUMBER OF CHUNKS IN SOURCE H5 FILE GRABBED AT ONCE. MUST BE A \"DIVISOR\" OF data_size/divisor_value:  \n",
    "# [1, 2, 3, 4, 6, ....\n",
    "chunks_grabbed_from_source_h5_file =2\n",
    "batch_size=3\n",
    "output_path_for_text_passage_embeddings = os.path.join(datasets_storage_on_disk, \"data\",\"text_passage_embeddings.h5\")\n",
    "\n",
    "\n",
    "\n",
    "create_text_passage_embeddings(file_source_path=path_to_inputs_tensor_as_numpy,\n",
    "                               chunks_grabbed_from_source_h5_file=chunks_grabbed_from_source_h5_file,\n",
    "                               batch_size=batch_size,\n",
    "                               output_path=output_path_for_text_passage_embeddings,\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset : (1755520, 768)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with h5py.File(output_path_for_text_passage_embeddings, 'r') as f:\n",
    "\n",
    "    for key in f.keys():\n",
    "\n",
    "        dataset = f[key]\n",
    "        print(f\"{key} : {dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_4_'></a>[III- 6. CREATE FAISS INDECES FROM TEXT PASSAGES EMBEDDINGS](#toc0_)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda:0')\n",
    "\n",
    "def create_faiss_IndexIVFFlat_index(file_path_for_embeddings_hd5, indeces_path, chunk_size, divisor, number_cluster_for_embeddings=100, \n",
    "                                    sample_embeddings_size=50000):\n",
    "\n",
    "    # Open the HDF5 file\n",
    "    with h5py.File(file_path_for_embeddings_hd5, 'r') as f: \n",
    "\n",
    "    # Iterate over the keys associated with the different datasets inside the HD5 file\n",
    "     for key in f.keys():  \n",
    "\n",
    "        # Grabs iteratively each dataset \n",
    "        dataset = f[key]\n",
    "\n",
    "        # Dimensionality of embeddings\n",
    "        embeddings_vectors_length = dataset.shape[1]\n",
    "\n",
    "        # NUMBER OF VECTORS EMBEDDINGS (ROWS)\n",
    "        total_embeddings = dataset.shape[0]\n",
    "        \n",
    "        # IF EMBEDDINGS ARE \"NORMALIZED\":\n",
    "        # Initialize a quantizer\n",
    "        # quantizer = faiss.IndexFlatIP(d) \n",
    "        # Initialize the IVF index\n",
    "        # index_cpu = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_COSINE)  \n",
    "              \n",
    "        quantizer = faiss.IndexFlatL2(embeddings_vectors_length) \n",
    "        # Initialize the IVF index\n",
    "        # \"number_cluster_for_embeddings\" IS THE Number of clusters INTO WHICH that ALL THE EMBEDDINGS FROM THE SOURCE FILE will be partitioned.\n",
    "        index_cpu = faiss.IndexIVFFlat(quantizer, embeddings_vectors_length, number_cluster_for_embeddings, faiss.METRIC_L2)\n",
    "\n",
    "        # \"index.nprobe\" IS typically set after the index has been created and before performing a search\n",
    "        # \"index.nprobe\" SPECIFIES HOW MANY OF THE CLUSTERS MADE USING THE EMBEDDINGS WILL BE INCLUDED FOR A SEARCH, balancing EFFICIENCY and ACCUARCY.                 \n",
    "        # BY Setting nprobe equal to nlist (number_cluster_for_embeddings) the index will search \"ALL\" clusters in the index TO MAXIMIZE \"RECALL\".\n",
    "        index_cpu.nprobe = number_cluster_for_embeddings\n",
    "\n",
    "        # Transfer CPU index to GPU (device 0)\n",
    "        try:            \n",
    "            #print(\"FAISS-GPU resources are available.\")\n",
    "            gpu_resources = faiss.StandardGpuResources()\n",
    "            index = faiss.index_cpu_to_gpu(gpu_resources, 0, index_cpu)  \n",
    "        except AttributeError:\n",
    "            print(f\" FAISS-GPU resources are not available. Ensure the GPU version of FAISS is installed.\", \"\\n\")\n",
    "        \n",
    "        \n",
    "        # SPECIFY THE SIZE OF EMBEDDINGS SAMPLE TO BE USED FOR \"TRAINING\" the index\n",
    "        sample_embeddings = dataset[:sample_embeddings_size]          \n",
    "        index.train(sample_embeddings)\n",
    "\n",
    "        # TRANSFER BACK THE \"TRAINED\" INDEX TO CPU BEFORE SAVINGS\n",
    "        index_cpu = faiss.index_gpu_to_cpu(index)\n",
    "\n",
    "        # Set up index to use disk storage\n",
    "        # If the index size exceeds available MEMORY, FAISS AUTOMATICALLY WILL USE disk-backed storage.        \n",
    "        faiss.write_index(index_cpu, indeces_path)\n",
    "\n",
    "        # SETS TO USE GPU FOR INDEXING THE EMBEDDINGS \"AFTER\" THE INDEX HAS BEEN TRAINED\n",
    "        index = faiss.index_cpu_to_gpu(gpu_resources, 0, index_cpu)\n",
    "\n",
    "        # Add embeddings in chunks TO THE INDEX\n",
    "        for start in range(0, total_embeddings, chunk_size):\n",
    "            end = min(start + chunk_size, total_embeddings)\n",
    "            embeddings_chunk = np.array(dataset[start:end], dtype='float32')\n",
    "            # Add to disk-backed index\n",
    "            index.add(embeddings_chunk)  \n",
    "\n",
    "            # Periodically save to disk\n",
    "            \n",
    "            if start % (chunk_size * divisor) == 0: \n",
    "\n",
    "                index_cpu = faiss.index_gpu_to_cpu(index) \n",
    "                # USE \"partial_index\" SCHEMES AVOIDS fragmentation AND unnecessary I/O on THE \"FINAL\" file\n",
    "                faiss.write_index(index_cpu, os.path.join(datasets_storage_on_disk,\"data\",\"partial_index.faiss\"))\n",
    "                print(f\"Partial Index saved at chunk starting at row: {start} and ending at row: {end}\")\n",
    "                index = faiss.index_cpu_to_gpu(gpu_resources, 0, index_cpu)\n",
    "\n",
    "        index_cpu = faiss.index_gpu_to_cpu(index) \n",
    "        \n",
    "        # Final save to ensure all embeddings are written\n",
    "        faiss.write_index(index_cpu, indeces_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial Index saved at chunk starting at row: 0 and ending at row: 1755520\n",
      "CPU times: user 1min 57s, sys: 12.6 s, total: 2min 9s\n",
      "Wall time: 2min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#output_path_for_text_passage_embeddings = os.path.join(datasets_storage_on_disk, \"data\",\"text_passage_embeddings.h5\")\n",
    "\n",
    "path_to_save_faiss_indeces= os.path.join(datasets_storage_on_disk, \"data\",\"disk_index.faiss\")\n",
    "chunks_of_embeddings = int(data_size/4)\n",
    "divisor = 26\n",
    "number_cluster_for_embeddings = 2048 #Max possible!!!!)\n",
    "sample_embeddings_size = data_size\n",
    "\n",
    "create_faiss_IndexIVFFlat_index(file_path_for_embeddings_hd5=output_path_for_text_passage_embeddings, \n",
    "                                indeces_path=path_to_save_faiss_indeces, \n",
    "                                chunk_size=chunks_of_embeddings,\n",
    "                                divisor = divisor,\n",
    "                                number_cluster_for_embeddings=number_cluster_for_embeddings, \n",
    "                                sample_embeddings_size=sample_embeddings_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading FAISS index from: /mnt/f/RAG DATASETS/data/disk_index.faiss (Memory-Mapped Mode)\n",
      "\n",
      "FAISS index successfully loaded in memory-mapped mode \n",
      "\n",
      "Number of vectors in the index: 1755520\n",
      "\n",
      "First 1 vectors in FAISS Index:\n",
      "[[ 6.26053572e-01  1.80991575e-01  1.84850961e-01 -3.43627572e-01\n",
      "  -1.00219652e-01  3.74523997e-01 -1.18327737e-01  4.07786965e-01\n",
      "   7.28981197e-02 -1.73243165e-01  4.62035723e-02  5.30305684e-01\n",
      "  -2.22905695e-01  7.20958635e-02 -2.12288484e-01 -2.46788859e-01\n",
      "   6.29681274e-02  3.51284862e-01 -1.17545463e-01 -1.56658351e-01\n",
      "  -1.31592703e+00 -4.11132090e-02 -2.58875996e-01 -1.81244135e-01\n",
      "   8.30460072e-01  1.45105049e-01  3.53189647e-01 -3.43689889e-01\n",
      "  -5.64491034e-01 -5.69420122e-02  7.03412235e-01  3.21658522e-01\n",
      "   2.97736347e-01  5.08594476e-02 -4.72325146e-01 -1.67698249e-01\n",
      "  -6.87645823e-02 -2.56237239e-01 -4.69520211e-01 -5.22452295e-01\n",
      "  -6.75320745e-01  2.34252170e-01  2.09486082e-01  2.46723413e-01\n",
      "  -2.91404128e-01 -5.81698418e-01 -1.86656082e+00  4.54716206e-01\n",
      "  -6.23090208e-01  1.60714939e-01 -5.18799782e-01  1.95504054e-01\n",
      "  -2.60060504e-02  2.03456566e-01 -2.39040762e-01  5.80554307e-01\n",
      "  -3.11323702e-01 -1.75151259e-01 -3.03916186e-01 -2.89246738e-01\n",
      "   6.58130169e-01 -4.87869710e-01  5.66151619e-01  2.19919205e-01\n",
      "  -1.47287706e-02  1.24049023e-01 -1.49544880e-01  4.43736553e-01\n",
      "  -4.52180654e-02 -6.39816701e-01  1.53281139e-02 -7.53517687e-01\n",
      "  -1.89447954e-01 -2.44007736e-01 -3.41145009e-01 -1.29447775e-02\n",
      "   9.40241665e-02  1.31195098e-01 -3.72561626e-02 -3.11692119e-01\n",
      "  -6.32185280e-01  5.83795190e-01 -1.68355733e-01 -2.03415111e-01\n",
      "   3.77053350e-01 -4.83546257e-02 -1.48351109e-02  7.60393858e-01\n",
      "  -2.70187780e-02  7.90991664e-01  4.51080471e-01 -5.40159106e-01\n",
      "   6.33031309e-01 -4.49748576e-01  2.64753073e-01 -6.29865587e-01\n",
      "   6.10824563e-02 -1.32547781e-01  4.34002429e-01 -3.18869799e-01\n",
      "  -2.96786368e-01 -1.33433461e-01  4.47513402e-01 -2.59217918e-01\n",
      "   1.19475737e-01  3.03979456e-01  1.66487038e-01 -1.10446370e+00\n",
      "   1.63858294e-01 -8.51560850e-03 -1.27665639e-01 -1.35028556e-01\n",
      "  -1.59493033e-02  1.68873742e-01 -4.12381440e-01  4.43103760e-01\n",
      "  -2.42999002e-01 -4.02794540e-01 -1.82108596e-01  1.63800567e-01\n",
      "   5.30857205e-01  8.56183708e-01  4.41852845e-02 -5.53976834e-01\n",
      "   2.72785664e-01 -2.92986929e-01  3.82331312e-01  9.71311629e-01\n",
      "  -1.18586171e+00 -9.09376591e-02 -3.18112504e-03  6.29879594e-01\n",
      "   2.12062299e-01  2.63648719e-01  3.50704156e-02 -1.01497881e-01\n",
      "   2.31528774e-01 -2.73601085e-01  1.25401095e-01 -5.39044440e-01\n",
      "  -2.46909797e-01 -3.03396463e-01 -3.90671492e-01 -4.54075873e-01\n",
      "   4.22065914e-01 -2.81341285e-01  5.59151471e-01 -7.61223137e-01\n",
      "  -6.43332124e-01 -3.94673824e-01 -2.55786002e-01 -3.62951070e-01\n",
      "  -6.77228421e-02  3.02141249e-01  1.91149905e-01  4.70299900e-01\n",
      "   7.07045913e-01 -3.58414888e-01  4.78227027e-02  5.36036529e-02\n",
      "   5.70431948e-01  9.46152583e-02 -6.54415727e-01 -6.73053190e-02\n",
      "  -1.49426684e-02 -1.27471760e-01 -9.93412733e-02  8.16837311e-01\n",
      "   4.03882384e-01 -2.24107534e-01 -5.38954139e-02  2.02342227e-01\n",
      "  -4.27536294e-02 -1.51461288e-01  4.10171896e-01  4.87517387e-01\n",
      "   5.65639555e-01  1.68572009e-01  4.20980483e-01  3.98027390e-01\n",
      "  -7.25379884e-01  3.93419117e-01 -2.04255972e-02 -3.93932909e-01\n",
      "   4.55817282e-02 -5.68250082e-02  2.70988017e-01  1.19709022e-01\n",
      "  -7.08476231e-02  5.35654575e-02 -4.31952536e-01 -7.52009571e-01\n",
      "  -1.48671314e-01 -5.96243441e-01  1.13281834e+00 -3.84573340e-01\n",
      "  -1.63627431e-01  8.08820903e-01  1.74201310e-01  1.17178440e-01\n",
      "  -4.63410199e-01  4.94935334e-01  3.28482687e-01 -7.02838004e-01\n",
      "  -6.08343542e-01 -5.28423131e-01  7.53342435e-02  3.65619361e-02\n",
      "   1.10977784e-01  5.42368948e-01  9.06879753e-02 -6.92971982e-03\n",
      "   3.91571790e-01 -4.61999804e-01 -3.22977006e-02  7.42539465e-01\n",
      "  -2.73407936e-01  8.53855073e-01 -2.53992528e-02 -3.48047882e-01\n",
      "   5.07087111e-01  1.18188411e-02 -3.98989499e-01  1.04508519e+00\n",
      "   1.91495046e-01  8.84952486e-01  4.41335082e-01 -3.22058499e-01\n",
      "   1.87477082e-01 -1.99537054e-01  8.18666816e-01  2.86181718e-01\n",
      "   5.17305911e-01 -7.79474974e-01 -3.02004933e-01  3.64613235e-01\n",
      "  -1.00559510e-01 -5.03894687e-01 -4.43383634e-01  6.11241758e-02\n",
      "   5.34380853e-01  2.26872712e-01 -5.78083634e-01  5.63898265e-01\n",
      "  -3.57115090e-01  2.90163029e-02 -4.00208920e-01 -8.59117582e-02\n",
      "   2.06364214e-01  2.02657983e-01 -1.48286104e-01  6.88286066e-01\n",
      "  -3.23168747e-02 -4.42156225e-01  3.42065841e-01 -6.61130100e-02\n",
      "  -1.41213253e-01  5.33860505e-01  5.69921851e-01 -9.08996724e-03\n",
      "   2.63444930e-01 -8.46189782e-02  2.17425004e-02  1.10778224e+00\n",
      "  -2.55013883e-01 -4.07518089e-01 -4.28332031e-01 -2.18905836e-01\n",
      "  -1.71859916e-02  3.43831927e-01  3.23400915e-01  9.55980346e-02\n",
      "  -9.52196300e-01  4.34632361e-01  1.82211369e-01 -1.09991646e+00\n",
      "   4.51304048e-01  3.10452163e-01  2.23867856e-02 -5.00238419e-01\n",
      "  -2.77837515e-01  7.14893937e-01  4.38906670e-01 -9.35390532e-01\n",
      "   1.22164026e-01 -8.34131896e-01  8.48286897e-02 -2.15347216e-01\n",
      "  -3.20438176e-01  6.50349677e-01 -4.33849543e-02  4.32236224e-01\n",
      "  -1.47705257e-01 -4.48836461e-02  1.27726421e-01  3.62204432e-01\n",
      "   2.65125096e-01  6.11805141e-01  2.66604096e-01 -1.38908252e-01\n",
      "   7.15800151e-02 -1.15771063e-01 -3.90108824e-01 -1.39749080e-01\n",
      "  -1.55760497e-01 -5.17708361e-01 -8.39291513e-02 -1.92447677e-01\n",
      "  -5.63773108e+00 -9.78702456e-02  3.82956386e-01 -5.27007520e-01\n",
      "   1.26460791e-01  2.23860025e-01  8.70607018e-01 -6.29199028e-01\n",
      "   6.19649231e-01  2.32874587e-01 -3.70164096e-01 -1.30629778e-01\n",
      "   1.52531534e-01 -3.35160941e-01 -5.39233200e-02  6.99118435e-01\n",
      "   3.44255149e-01  5.75881712e-02  3.18886369e-01 -7.07176374e-03\n",
      "  -2.72265911e-01 -3.95254284e-01  3.54348630e-01  1.40674937e+00\n",
      "  -8.61029401e-02  1.77598953e-01 -6.93336785e-01  1.98385000e-01\n",
      "   4.48663324e-01  5.03100872e-01  5.19591510e-01  8.79206881e-02\n",
      "   1.38300717e-01  5.53318501e-01 -4.40293960e-02  2.86775865e-02\n",
      "   8.70465696e-01 -2.92073697e-01 -6.77326471e-02 -5.65839052e-01\n",
      "   7.25484192e-01  9.74932373e-01 -1.49469912e-01 -1.75267339e-01\n",
      "   4.26929682e-01 -7.68919468e-01 -1.87082171e-01 -2.94486970e-01\n",
      "  -5.04015684e-01  3.98423404e-01  1.40920758e-01 -2.08093766e-02\n",
      "   3.02645177e-01 -2.18828082e-01  4.47528601e-01 -4.75896806e-01\n",
      "  -3.44947666e-01 -1.28495187e-01 -1.38934270e-01  4.58209455e-01\n",
      "   9.05332416e-02 -1.55455649e-01 -3.35756332e-01 -2.13394731e-01\n",
      "  -2.47321621e-01  2.73396820e-01 -7.40982234e-01 -6.91827357e-01\n",
      "   7.39175439e-01 -8.31964519e-03 -4.10269320e-01  7.46026993e-01\n",
      "   1.92779094e-01 -5.47558248e-01  2.33700901e-01 -7.44186103e-01\n",
      "   3.98395061e-01  2.73828566e-01  9.65136513e-02  3.30136716e-01\n",
      "   2.67869651e-01  2.22686678e-01 -2.05398828e-01 -1.17186606e-01\n",
      "   7.54085109e-02 -6.66478038e-01 -8.19618329e-02  4.65103500e-02\n",
      "  -2.12635964e-01 -5.59515394e-02  1.02080262e+00 -4.71381098e-02\n",
      "   7.83231333e-02  2.70762861e-01 -3.72331380e-03  1.73227698e-01\n",
      "   2.52280265e-01  6.22831136e-02  2.89377999e-02 -4.16859537e-01\n",
      "   7.40537643e-01  7.16165900e-01 -5.39428174e-01 -1.12480402e+00\n",
      "  -1.58573359e-01 -1.33898407e-01 -5.93472779e-01 -1.98704600e-01\n",
      "   7.39074349e-01  3.20887528e-02 -2.19351828e-01  6.60667419e-02\n",
      "  -8.06478083e-01 -2.02544466e-01 -1.80600613e-01  2.19451100e-01\n",
      "  -9.05245543e-01  1.89817905e-01  2.33715460e-01 -1.81094572e-01\n",
      "   4.84656245e-01 -5.80337048e-02 -8.38879943e-01  2.78186291e-01\n",
      "   1.76687837e-01 -4.34426099e-01 -1.10426292e-01  1.44918531e-01\n",
      "  -1.34232223e-01 -2.78463215e-01  9.74570870e-01 -1.35998592e-01\n",
      "  -6.56795919e-01 -3.90919000e-01  4.91516203e-01  3.03022355e-01\n",
      "  -2.24298045e-01 -4.87816989e-01 -2.74750054e-01 -4.51679945e-01\n",
      "  -1.25981331e-01  2.24548485e-02  1.49830222e-01  4.93985325e-01\n",
      "   6.84278369e-01 -2.26313919e-01 -5.42468190e-01 -9.93285775e-02\n",
      "   4.79950190e-01 -3.14371511e-02 -3.22505653e-01 -2.89689332e-01\n",
      "  -1.50292799e-01  1.39216959e-01 -3.19382489e-01 -1.10227816e-01\n",
      "   2.65221506e-01 -3.08134168e-01 -5.14087379e-01 -5.08234143e-01\n",
      "   9.03870016e-02 -1.54131398e-01 -8.45215261e-01  9.95377749e-02\n",
      "  -5.27684279e-02 -2.14085802e-01 -8.39394271e-01  5.98402321e-02\n",
      "   2.34612763e-01  2.57603437e-01 -3.19651999e-02 -5.70474744e-01\n",
      "  -6.53944314e-01 -3.99093211e-01  1.37595549e-01  2.22020388e-01\n",
      "   1.92602396e-01  3.21382403e-01  2.19342429e-02  1.59076124e-01\n",
      "  -1.76806673e-01 -6.25358760e-01 -4.87079918e-01  1.66953295e-01\n",
      "   8.06505501e-01 -4.25563365e-01 -8.65739584e-02  1.82118744e-01\n",
      "  -7.66081750e-01 -3.08550209e-01 -2.95891017e-01  1.68934733e-01\n",
      "   4.80617523e-01  2.49465376e-01  6.26590550e-02 -9.75770593e-01\n",
      "  -4.42422330e-01 -3.80066670e-02 -1.32819682e-01 -4.10869449e-01\n",
      "   5.59998937e-02  6.32061064e-01 -7.82013834e-01 -3.43505353e-01\n",
      "  -1.33803263e-01 -2.48100534e-01 -2.87924651e-02  4.48905071e-03\n",
      "  -1.08647525e-01  4.11797822e-01 -6.30026534e-02 -1.74401939e-01\n",
      "  -2.02119201e-01  4.05445844e-01 -1.84273064e-01  4.08659130e-01\n",
      "  -5.91641188e-01 -4.10958052e-01 -4.01127040e-02  1.57255381e-01\n",
      "   1.64298818e-01 -1.27707243e-01 -8.96848440e-01  7.12974817e-02\n",
      "  -3.54886949e-01  6.66060269e-01  4.43632305e-01 -7.93934882e-01\n",
      "  -5.44522107e-01 -1.15566269e-01  5.52855022e-02 -6.40862808e-02\n",
      "   1.68091968e-01  4.63313758e-01  5.21724485e-02 -1.80065811e-01\n",
      "   5.26468337e-01  4.17374969e-01  2.21031755e-02  4.06385779e-01\n",
      "  -3.97256285e-01  4.44329888e-01 -2.13086724e-01 -1.88120306e-02\n",
      "   3.54134202e-01  7.58395076e-01  3.36090624e-01 -2.62787074e-01\n",
      "  -6.57325506e-01  3.00736904e-01 -7.58454651e-02 -5.18737733e-02\n",
      "  -2.55715579e-01  3.96240413e-01 -2.36446396e-01 -2.82740146e-01\n",
      "  -5.69211543e-01  2.40600616e-01 -2.13092640e-01  4.48671840e-02\n",
      "   6.36492893e-02  5.72544754e-01 -4.95141089e-01 -1.52944356e-01\n",
      "   5.12089670e-01  5.95658541e-01 -1.47855759e-01 -9.60712358e-02\n",
      "  -3.15036863e-01 -1.45604700e-01  4.60510552e-01 -2.47312099e-01\n",
      "   3.94270062e-01  9.51294005e-01  5.17315447e-01  3.83421421e-01\n",
      "  -1.24279492e-01 -1.85150892e-01 -4.66016442e-01  3.71493399e-01\n",
      "  -4.52056736e-01 -6.71307862e-01 -6.88752383e-02 -1.94512635e-01\n",
      "   4.52014238e-01 -7.28915751e-01  5.25524765e-02 -1.27292231e-01\n",
      "  -3.51458490e-02 -5.56960583e-01 -2.17273250e-01  3.67475420e-01\n",
      "  -2.98752874e-01  2.20663622e-01 -7.18296528e-01  3.18424441e-02\n",
      "   2.72526443e-01 -8.91057327e-02 -8.37104738e-01  8.97952989e-02\n",
      "   2.63612151e-01 -8.39675125e-03  5.99917054e-01 -5.72399139e-01\n",
      "   3.54869902e-01  5.07985175e-01  4.66355681e-02 -4.35574830e-01\n",
      "  -1.19387291e-01 -2.91872233e-01 -2.66776353e-01 -5.40762424e-01\n",
      "  -8.33920464e-02  3.05644006e-01 -9.62335408e-01  1.24704555e-01\n",
      "  -4.04439643e-02 -2.78219819e-01  4.58153784e-01 -1.05539769e-01\n",
      "   2.10649163e-01  1.95870996e-01  3.64146084e-01 -4.64883260e-02\n",
      "  -1.41281439e-02  1.95203602e-01 -4.84198242e-01  5.31306326e-01\n",
      "   5.75111322e-02 -7.33104572e-02  8.61820430e-02  4.95034128e-01\n",
      "  -2.31331378e-01  2.68554986e-01  5.09795286e-02 -2.67514467e-01\n",
      "   4.69545335e-01 -5.80644011e-01 -6.37180746e-01  4.05935258e-01\n",
      "   4.64400589e-01  2.59178966e-01 -2.81674594e-01  6.16029054e-02\n",
      "  -2.25125775e-01  2.34986931e-01  8.13312113e-01  4.55449939e-01\n",
      "  -8.79999995e-03  4.71355259e-01  1.93733037e-01  3.13425422e-01\n",
      "   3.90873253e-01  6.35691762e-01 -1.93757504e-01 -5.92479885e-01\n",
      "   1.44502625e-01 -1.67028327e-02  1.55268103e-01 -1.52800843e-01\n",
      "  -9.20965374e-02  6.13884568e-01  3.72151107e-01  3.77473161e-02\n",
      "   4.82261777e-01  4.36434090e-01  2.61568069e-01 -4.12255764e-01\n",
      "   1.66265517e-01  3.92060459e-01 -2.16978729e-01  3.41436975e-02\n",
      "   7.29975104e-02  2.58205861e-01 -1.96091875e-01  1.82672173e-01\n",
      "   5.84945798e-01  4.49725330e-01 -8.61928090e-02  2.71747798e-01\n",
      "  -5.91613233e-01  1.14169791e-02  4.08466458e-02 -3.34291756e-01\n",
      "  -1.01220338e-02  8.73857141e-02 -5.36559999e-01  2.61319757e-01\n",
      "  -2.90570498e-01 -4.39744860e-01 -2.57531881e-01 -5.99878669e-01\n",
      "  -1.78461716e-01  4.82406169e-01  3.39368246e-02  1.84662610e-01\n",
      "   1.40253961e-01  8.33150625e-01  1.82612270e-01  1.85138300e-01\n",
      "  -4.85454649e-01  3.08671445e-01  7.33159110e-03  1.68938577e-01\n",
      "   2.77041346e-01  3.74248713e-01 -7.67651200e-01 -3.41190142e-04\n",
      "  -4.88529354e-01  2.99230844e-01  1.65806442e-01  6.41695186e-02\n",
      "  -1.19481511e-01 -4.36673790e-01  1.77799836e-01  1.55461328e-02\n",
      "   4.24249172e-01 -1.15185954e-01 -4.82692523e-03 -1.39055774e-01\n",
      "   5.55956215e-02  6.59207940e-01 -1.56897590e-01  5.20850085e-02\n",
      "   1.69828653e-01  4.56637681e-01  8.74660909e-02  2.13675007e-01\n",
      "  -3.94946158e-01  4.44229603e-01  7.62098953e-02 -1.16688356e-01\n",
      "  -4.37629551e-01  4.18557644e-01 -8.27264488e-02 -2.75213033e-01\n",
      "   3.90826404e-01  1.35473564e-01  2.91830301e-01 -8.91339630e-02\n",
      "   5.12484968e-01 -5.78762777e-02  8.01660120e-02  4.70559821e-02\n",
      "  -6.20288551e-01  1.78567499e-01  5.35697818e-01 -2.60441422e-01\n",
      "  -1.03184557e+00  3.16280693e-01 -2.80527622e-01 -1.10333550e+00\n",
      "  -7.25183170e-03  2.55342841e-01 -9.69375297e-02  4.93084341e-01\n",
      "   1.55184820e-01 -3.72509986e-01 -3.41151267e-01  2.10358754e-01]]\n",
      "CPU times: user 0 ns, sys: 318 ms, total: 318 ms\n",
      "Wall time: 4.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "vectors_to_show = 1\n",
    "\n",
    "# TO \"USE\" A FAISS INDEX ALREADY \"STORED\" ON DISK WITHOUT LOADING IT \"ENTIRELY\" INTO MEMORY, USE \"MEMORY-MAPPED\" DISK STORAGE MODE.\n",
    "# TO ONLY INSPECT THE \"METADATA\" WITHOUT LOADING THE \"FULL\" INDEX USE THE \".read_index()\" METHOD THE WITH \"faiss.IO_FLAG_MMAP\" PARAMETER\n",
    "print(f\"\\nLoading FAISS index from: {path_to_save_faiss_indeces} (Memory-Mapped Mode)\")\n",
    "index = faiss.read_index(path_to_save_faiss_indeces, faiss.IO_FLAG_MMAP)\n",
    "print(\"\\nFAISS index successfully loaded in memory-mapped mode \")\n",
    "\n",
    "# Get the number of vectors stored in the index\n",
    "num_vectors = index.ntotal  \n",
    "\n",
    "print(f\"\\nNumber of vectors in the index: {num_vectors}\\n\")\n",
    "\n",
    "# Extract the first \"vectors_to_show\" vectors (if available)\n",
    "if num_vectors >= vectors_to_show:\n",
    "    vectors = np.zeros((vectors_to_show, index.d), dtype=np.float32)  # Create an empty NumPy array\n",
    "    \n",
    "    # Retrieve first 5 vectors directly\n",
    "    vectors = index.reconstruct_n(0, vectors_to_show)\n",
    "    \n",
    "    print(f\"First {vectors_to_show} vectors in FAISS Index:\")\n",
    "    print(vectors)\n",
    "else:\n",
    "    print(f\"The FAISS index has less than {vectors_to_show} vectors.\")\n",
    "\n",
    "\n",
    "#del index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_5_'></a>[III- 7. EXAMPLE: FINDING \"BY HAND\" SIMILAR DOCUMENTS TO A QUESTION](#toc0_)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_5_1_'></a>[III- 7.1 EMBED THE USER \"PROMPT/QUESTION\" INTO A VECTOR](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embed_question_as_vector(question):\n",
    "\n",
    "    device = torch.device('cuda:0')\n",
    "\n",
    "    # INSTANTIATE THE pre-trained TOKENIZER TO BE APPLIED THE TEXT PASSAGES (AKA KNOWLEDGE BASE )\n",
    "    trained_model_used_in_question_tokenizer = 'facebook/dpr-question_encoder-single-nq-base'\n",
    "\n",
    "    # tokenizer_passage_encoder = AutoTokenizer.from_pretrained(trained_model_used_in_context_tokenizer) \n",
    "    # tokenizer_passage_encoder = DPRQuestionEncoderTokenizer.from_pretrained(trained_model_used_in_context_tokenizer)     \n",
    "    # REGARDING WARNING AT EXECUTION TIME: \"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from\"\n",
    "    # Functionally, both DPRQuestionEncoderTokenizer and DPRContextEncoderTokenizer tokenizers are identical, so it doesn’t impact model performance. \n",
    "    tokenizer_question_encoder = DPRQuestionEncoderTokenizer.from_pretrained(trained_model_used_in_question_tokenizer) \n",
    "\n",
    "\n",
    "    \n",
    "    #inputs = tokenizer_question_encoder(question, return_tensors='pt', padding=True, max_length=max_length, truncation=True).to(device)\n",
    "    inputs = tokenizer_question_encoder(question, return_tensors='pt').to(device)\n",
    "\n",
    "\n",
    "    model_question_encoder = DPRQuestionEncoder.from_pretrained(trained_model_used_in_question_tokenizer) \n",
    "    model_question_encoder = model_question_encoder.to(device)\n",
    "    # Perform the forward pass WITH gradients DISABLED for efficiency\n",
    "    with torch.no_grad():\n",
    "        question_embedding = model_question_encoder(**inputs).pooler_output\n",
    "\n",
    "    embedding_numpy = question_embedding.cpu().detach().numpy().astype('float32')\n",
    "    print(\"\\n\", \"Shape of Embedded Question\", embedding_numpy.shape)\n",
    "\n",
    "    return embedding_numpy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Shape of Embedded Question (1, 768)\n"
     ]
    }
   ],
   "source": [
    "#user_question =  \"Who won the world series in 2020?\"\n",
    "user_question= \"Human fertilization\"\n",
    "\n",
    "question_embedding_numpy = embed_question_as_vector(user_question)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_5_2_'></a>[III- 7.2 RETRIEVE RELEVANT DOCUMENTS FOR THE USER \"PROMPT/QUESTION\"](#toc0_)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents_similar_to_question(path, embedded_question, number_docs):\n",
    "        \n",
    "    # TO \"USE\" A FAISS INDEX ALREADY \"STORED\" ON DISK WITHOUT LOADING IT \"ENTIRELY\" INTO MEMORY, USE \"MEMORY-MAPPED\" DISK STORAGE MODE.\n",
    "    # TO ONLY INSPECT THE \"METADATA\" WITHOUT LOADING THE \"FULL\" INDEX USE THE \".read_index()\" METHOD THE WITH \"faiss.IO_FLAG_MMAP\" PARAMETER\n",
    "    print(f\"\\nLoading FAISS index from: {path} (Memory-Mapped Mode)\")\n",
    "    index = faiss.read_index(path, faiss.IO_FLAG_MMAP)\n",
    "    print(\"\\nFAISS index successfully loaded in memory-mapped mode \")\n",
    "\n",
    "\n",
    "    # Convert the index to a GPU index\n",
    "    gpu_resources = faiss.StandardGpuResources()\n",
    "    gpu_index = faiss.index_cpu_to_gpu(gpu_resources, 0, index)  # 0 is the GPU ID\n",
    "\n",
    "\n",
    "    \n",
    "    # Perform the search on GPU\n",
    "    D, I = gpu_index.search(embedded_question, k=number_docs)\n",
    "\n",
    "    print(\"\\n\", \"Distances to Question:\", D, \"\\n\", \"FAISS Indeces of Relevant Context Documents:\", I)\n",
    "\n",
    "    return D, I\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading FAISS index from: /mnt/f/RAG DATASETS/data/disk_index.faiss (Memory-Mapped Mode)\n",
      "\n",
      "FAISS index successfully loaded in memory-mapped mode \n",
      "\n",
      " Distances to Question: [[74.96144  75.66108  77.482864 77.482864]] \n",
      " FAISS Indeces of Relevant Context Documents: [[791664  42048 732162 909755]]\n"
     ]
    }
   ],
   "source": [
    "number_of_top_passages_to_keep = 4\n",
    "\n",
    "D, I = retrieve_documents_similar_to_question(path_to_save_faiss_indeces, question_embedding_numpy, number_of_top_passages_to_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_5_3_'></a>[III- 7.3 CHECK THE RELEVANT DOCUMENTS RETRIEVED FOR THE USER \"PROMPT/QUESTION\"](#toc0_)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def map_documents_to_faiss_id(tsv_file, my_list):\n",
    "    \"\"\"\n",
    "    Reads only the rows specified in `my_list` from a large TSV file.\n",
    "    \n",
    "    :param tsv_file: Path to the TSV file.\n",
    "    :param my_list: List of row numbers to extract (1-based index).\n",
    "    :return: List of extracted rows.\n",
    "    \"\"\"\n",
    "    extracted_documents = []\n",
    "\n",
    "    with open(tsv_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        \n",
    "        for row_number, line in enumerate(file, start=0):  # FIRST ROW IS AT INDEX 0 AND THERE IS NO HEADER\n",
    "            if row_number in my_list:\n",
    "                columns = line.strip().split(\"\\t\")  # Split by tab to get columns. GET THE LINE THAN SPLIT ON ITS COLUMNS\n",
    "                extracted_documents.append((row_number,columns[1]))  # Extract the second column (index 1)\n",
    "                \n",
    "            if row_number > max(my_list):  # Stop early if max row is reached\n",
    "                break\n",
    "\n",
    "    return extracted_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/f/RAG DATASETS/data/text_passages_reduced_length.tsv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "\n",
    "# \"I\" IS A LIST OF ONLY 1 ELEMENT. THAT ELEMENT IS A 1-D NUMPY ARRAY\n",
    "row_numbers_of_relevant_documents_list =  list(I[0]) # Row indices (0-based)\n",
    "\n",
    "\n",
    "path_to_passages_that_were_embedded = tsv_file_path1\n",
    "print(path_to_passages_that_were_embedded)\n",
    "\n",
    "relevant_documents_list = map_documents_to_faiss_id(path_to_passages_that_were_embedded, row_numbers_of_relevant_documents_list)\n",
    "\n",
    "len(relevant_documents_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 4 Relevant Contexts:\n",
      "\n",
      " 1: genetic controls of female development, and much remains unknown about the female embryonic process. males become externally distinct between 8 and 12 weeks, as androgens enlarge the phallus and cause the urogenital groove and sinus to fuse in the midline, producing an unambiguous penis with a phallic urethra, and a thinned, rugated scrotum. dihydrotestosterone will differentiate the remaining male characteristics of the external genitalia. a sufficient amount of any androgen can cause external masculinization. the most potent is dihydrotestosterone ( dht ), generated from testosterone in skin and genital tissue by the action of 5 \\ u03b1 - reductase. a male fetus may be incompletely masculinized if this enzyme is deficient. in some diseases and circumstances, other androgens may be present in high enough concentrations to cause partial or ( rarely ) complete masculinization of the external genitalia of a genetically female fetus. the testes begin to secrete three hormones that influence the male internal and external genitalia. they secrete anti - m \\ u00fcllerian hormone, testosterone, and \n",
      " Distance 75.66107940673828 (Passage ID: 42048)\n",
      "\n",
      " 2: fertilization while the remains of the dominant follicle in the ovary become a corpus luteum ; this body has a primary function of producing large amounts of progesterone. under the influence of progesterone, the uterine lining changes to prepare for potential implantation of an embryo to establish a pregnancy. if implantation does not occur within approximately two weeks, the corpus luteum will involute, causing a sharp drop in levels of both progesterone and estrogen. the hormone drop causes the uterus to shed its lining in a process termed menstruation. menstruation also occurs in closely related primates ( apes and monkeys ). the average age of menarche is 12 - - 15. they may occasionally start as early as eight, and this onset may still be normal. this first period often occurs later in the developing world than the developed world. the average age of menarche is approximately 12. 5 years in the united states, 12. 7 in canada, 12. 9 in the uk and 13. 1 years in iceland. factors such as genetics, diet and overall health can affect timing. the cessation of menstrual cycles at the end of a woman ' s \n",
      " Distance 77.48286437988281 (Passage ID: 732162)\n",
      "\n",
      " 3: human male. although many of these cases involve cloned embryos or subsequent rejection of the paternal mitochondria, others document in vivo inheritance and persistence under lab conditions. an ivf technique known as mitochondrial donation or mitochondrial replacement therapy ( mrt ) results in offspring containing mtdna from a donor female, and nuclear dna from the mother and father. in the spindle transfer procedure, the nucleus of an egg is inserted into the cytoplasm of an egg from a donor female which has had its nucleus removed, but still contains the donor female ' s mtdna. the composite egg is then fertilized with the male ' s sperm. the procedure is used when a woman with genetically defective mitochondria wishes to procreate and produce offspring with healthy mitochondria. the first known child to be born as a result of mitochondrial donation was a boy born to a jordanian couple in mexico on 6 april 2016. in most multicellular organisms, the mtdna - - or mitogenome - - is organized as a circular, covalently closed, double - stranded dna. but in many unicellular ( e. g. the ciliate tetrahymena or the green alga chlamydomonas reinhardtii ) \n",
      " Distance 74.96144104003906 (Passage ID: 791664)\n",
      "\n",
      " 4: fertilization while the remains of the dominant follicle in the ovary become a corpus luteum ; this body has a primary function of producing large amounts of progesterone. under the influence of progesterone, the uterine lining changes to prepare for potential implantation of an embryo to establish a pregnancy. if implantation does not occur within approximately two weeks, the corpus luteum will involute, causing a sharp drop in levels of both progesterone and estrogen. the hormone drop causes the uterus to shed its lining in a process termed menstruation. menstruation also occurs in closely related primates ( apes and monkeys ). the average age of menarche is 12 - - 15. they may occasionally start as early as eight, and this onset may still be normal. this first period often occurs later in the developing world than the developed world. the average age of menarche is approximately 12. 5 years in the united states, 12. 7 in canada, 12. 9 in the uk and 13. 1 years in iceland. factors such as genetics, diet and overall health can affect timing. the cessation of menstrual cycles at the end of a woman ' s \n",
      " Distance 77.48286437988281 (Passage ID: 909755)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"\\nTop {number_of_top_passages_to_keep} Relevant Contexts:\")\n",
    "\n",
    "for i, doc in enumerate(relevant_documents_list):\n",
    "\n",
    "    distance_index = row_numbers_of_relevant_documents_list.index(doc[0])\n",
    "    print(f\"\\n\", f\"{i+1}: {doc[1]}\", \"\\n\", \\\n",
    "           f\"Distance {D[0][distance_index]} (Passage ID: {doc[0]})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_5_4_'></a>[III- 7.4 COMPARE BART LLM RESPONSE WITH AND WIHTOUT CONTEXT](#toc0_)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_without_rerieval(question):\n",
    "    \n",
    "    # News summarization\n",
    "    #pre_trained_model = \"facebook/bart-large-cnn\"\n",
    "    # Lighter version for inference\n",
    "    #pre_trained_model = \"facebook/bart-base\"\n",
    "    # General text generation\n",
    "    pre_trained_model = \"facebook/bart-large\"\n",
    "\n",
    "\n",
    "    # Hugging Face's TOKENIZERS \"CANNOT\" USE GPU Because Tokenizations are purely CPU-based operations.\n",
    "    tokenizer = BartTokenizer.from_pretrained(pre_trained_model)\n",
    "    inputs = tokenizer(question, return_tensors = 'pt', max_length=1024, padding=True, truncation = True)\n",
    "    \n",
    "    device = torch.device('cuda:0')\n",
    "    model_for_tokenizer = BartForConditionalGeneration.from_pretrained(pre_trained_model).to(device)\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        response_ids = model_for_tokenizer.generate(inputs['input_ids'].to(device), max_length=256, min_length=40, length_penalty = 2.0, \\\n",
    "                                                    num_beams = 4, early_stopping = True)\n",
    "    \n",
    "    #print(\"\\nresponse_ids:\\n\", response_ids)\n",
    "    answer = tokenizer.decode(response_ids[0].detach().cpu(), skip_special_tokens = True)\n",
    "\n",
    "    print(f\"\\nGenerated Answer Without Rerieval: \\n {answer}\")\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Answer Without Rerieval: \n",
      " Human fertilizationFertilizationFetilisationFetal implantationFetabolismFetanomycinFetanylphosphataseFetylphosphateFetanolophosphate\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Human fertilizationFertilizationFetilisationFetal implantationFetabolismFetanomycinFetanylphosphataseFetylphosphateFetanolophosphate'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#user_question =  \"Who won the world series in 2020?\"\n",
    "user_question= \"Human fertilization\"\n",
    "\n",
    "generate_answer_without_rerieval(user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_using_rerieval(context):\n",
    "    \n",
    "    # News summarization\n",
    "    #pre_trained_model = \"facebook/bart-large-cnn\"\n",
    "    # Lighter version for inference\n",
    "    #pre_trained_model = \"facebook/bart-base\"\n",
    "    # General text generation\n",
    "    pre_trained_model = \"facebook/bart-large\"\n",
    "\n",
    "\n",
    "    # Hugging Face's TOKENIZERS \"CANNOT\" USE GPU Because Tokenizations are purely CPU-based operations.\n",
    "    tokenizer = BartTokenizer.from_pretrained(pre_trained_model)\n",
    "\n",
    "    input_text = \" \".join(context)\n",
    "    inputs = tokenizer(input_text, return_tensors = 'pt', max_length=1024, padding=True, truncation = True)\n",
    "    \n",
    "    device = torch.device('cuda:0')\n",
    "    model_for_tokenizer = BartForConditionalGeneration.from_pretrained(pre_trained_model).to(device)\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        response_ids = model_for_tokenizer.generate(inputs['input_ids'].to(device), max_length=256, min_length=40, length_penalty = 2.0, \\\n",
    "                                                    num_beams = 4, early_stopping = True)\n",
    "        \n",
    "    answer = tokenizer.decode(response_ids[0].detach().cpu(), skip_special_tokens = True)\n",
    "    \n",
    "    #print(\"\\nresponse_ids:\\n\", response_ids)\n",
    "    print(f\"\\nGenerated Answer USING Rerieval: \\n {answer}\")\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Answer USING Rerieval: \n",
      " genetic controls of female development, and much remains unknown about the female embryonic process. in most multicellular organisms, the mtdna - - or mitogenome - - is organized as a circular, covalently closed, double - stranded dna. but in many unicellular ( e. g. the ciliate tetrahymena or the green alga chlamydomonas reinhardtii ) fertilization while the remains of the dominant follicle in the ovary become a corpus luteum ; this body has a primary function of producing large amounts of progesterone. the hormone drop causes the uterus to shed its lining in a process termed menstruation. menstruation also occurs in closely related primates ( apes and monkeys ). the average age of menarche is 12 - - 15. they may occasionally start as early as eight, and this onset may still be normal. the cessation of menstrual cycles at the end of a woman ' s human male. this first period often occurs later in the developing world than the developed world. factors such as genetics, diet and overall health can affect timing. in the spindle transfer procedure, the nucleus of an egg from a donor female which has had its nucleus removed, but still contains\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"genetic controls of female development, and much remains unknown about the female embryonic process. in most multicellular organisms, the mtdna - - or mitogenome - - is organized as a circular, covalently closed, double - stranded dna. but in many unicellular ( e. g. the ciliate tetrahymena or the green alga chlamydomonas reinhardtii ) fertilization while the remains of the dominant follicle in the ovary become a corpus luteum ; this body has a primary function of producing large amounts of progesterone. the hormone drop causes the uterus to shed its lining in a process termed menstruation. menstruation also occurs in closely related primates ( apes and monkeys ). the average age of menarche is 12 - - 15. they may occasionally start as early as eight, and this onset may still be normal. the cessation of menstrual cycles at the end of a woman ' s human male. this first period often occurs later in the developing world than the developed world. factors such as genetics, diet and overall health can affect timing. in the spindle transfer procedure, the nucleus of an egg from a donor female which has had its nucleus removed, but still contains\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_contexts = [doc[1] for doc in relevant_documents_list]\n",
    "generate_answer_using_rerieval(top_contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_6_'></a>[IV- ORGANIZE TEXT PASSAGES AND THEIR EMBEDDINGS IN THE FORMAT NEEDED BY THE RAG \"MODEL\"](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_6_1_'></a>[IV- 11.1 JOIN TEXT PASSAGES AND THEIR EMBEDDINGS IN THE SAME FILE](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Length of Passages: 1629\n",
      "CPU times: user 3min 48s, sys: 1.09 s, total: 3min 49s\n",
      "Wall time: 4min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# FIND MAX LENGHTS ACCROSS ALL TEXT PASSAGES\n",
    "\n",
    "\n",
    "# RECALL: tsv_file_path1 = os.path.join(datasets_storage_on_disk, \"data\",\"text_passages_reduced_length.tsv\")\n",
    "\n",
    "#for chunk_df in pd.read_csv(file_path, skiprows=range(1, start_row + 1), chunksize=chunk_size):\n",
    "for chunk in pd.read_csv(tsv_file_path1, sep='\\t', names=['id', 'text'], chunksize=50000):  \n",
    "\n",
    "    passages_max_len = 0\n",
    "    column_name = chunk.columns[1]\n",
    "    length_of_texts = [len(row[column_name]) for index, row in chunk.iterrows()]\n",
    "    if passages_max_len < max(length_of_texts):\n",
    "        passages_max_len =  max(length_of_texts)\n",
    "\n",
    "passages_max_len = passages_max_len +10\n",
    "print(f\"Maximum Length of Passages: {passages_max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_containing_text_passages_with_embeddings(text_passages_path, file_path_for_embeddings_hd5, \n",
    "                                                           chunk_size, batch_size, max_len, output_path, index_size):\n",
    "\n",
    "\n",
    "    # Open the HDF5 file\n",
    "    with h5py.File(file_path_for_embeddings_hd5, 'r') as f: \n",
    "     \n",
    "\n",
    "    # Iterate over the keys associated with the different datasets inside the HD5 file\n",
    "     for key in f.keys():  \n",
    "\n",
    "        # Grabs iteratively each dataset \n",
    "        dataset = f[key]\n",
    "        total_embeddings = dataset.shape[0]\n",
    "        #print(total_embeddings)\n",
    "\n",
    "        batch_of_dicts = []\n",
    "\n",
    "        total_rows_loaded = 0\n",
    "\n",
    "        #for chunk_df in pd.read_csv(file_path, skiprows=range(1, start_row + 1), chunksize=chunk_size):\n",
    "        for chunk in pd.read_csv(text_passages_path, sep='\\t', names=['id', 'text'], chunksize=chunk_size): \n",
    "\n",
    "\n",
    "            if total_rows_loaded == index_size:\n",
    "                print(f\"Loaded ALREADY exactly {total_rows_loaded} rows. Stopping.\")\n",
    "                break  # Stop the current \"FOR\" loop AND PROCESS THE CODE AFTER THE IT. \"return\" WOULD Completely exit the function!\n",
    "            \n",
    "            column_name = chunk.columns[1]\n",
    "            \n",
    "            list_of_dicts_from_chunks = [{column_name: row[column_name], \"title\": 'passage '+ str(index), \"embeddings\": dataset[index]} \n",
    "                                                                                                       for index, row in chunk.iterrows()\n",
    "                                        ] \n",
    "            \n",
    "            batch_of_dicts = batch_of_dicts + list_of_dicts_from_chunks\n",
    "            #print(len(batch_of_dicts))         \n",
    "              \n",
    "\n",
    "            if len(batch_of_dicts)> batch_size:\n",
    "                        \n",
    "                print(len(batch_of_dicts)) \n",
    "\n",
    "                # 'S100' is for a fixed-length ASCII bytes USED FOR STRINGS (HERE 100 BYTES)\n",
    "                # ('f4', (chunk_size, 768)) is for  N-D numpy arrays with shape (chunk_size, 768) of float32 ('f4') type\n",
    "                vector_sizes = batch_of_dicts[0][\"embeddings\"].shape[0]\n",
    "                #print(vector_sizes)\n",
    "\n",
    "                dtype = np.dtype([(column_name, 'S' + str(max_len)),  ('title', 'S' + str(30)),  \n",
    "                                                    ('embeddings', ('f4', (1, vector_sizes))) ])\n",
    "\n",
    "                #structured_array = np.array([tuple(d.values()) for d in batch_of_dicts], dtype=dtype)\n",
    "                structured_array = np.array([(d[column_name].encode('utf-8'), d[\"title\"].encode('utf-8'), d[\"embeddings\"]) \n",
    "                                                                                    for d in batch_of_dicts], dtype=dtype)\n",
    "                             \n",
    "\n",
    "                # \"h5py.File(output_path, 'a')\" OPENS A IN  \"append\" mode WITHOUT DELETING EXISTING data OR AUTOMATICALLY CREATE IT \"at output_path\"\n",
    "                # IF IT DOES NOT ALREADY EXIST.                    \n",
    "                with h5py.File(output_path, 'a') as f2:\n",
    "\n",
    "                        # AN HD5 (Hierarchical Data Format version 5) FILE CAN CONTAIN MULTIPLE DATASETS. WE CALL \"dataset\" THE ONE WE ARE CREATING\n",
    "                        # Check if the dataset already exists OTHERWISE creates it with expandable dimensions\n",
    "                        if \"dataset\" not in f2:\n",
    "\n",
    "                            # WHEN INITIALIZING \"dataset\" ALLOW UNLIMITED ROWS VIA \"None\" (HERE EACH ROW WILL BE A CHUNK). \"dataset\" WILL INCREMENTALLY \n",
    "                            # RECEIVE EMBEDDINGS OF DOCUMENTS IN CHUNKS OF NUMPY ARRAYS HAVING EACH THE NUMBER OF \"COLUMNS\" DEFINED BY: \n",
    "                            # batch_of_embeddings[0].shape[1:] WHERE \"batch_of_embeddings[0]\" IS THE FIRST ARRAY IN THE LIST \"batch_of_embeddings\" AND\n",
    "                            # \".shape[1:]\" IS THE SHAPE OF THE THAT ARRAY EXCEPT THE FIRST DIMENSION. THUS IT IS JUST THE NUMBER OF \"COLUMNS\"\n",
    "                            # NOTE: WE SKIPPED GRABBING THE FIRST DIMENSION (# OF ROWS) BECAUSE WE ALREADY ADDRESSED THAT VIA \"NONE\"\n",
    "                                                        \n",
    "                            maxshape = (None,) + structured_array.shape[1:]\n",
    "                            \n",
    "                            # CREATE WITH 0 ROWS AND \"batch_of_embeddings[0].shape[1:]\" NUMBER OF COLUMNS\n",
    "                            dset = f2.create_dataset(\"dataset\", data=structured_array, maxshape=maxshape)\n",
    "\n",
    "                            # Add column names as metadata\n",
    "                            dset.attrs['column_names'] = [column_name, 'title', 'embeddings']\n",
    "                                                  \n",
    "                        else:\n",
    "                            # Reopen the existing dataset if it already exists\n",
    "                            dset = f2[\"dataset\"]\n",
    "                            dset.resize((dset.shape[0] + len(batch_of_dicts)), axis=0)\n",
    "                            dset[-len(batch_of_dicts):] = structured_array\n",
    "\n",
    "\n",
    "                # Clear the batch and row numbers after writing to the file         \n",
    "                list_of_dicts_from_chunks.clear()\n",
    "                batch_of_dicts.clear()    \n",
    "                del structured_array \n",
    "                gc.collect()   \n",
    "\n",
    "            total_rows_loaded += len(chunk)\n",
    "\n",
    "    if len(batch_of_dicts)>0: \n",
    "                            \n",
    "        print(len(batch_of_dicts))       \n",
    "\n",
    "        # 'S100' is for a fixed-length ASCII bytes USED FOR STRINGS (HERE 100 BYTES)\n",
    "        # ('f4', (chunk_size, 768)) is for  N-D numpy arrays with shape (chunk_size, 768) of float32 ('f4') type\n",
    "        vector_sizes = batch_of_dicts[0][\"embeddings\"].shape[0]\n",
    "        #print(vector_sizes)\n",
    "\n",
    "        dtype = np.dtype([(column_name, 'S' + str(max_len)),  ('title', 'S' + str(30)),  ('embeddings', ('f4', (1, vector_sizes))) ])\n",
    "        \n",
    "        #structured_array = np.array([tuple(d.values()) for d in batch_of_dicts], dtype=dtype)\n",
    "        structured_array = np.array([(d[column_name].encode('utf-8'), d[\"title\"].encode('utf-8'), d[\"embeddings\"]) \n",
    "                                                                                                                      for d in batch_of_dicts], dtype=dtype)\n",
    "\n",
    "        # \"h5py.File(output_path, 'a')\" OPENS A IN  \"append\" mode WITHOUT DELETING EXISTING data OR AUTOMATICALLY CREATE IT \"at output_path\"\n",
    "        # IF IT DOES NOT ALREADY EXIST.                    \n",
    "        with h5py.File(output_path, 'a') as f2:\n",
    "\n",
    "                # AN HD5 (Hierarchical Data Format version 5) FILE CAN CONTAIN MULTIPLE DATASETS. WE CALL \"dataset\" THE ONE WE ARE CREATING\n",
    "                # Check if the dataset already exists OTHERWISE creates it with expandable dimensions\n",
    "                if \"dataset\" not in f2:\n",
    "\n",
    "                    # WHEN INITIALIZING \"dataset\" ALLOW UNLIMITED ROWS VIA \"None\" (HERE EACH ROW WILL BE A CHUNK). \"dataset\" WILL INCREMENTALLY \n",
    "                    # RECEIVE EMBEDDINGS OF DOCUMENTS IN CHUNKS OF NUMPY ARRAYS HAVING EACH THE NUMBER OF \"COLUMNS\" DEFINED BY: \n",
    "                    # batch_of_embeddings[0].shape[1:] WHERE \"batch_of_embeddings[0]\" IS THE FIRST ARRAY IN THE LIST \"batch_of_embeddings\" AND\n",
    "                    # \".shape[1:]\" IS THE SHAPE OF THE THAT ARRAY EXCEPT THE FIRST DIMENSION. THUS IT IS JUST THE NUMBER OF \"COLUMNS\"\n",
    "                    # NOTE: WE SKIPPED GRABBING THE FIRST DIMENSION (# OF ROWS) BECAUSE WE ALREADY ADDRESSED THAT VIA \"NONE\"\n",
    "                                                \n",
    "                    maxshape = (None,) + structured_array.shape[1:]\n",
    "                    \n",
    "                    # CREATE WITH 0 ROWS AND \"batch_of_embeddings[0].shape[1:]\" NUMBER OF COLUMNS\n",
    "                    dset = f2.create_dataset(\"dataset\", data=structured_array, maxshape=maxshape)\n",
    "\n",
    "                    # Add column names as metadata\n",
    "                    dset.attrs['column_names'] = [column_name, 'title', 'embeddings']\n",
    "                                            \n",
    "                else:\n",
    "                    # Reopen the existing dataset if it already exists\n",
    "                    dset = f2[\"dataset\"]\n",
    "                    dset.resize((dset.shape[0] + len(batch_of_dicts)), axis=0)\n",
    "                    dset[-len(batch_of_dicts):] = structured_array\n",
    "\n",
    "\n",
    "        # Clear the batch and row numbers after writing to the file         \n",
    "        list_of_dicts_from_chunks.clear()\n",
    "        batch_of_dicts.clear()    \n",
    "        del structured_array \n",
    "        gc.collect()          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "548600\n",
      "548600\n",
      "548600\n",
      "Loaded ALREADY exactly 1755520 rows. Stopping.\n",
      "109720\n",
      "CPU times: user 2min 1s, sys: 11.8 s, total: 2min 13s\n",
      "Wall time: 5min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "output_path_for_text_passage_embeddings = os.path.join(datasets_storage_on_disk, \"data\",\"text_passage_embeddings.h5\")\n",
    "chunk_size_for_text_passages_with_embeddings = 109720\n",
    "batch_size_for_text_passages_with_embeddings = chunk_size_for_text_passages_with_embeddings*4\n",
    "\n",
    "output_path_for_text_passage_with_embeddings_added = os.path.join(datasets_storage_on_disk, \"data\",\\\n",
    "                                                                  \"text_passages_with_embeddings_added.h5\")\n",
    "\n",
    "faiss_index_size = index.ntotal\n",
    "\n",
    "create_file_containing_text_passages_with_embeddings(tsv_file_path1, output_path_for_text_passage_embeddings, \n",
    "                                                       chunk_size=chunk_size_for_text_passages_with_embeddings,\n",
    "                                                       batch_size=batch_size_for_text_passages_with_embeddings,\n",
    "                                                       max_len = passages_max_len,\n",
    "                                                       output_path=output_path_for_text_passage_with_embeddings_added,\n",
    "                                                       index_size = faiss_index_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset : (1755520,)\n",
      "Dataset shape: (1755520,)\n",
      "<class 'numpy.void'> (b'customers to purchase something immediately, and sharing third - party ads. email marketing has evolved rapidly alongside the technological growth of the 21st century. prior to this growth, when emails were novelties to the majority of customers, email marketing was not as effective. in 1978, gary thuerk of digital equipment corporation ( dec ) sent out the first mass email to approximately 400 potential clients via the advanced research projects agency network ( arpanet ). this email resulted in $ 13 million worth of sales in dec products, and highlighted the potential of marketing through mass emails. however, as email marketing developed as an effective means of direct communication, users began blocking out content from emails with filters and blocking programs. in order to effectively communicate a message through email, marketers had to develop a way of pushing content through to the end user, without being cut out by automatic filters and spam removing software. this resulted in the birth of triggered marketing emails, which are sent to specific users based on their tracked online browsing patterns. historically, it has been difficult to measure the effectiveness of marketing campaigns because target markets can not be adequately defined. email marketing carries the benefit of allowing marketers to identify returns on investment and measure and improve efficiency. email marketing allows marketers to see feedback from users in', b'passage 1', [[ 5.61667025e-01,  6.44075051e-02,  3.07290107e-01, -2.06588447e-01, -9.25062224e-02,  3.27800333e-01, -2.14974061e-01,  3.98564786e-01, -1.07005037e-01, -2.94385731e-01,  1.19796574e-01,  7.18462527e-01, -1.58968285e-01,  2.04030231e-01, -1.19624257e-01, -6.37844950e-02,  1.25973269e-01,  3.49624634e-01, -2.63424188e-01, -2.06873551e-01, -1.07330143e+00, -1.63872883e-01, -7.51903504e-02, -4.32794951e-02,  8.31588924e-01,  1.69308543e-01,  3.08310151e-01, -3.37249815e-01, -3.40050936e-01,  1.52968973e-01,  3.99264991e-01,  3.07012707e-01,  1.51274666e-01,  2.71436665e-02, -5.10536015e-01, -1.13996642e-03, -5.10030352e-02, -1.99076131e-01, -3.91668648e-01, -2.50621110e-01, -6.88113630e-01,  2.26493254e-01,  8.49041939e-02,  1.22961164e-01, -2.31405497e-01, -2.95373797e-01, -2.02089143e+00,  1.64125904e-01, -5.10033309e-01,  8.93813372e-02, -3.78476351e-01,  2.89369404e-01,  5.62939327e-03,  3.41786683e-01, -2.56446809e-01,  4.54720467e-01, -3.57837051e-01, -8.50805268e-02, -4.28439677e-01, -2.41196051e-01,  6.10643327e-01, -1.27152383e-01,  3.15129548e-01,  1.60417080e-01, -2.40670845e-01,  2.96033025e-01, -9.44901481e-02,  4.62844104e-01, -3.96515429e-02, -5.91525197e-01, -5.11011556e-02, -9.75097954e-01, -3.81121971e-02, -1.82011440e-01, -4.70101446e-01,  2.50415832e-01,  8.66224151e-03, -3.41938250e-03,  1.90587193e-01, -3.76481444e-01, -4.84603018e-01,  4.78820384e-01,  1.85804799e-01, -2.11201340e-01,  3.29149365e-01, -1.22498199e-02, -5.22388779e-02,  6.73205376e-01,  1.56030238e-01,  5.87950468e-01,  2.83824205e-01, -6.35365963e-01,  7.01766133e-01, -4.75976437e-01,  2.14613736e-01, -6.17903590e-01,  1.98689193e-01,  1.94480002e-01,  4.37056690e-01, -6.66513741e-02, -1.56337157e-01, -1.17506817e-01,  3.30011278e-01, -2.68866092e-01,  9.37390774e-02,  1.39172465e-01,  7.23958760e-02, -1.07396793e+00,  1.23134479e-01,  1.99194804e-01, -1.44724086e-01, -2.18115494e-01,  1.25027262e-02,  1.93581268e-01, -3.11796933e-01,  3.26845527e-01, -1.95438683e-01, -2.44801238e-01, -5.52471615e-02,  1.49268955e-01,  3.97280633e-01,  5.61649859e-01,  4.86643985e-02, -2.47871354e-01,  2.00733840e-01, -1.51448548e-01,  5.06886065e-01,  8.64865780e-01, -9.60628331e-01,  5.62732257e-02,  1.05117992e-01,  6.77686691e-01,  1.80785865e-01,  2.35516399e-01,  1.45678818e-01, -1.50253832e-01,  4.38872486e-01, -2.50716090e-01,  8.19744691e-02, -6.34395242e-01, -1.65346771e-01, -2.12247133e-01, -2.98350364e-01, -5.42279363e-01,  3.10888618e-01, -1.75210804e-01,  6.29646838e-01, -5.61761677e-01, -7.20531464e-01, -4.96373624e-01, -1.81245774e-01, -4.65095699e-01, -4.87492010e-02,  2.14809045e-01,  1.39047101e-01,  4.81597096e-01,  2.59644181e-01, -2.60676712e-01, -1.83396444e-01,  8.24752375e-02,  5.02351701e-01,  1.86875224e-01, -3.34081262e-01, -1.82847410e-01,  2.74089836e-02, -2.56653100e-01, -2.06410483e-01,  6.34670198e-01,  2.80576378e-01, -9.04912949e-02, -1.86990932e-01,  3.29702020e-01, -2.28306770e-01, -2.25690052e-01,  2.28746474e-01,  5.42237520e-01,  5.95385432e-01,  2.43603289e-01,  2.85185844e-01,  3.74669880e-01, -6.68729901e-01,  5.00927150e-01,  4.38046753e-02, -5.09650350e-01,  2.78020531e-01, -3.60178873e-02,  5.67504764e-01,  6.46052361e-02, -6.24054186e-02,  6.05976784e-05, -3.70341003e-01, -5.85690498e-01, -5.70303239e-02, -6.45233512e-01,  1.36229491e+00, -2.98712879e-01, -1.72702909e-01,  7.61831045e-01, -2.26011942e-03,  1.57081991e-01, -2.18578696e-01,  4.95540828e-01,  3.99167150e-01, -7.50925660e-01, -6.84259534e-01, -6.10080242e-01, -7.83540215e-03, -1.96023867e-01,  1.16676606e-01,  4.97605592e-01,  2.37448797e-01, -1.31601244e-01,  2.01224059e-01, -4.57915574e-01, -2.72206999e-02,  7.12099135e-01, -7.18395114e-02,  6.34230971e-01,  9.26841497e-02, -5.07592559e-01,  3.52432847e-01,  1.49030343e-01, -2.99421906e-01,  1.02846682e+00,  1.69101894e-01,  9.33933556e-01,  3.81704301e-01, -1.99537605e-01,  1.89538226e-01, -1.12309173e-01,  6.02518439e-01,  3.28762770e-01,  4.68606293e-01, -6.41059697e-01, -1.17784686e-01,  2.09002987e-01, -1.13480918e-01, -4.87957418e-01, -6.34941101e-01,  1.09806463e-01,  7.22677648e-01,  7.14083388e-02, -4.87076104e-01,  4.19854343e-01, -3.50877166e-01, -2.08145790e-02, -4.63188201e-01,  3.29333603e-01,  1.07454069e-01,  6.50240481e-02, -1.00127971e-02,  6.90585732e-01, -4.05773036e-02, -4.81871784e-01,  7.36795515e-02, -1.43531665e-01, -3.42917204e-01,  4.28980559e-01,  4.65786427e-01, -7.25870579e-03,  2.71317422e-01, -2.00955093e-01,  5.41655831e-02,  9.81164157e-01, -1.48601890e-01, -5.52912772e-01, -7.65702546e-01, -1.59429088e-01,  7.93140754e-02,  4.68808264e-01,  3.17982405e-01, -5.48842363e-02, -8.40841174e-01,  2.72939086e-01,  1.27730563e-01, -8.32113326e-01,  4.15651590e-01,  5.21026373e-01,  1.15161628e-01, -5.29778719e-01, -9.82164517e-02,  6.71795011e-01,  7.11699605e-01, -7.67231405e-01, -1.64115146e-01, -6.19657457e-01,  6.40356466e-02, -3.25453192e-01, -3.30474764e-01,  4.66339737e-01, -5.27036414e-02,  6.28066361e-01, -2.61977613e-01, -2.87397206e-01,  2.93301851e-01,  3.79877388e-01,  8.20407942e-02,  5.15418828e-01,  4.00991887e-01, -8.76923576e-02,  5.47877233e-03, -2.04833731e-01, -3.01365942e-01, -9.20697078e-02,  1.61851924e-02, -5.06500304e-01,  4.78439266e-03, -1.37411147e-01, -5.86120939e+00, -2.41285115e-01,  1.94594771e-01, -3.93261433e-01, -6.69585168e-02, -5.46301119e-02,  9.78200138e-01, -4.34863120e-01,  4.30367768e-01,  2.75316864e-01, -4.38602805e-01, -1.03787541e-01, -2.75727827e-03, -1.97259426e-01, -8.73926580e-02,  8.25911462e-01,  5.16192138e-01,  1.23026118e-01,  2.52790987e-01,  8.95570070e-02, -2.03062490e-01, -6.41571105e-01,  5.09656429e-01,  1.19746447e+00, -6.88332841e-02,  6.27567470e-02, -8.07062984e-01,  1.84844360e-01,  3.05298686e-01,  3.02435070e-01,  4.58713531e-01,  1.71961803e-02, -1.02278873e-01,  3.46675366e-01,  2.56761163e-01, -3.02055571e-02,  7.65469253e-01, -3.52409303e-01, -1.55484706e-01, -4.60995466e-01,  5.51712990e-01,  7.49941051e-01, -1.30525585e-02, -9.35834181e-03,  4.19621468e-01, -5.42454541e-01, -1.73063964e-01, -2.05676302e-01, -5.71458578e-01,  5.24301410e-01,  3.32707286e-01, -2.55192131e-01,  3.01012158e-01, -1.92246199e-01,  4.50206310e-01, -2.41612047e-01, -2.70942003e-01, -1.10619469e-02, -8.04448798e-02,  3.74005854e-01, -4.58236039e-02, -1.55031785e-01, -2.80891627e-01, -3.84755194e-01, -3.41700852e-01,  2.69182861e-01, -7.66255736e-01, -6.49090409e-01,  5.26971459e-01,  3.22183490e-01, -2.14682698e-01,  6.81652963e-01,  1.33431807e-01, -5.04951835e-01,  1.14054896e-01, -7.11084127e-01,  2.60762930e-01,  2.45909959e-01, -4.80824225e-02,  2.11357936e-01,  4.14729476e-01,  1.51215523e-01, -8.59109014e-02, -1.36210814e-01,  1.31507754e-01, -5.21348357e-01, -6.73504099e-02,  8.46346840e-02, -2.32304722e-01,  2.88114306e-02,  8.19483399e-01, -5.18077351e-02, -1.27095506e-01,  1.15343310e-01, -5.38255554e-04,  1.01517357e-01,  3.75827104e-01,  2.97544301e-01, -4.44095284e-02, -4.99316394e-01,  6.55171990e-01,  5.00423908e-01, -4.46848750e-01, -9.68221188e-01, -1.34241864e-01, -4.81326543e-02, -6.02053583e-01, -1.66787937e-01,  7.16727912e-01, -3.71424258e-02, -2.15562463e-01, -3.79464626e-02, -9.05997753e-01, -1.43817484e-01, -4.16058749e-01,  2.25332499e-01, -7.01078773e-01, -8.60405434e-03,  2.57648975e-01, -1.91147611e-01,  4.07057405e-01,  1.02156840e-01, -7.97844231e-01,  3.27872932e-02,  3.01704466e-01, -4.04652029e-01,  2.09281698e-01,  1.44547462e-01, -1.91405922e-01, -1.11718252e-01,  7.17049599e-01, -3.48814204e-02, -6.40872896e-01, -4.89815384e-01,  6.53743625e-01,  1.91060945e-01, -1.25123546e-01, -4.23849076e-01, -1.37006313e-01, -2.51442611e-01, -1.60688385e-01,  1.92997769e-01,  2.33762279e-01,  3.33197027e-01,  6.57238007e-01, -1.70859724e-01, -4.55469102e-01, -1.79177135e-01,  3.45620304e-01, -2.34415010e-01, -1.78462163e-01, -3.83455575e-01, -2.88316347e-02,  2.25760117e-01, -1.33415326e-01, -2.41234124e-01,  1.07793599e-01, -1.03205249e-01, -5.74700713e-01, -6.21791303e-01,  1.75372660e-01, -2.52303958e-01, -7.04077125e-01,  2.13167131e-01,  1.65974244e-01, -1.73784539e-01, -7.77392864e-01, -1.00813612e-01,  2.10121185e-01,  2.15641633e-01,  7.63157085e-02, -6.40983462e-01, -4.92647290e-01, -3.41735989e-01,  5.49296886e-02, -2.10921466e-03,  3.60261649e-01,  2.52166897e-01,  2.58068014e-02, -1.04208834e-01, -1.22368596e-01, -4.38086122e-01, -5.92168212e-01,  5.26528917e-02,  6.71469450e-01, -2.73554265e-01, -1.63540795e-01,  2.62324601e-01, -8.10044408e-01, -2.69902259e-01, -2.90489763e-01,  2.00681940e-01,  4.68774557e-01,  2.14578733e-01,  6.19419850e-02, -1.01629257e+00, -3.77953112e-01, -6.22968450e-02, -1.52631000e-01, -4.04955894e-01, -3.36231515e-02,  5.93607426e-01, -6.11117303e-01, -3.68067414e-01,  1.13146314e-02, -1.62979543e-01, -1.37266576e-01,  1.35533392e-01, -1.40258014e-01,  1.76397815e-01, -6.13040626e-02, -1.76006123e-01, -2.80761302e-01,  5.29593468e-01, -3.09950292e-01,  3.10482740e-01, -2.73700595e-01, -3.27643663e-01, -8.29592720e-02,  1.94671452e-01,  9.14675742e-02, -1.84641376e-01, -1.03097582e+00,  1.53371185e-01, -2.02170566e-01,  5.60706675e-01,  5.08790255e-01, -8.97570729e-01, -3.61616939e-01, -1.62933305e-01, -7.78423101e-02,  4.88784723e-02,  3.48910242e-02,  3.74032080e-01,  7.66749009e-02,  6.83923662e-02,  4.12913650e-01,  5.55090368e-01, -2.16896579e-01,  4.90763098e-01, -4.61871058e-01,  6.12374127e-01, -1.23785257e-01, -8.24964023e-04,  3.29603493e-01,  5.26409388e-01,  3.86361092e-01, -2.77231693e-01, -6.92126393e-01,  3.09209794e-01, -7.07369894e-02,  7.31737912e-02, -1.48131341e-01,  3.43695819e-01, -1.53184563e-01, -1.29230380e-01, -6.95900679e-01,  1.15518600e-01, -3.68363023e-01,  1.58256516e-01, -4.31022719e-02,  6.97085917e-01, -5.36724269e-01, -7.02324063e-02,  7.42306352e-01,  8.05095494e-01, -3.41559142e-01, -3.46967489e-01, -3.38158458e-01, -1.47486731e-01,  3.98432583e-01, -2.59968102e-01,  3.05247784e-01,  9.55432534e-01,  6.68606758e-01,  3.04098219e-01, -2.51953840e-01, -9.35042873e-02, -4.07236695e-01,  1.67045087e-01, -1.15964793e-01, -7.10283458e-01,  8.77162674e-04, -2.58216351e-01,  6.27384126e-01, -6.50657415e-01, -1.39607638e-01, -2.44889647e-01, -1.12875521e-01, -5.36636531e-01, -1.99272022e-01,  4.35997725e-01, -2.79088736e-01, -7.25154877e-02, -4.81282622e-01,  1.31289840e-01,  4.23325062e-01, -2.97193557e-01, -6.83948815e-01, -5.95169216e-02,  3.18812072e-01, -4.65523489e-02,  6.52594149e-01, -4.94750142e-01,  3.45777214e-01,  5.40313065e-01, -6.24193102e-02, -1.87181920e-01,  1.54074458e-02, -1.65397897e-01, -4.65216972e-02, -4.95360851e-01, -1.15331769e-01,  2.28024453e-01, -9.63673413e-01,  4.79044579e-02,  1.88633367e-01, -3.44868779e-01,  3.61643225e-01, -8.94330791e-04,  1.98243335e-01,  3.46323162e-01,  2.75099695e-01,  4.11969610e-02, -1.02755651e-01,  1.77539632e-01, -3.86261642e-01,  5.70231676e-01,  1.92228016e-02, -1.18368112e-01,  2.17084009e-02,  6.58426106e-01, -2.87698179e-01,  2.43969023e-01, -6.63066457e-04, -2.22946227e-01,  2.96205878e-01, -7.02405930e-01, -4.10899788e-01,  2.60932058e-01,  7.04721510e-01,  3.80352229e-01, -2.66766432e-03,  1.74560549e-03, -1.84670150e-01,  3.30379963e-01,  6.44301713e-01,  3.75715286e-01,  1.95630074e-01,  5.61355948e-01,  1.31447569e-01,  6.67641684e-02,  2.73766041e-01,  8.86195958e-01, -2.52498150e-01, -7.13802040e-01,  1.65630370e-01, -6.53857440e-02, -1.48959100e-01, -2.80942291e-01, -2.52087593e-01,  5.74234664e-01,  2.12723166e-01, -6.37051314e-02,  2.52711296e-01,  5.30993938e-01,  1.86050609e-01, -1.61651373e-01,  1.61232591e-01,  3.04057837e-01, -1.55102268e-01,  6.88736737e-02,  2.13923246e-01,  2.16421381e-01, -4.82435152e-02, -6.46047667e-02,  5.92581987e-01,  4.26375657e-01, -1.44206718e-01,  1.46857455e-01, -5.56746900e-01,  1.01144426e-01,  4.99907732e-02, -2.38368288e-01, -5.26314825e-02,  2.66001523e-01, -5.86118460e-01,  4.24160548e-02, -1.41780362e-01, -4.39593792e-01, -4.12958056e-01, -6.48237288e-01, -3.47878374e-02,  3.36115509e-01,  9.25822631e-02,  4.01846766e-01,  7.13992566e-02,  7.83162475e-01,  1.55641630e-01,  1.12942271e-02, -2.92989880e-01,  2.64181376e-01, -9.31593627e-02,  1.10563129e-01,  3.80550861e-01,  5.73308289e-01, -7.84626901e-01, -1.12281375e-01, -2.66528904e-01,  3.81361127e-01,  2.46110886e-01,  1.05758429e-01, -1.19164787e-01, -2.39942387e-01,  1.22041710e-01, -9.80093926e-02,  6.11980319e-01, -4.20067720e-02,  2.80636009e-02, -3.21010232e-01, -1.16962343e-01,  2.95127869e-01,  1.91562735e-02,  4.88771535e-02,  2.69541681e-01,  1.99373081e-01,  8.77454132e-02,  2.85451174e-01, -2.62560993e-01,  4.77987498e-01,  6.49037957e-02, -9.29141492e-02, -2.87428468e-01,  2.48804063e-01, -1.77163836e-02, -2.13844344e-01,  4.46210980e-01,  1.83190733e-01,  2.43412584e-01, -2.80407846e-01,  3.77449393e-01, -3.76906753e-01,  1.84785351e-02,  1.18088894e-01, -4.56790835e-01,  1.13850795e-01,  3.82113844e-01, -4.14256006e-01, -9.81964111e-01,  3.68006945e-01, -3.66738141e-01, -8.18877816e-01, -2.37604961e-01,  1.65060833e-01, -4.85497564e-01,  4.07179058e-01,  1.25051960e-01, -2.55938083e-01, -3.65038902e-01,  9.34359618e-03]])\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(output_path_for_text_passage_with_embeddings_added, 'r') as f:\n",
    "    for key in f.keys():\n",
    "        dataset = f[key]\n",
    "        print(f\"{key} : {dataset.shape}\")\n",
    "\n",
    "        # Print the shape of the dataset\n",
    "        print(\"Dataset shape:\", dataset.shape)\n",
    "\n",
    "        # Read and print the first 2 rows\n",
    "        first_two_rows = dataset[:2]  # Slices the first two rows\n",
    "        print(type(first_two_rows[1]), (first_two_rows[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_1_6_2_'></a>[III- 11.2 SPLIT INTO CHUNKS OF APACHE \"ARROW\" FORMAT FILES THE TEXT PASSAGES AND THEIR EMBEDDINGS](#toc0_)\n",
    "\n",
    "\n",
    "\n",
    "* THE Hugging Face's \"DATASET\" LIBRARY USES THE \".save_to_disk()\" METHOD TO STORE IN-MEMORY DATASETS TO ARROW FORMAT\n",
    "\n",
    "* It is crucial efficient retrieval, fast querying, and scalability because Apache Arrow is columnar and memory-mapped. Only the needed columns (e.g., embeddings) are loaded into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def break_dataset_into_chunks_in_arrow_format(hdf5_file_path, chunk_size, batch_size_chunk, output_path_for_dataset_chunks):\n",
    "\n",
    "    with h5py.File(hdf5_file_path, 'r') as f:\n",
    "        \n",
    "        \n",
    "        # Get the dtype of the dataset\n",
    "        dtype =  f[\"dataset\"].dtype          \n",
    "\n",
    "        total_size = f[\"dataset\"].shape[0]  \n",
    "        print(total_size)  \n",
    "\n",
    "\n",
    "        list_of_dicts_from_chunk = [] \n",
    "\n",
    "        chunk_num = 0\n",
    "        \n",
    "        for start in range(0, total_size, chunk_size):\n",
    "                              \n",
    "               end = min(start + chunk_size, total_size)\n",
    "               \n",
    "               # Read a batch of data\n",
    "               data_chunk = f[\"dataset\"][start:end]\n",
    "\n",
    "               chunk_num +=1\n",
    "               #print(chunk_num)\n",
    "            \n",
    "               \n",
    "               # Convert the batch to a list of dictionaries\n",
    "               #list_chunk = [{\"data\": item} for item in data_chunk]\n",
    "               for item in data_chunk:\n",
    "                    \n",
    "                    text = item[dtype.names[0]].decode('utf-8')\n",
    "                    #print(text)\n",
    "                    title = item[dtype.names[1]].decode('utf-8')\n",
    "                    #print(title)\n",
    "                    embeddings = item[dtype.names[2]]\n",
    "                    #print(embeddings)\n",
    "\n",
    "                    row_into_dict = {\"text\":text, \"title\":title, \"embeddings\":embeddings}\n",
    "\n",
    "                    list_of_dicts_from_chunk.append(row_into_dict)\n",
    "                    #print(\"length of list_of_dicts_from_chunk:\", len(list_of_dicts_from_chunk))\n",
    "                    \n",
    "\n",
    "               if len(list_of_dicts_from_chunk) >= batch_size_chunk:\n",
    "                    \n",
    "                    print(len(list_of_dicts_from_chunk))\n",
    "                    \n",
    "                    # Create a Dataset from the chunk\n",
    "                    chunk_dataset = Dataset.from_list(list_of_dicts_from_chunk)\n",
    "                         \n",
    "                    # Convert the Dataset to a pandas DataFrame by using the 'to_pandas()' method\n",
    "                    #chunk_df = chunk_dataset.to_pandas() \n",
    "\n",
    "                    # SAVE AS PARQUET FILE\n",
    "                    #chunk_df.to_parquet( os.path.join(output_path_for_dataset_chunks, f'chunk_{chunk_num}.parquet'), index=False) \n",
    "                    chunk_dataset.save_to_disk( os.path.join(output_path_for_dataset_chunks, f'chunk_{chunk_num}'))\n",
    "                    #chunk_dataset.save_to_disk( output_path_for_dataset_chunks)   \n",
    "                    \n",
    "                    list_of_dicts_from_chunk.clear()\n",
    "                    chunk_dataset = None\n",
    "                    chunk_df = None\n",
    "                    del chunk_dataset, chunk_df\n",
    "                    gc.collect\n",
    "               \n",
    "        if len(list_of_dicts_from_chunk) >0:\n",
    "                    \n",
    "                                        \n",
    "                    print(len(list_of_dicts_from_chunk))\n",
    "                    \n",
    "                    # Create a Dataset from the chunk\n",
    "                    chunk_dataset = Dataset.from_list(list_of_dicts_from_chunk)\n",
    "                         \n",
    "                    # Convert the Dataset to a pandas DataFrame by using the 'to_pandas()' method\n",
    "                    #chunk_df = chunk_dataset.to_pandas() \n",
    "\n",
    "                    # SAVE AS PARQUET FILE\n",
    "                    #chunk_df.to_parquet( os.path.join(output_path_for_dataset_chunks, f'chunk_{chunk_num}.parquet'), index=False)  \n",
    "                    chunk_dataset.save_to_disk( os.path.join(output_path_for_dataset_chunks, f'chunk_{chunk_num}'))  \n",
    "                    #chunk_dataset.save_to_disk( output_path_for_dataset_chunks)  \n",
    "\n",
    "                    \n",
    "               \n",
    "        list_of_dicts_from_chunk.clear()\n",
    "        chunk_dataset = None\n",
    "        chunk_df = None\n",
    "        del chunk_dataset, chunk_df\n",
    "        gc.collect\n",
    "\n",
    "                              \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1755520\n",
      "135040\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc634decba147d59039314d444ad6ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/135040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135040\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c6f31463ef441da686801077d355ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/135040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135040\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38e68127c25490e93b0509a75616047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/135040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135040\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dca33a2dc644897a9498e272b6ce722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/135040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135040\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b4cb066f9b4e4897d144b043ecd432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/135040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135040\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf7b5ce2be64839b2b14d17bcaf9d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/135040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135040\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885fed3f92eb42cb8076ed68f8d60852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/135040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135040\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dfd6ef8d2c24dea9866eb3eb8b2eaf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/135040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135040\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd9c071798384741af4303c49b143569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/135040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135040\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e615bfcf27384e40b6b2f58990df2794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/135040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135040\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41598476e20f46969c34aa40b0abe213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/135040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135040\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ffdc393c794d8d99e947f7efc21f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/135040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135040\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d445c009f4646caab7b7bcc54befcc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/135040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 19s, sys: 14.6 s, total: 1min 34s\n",
      "Wall time: 2min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "output_folder_for_dataset_chunks = os.path.join(datasets_storage_on_disk,\"data\",\"chunks_of_passages_with_their_embeddings\")\n",
    "\n",
    "break_dataset_into_chunks_in_arrow_format(hdf5_file_path = output_path_for_text_passage_with_embeddings_added, \n",
    "                                          chunk_size=135040,batch_size_chunk=135040, \n",
    "                                        output_path_for_dataset_chunks = output_folder_for_dataset_chunks)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_rows_in_arrow_files(folders):\n",
    "    total_rows = 0\n",
    "    \n",
    "    for folder in folders:\n",
    "        if os.path.exists(folder) and os.path.isdir(folder):\n",
    "\n",
    "            dataset = load_from_disk(folder)  # Load the Arrow file as a dataset \n",
    "            \n",
    "            total_rows += len(dataset)  # Add the number of rows to total_rows\n",
    "        else:\n",
    "            print(f\"The folder {folder} doesn't exist or isn't a directory.\")\n",
    "    \n",
    "    return total_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1755520"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "folders = [ os.path.join(output_folder_for_dataset_chunks, f\"{folder}\") for folder in os.listdir(output_folder_for_dataset_chunks)]\n",
    "\n",
    "number_of_chunks_created = len(folders)                   \n",
    "\n",
    "count_rows_in_arrow_files(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_arrow_files(total_chunks, chunk_folder, output_folder, batch_size=30):\n",
    "    \"\"\"\n",
    "    Efficiently concatenates large dataset chunks to avoid memory overflow.\n",
    "    \n",
    "    - `batch_size`: Controls how many chunks are loaded at a time.\n",
    "    - Saves intermediate results to disk to prevent excessive memory usage.\n",
    "    \"\"\"\n",
    "    \n",
    "    temp_output_base = os.path.join(output_folder, \"temp\")  # Base temp folder\n",
    "    os.makedirs(temp_output_base, exist_ok=True)\n",
    "\n",
    "    final_dataset = None\n",
    "\n",
    "    for i in range(0, total_chunks, batch_size):  \n",
    "        batch = []\n",
    "        \n",
    "        # Load a batch of chunks\n",
    "        for j in range(i, min(i + batch_size, total_chunks)):\n",
    "            chunk_path = os.path.join(chunk_folder, f\"chunk_{j+1}\")\n",
    "            batch.append(load_from_disk(chunk_path))\n",
    "        \n",
    "        # Merge the batch\n",
    "        batch_dataset = concatenate_datasets(batch)\n",
    "\n",
    "        # Unique temp folder for each batch\n",
    "        temp_output = os.path.join(temp_output_base, f\"batch_{i}\")\n",
    "        os.makedirs(temp_output, exist_ok=True)\n",
    "\n",
    "        # If first batch, initialize final_dataset\n",
    "        if final_dataset is None:\n",
    "            final_dataset = batch_dataset\n",
    "        else:\n",
    "            final_dataset = concatenate_datasets([final_dataset, batch_dataset]) \n",
    "\n",
    "        # Save intermediate dataset with unique path & delete to free memory\n",
    "        final_dataset.save_to_disk(temp_output)\n",
    "        del final_dataset  \n",
    "        gc.collect()  # Force garbage collection\n",
    "        \n",
    "        # Reload from disk for next iteration\n",
    "        final_dataset = load_from_disk(temp_output)  \n",
    "\n",
    "    # Save final result\n",
    "    final_dataset.save_to_disk(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f6f25b231c4669a1ddfca059385177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/12 shards):   0%|          | 0/1350400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55a4047b29a47578061530b20e14486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/16 shards):   0%|          | 0/1755520 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22691a468daa416a8e528007d9ee3a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/16 shards):   0%|          | 0/1755520 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the final dataset storage path\n",
    "output_folder_for_dataset = os.path.join(datasets_storage_on_disk,\"data\",\"passages_final_dataset\")\n",
    "\n",
    "\n",
    "concatenate_arrow_files(total_chunks = number_of_chunks_created,\n",
    "                        chunk_folder = output_folder_for_dataset_chunks,\n",
    "                        output_folder = output_folder_for_dataset,\n",
    "                        batch_size=10\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1755520"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_rows_in_arrow_files([output_folder_for_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO MAKE A SINGLE \"BIG\"APACHE ARROW FILE FROM ALL THE CHUNKS ARROW FILES\n",
    "\n",
    "# from datasets import load_from_disk, concatenate_datasets\n",
    "# chunk_dirs = [os.path.join(datasets_storage_on_disk, \"data\", \"passages\", f\"chunk_{i+1}\")  for i in range(number_of_chunks_created)]\n",
    "# datasets = [load_from_disk(chunk_dir) for chunk_dir in chunk_dirs]\n",
    "# # Concatenate all chunks into a single dataset\n",
    "# combined_dataset = concatenate_datasets(datasets)\n",
    "\n",
    "# combined_dataset.save_to_disk(output_folder_for_dataset_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT NEEDED: CONVERT FILE WITH PASSAGES TO \"PICKLE\" FORMAT!!!!!!!!\n",
    "#pkl_file_path = os.path.join(datasets_storage_on_disk, \"data\",\"passages.pkl\")\n",
    "# Save the list of dictionaries to a .pkl file\n",
    "#with open(pkl_file_path, 'wb') as f:\n",
    "#    pickle.dump(passages_as_list_of_dict, f)\n",
    "\n",
    "# Check if the file exists\n",
    "#if not os.path.exists(pkl_file_path):\n",
    "#    raise FileNotFoundError(f\"The file {pkl_file_path} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[IV- \"FACEBOOK\" RAG \"MODEL\"](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_1_'></a>[IV- 1. THE RAG \"RETRIEVER\"](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "#torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of similar documents retrieved is determined by the behavior of \"RagRetriever\" AND By default RAG retrieves 5 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever_for_similar_passages(question, number_docs_retrieved, path_to_chunks_in_arrow_format, path_to_indeces): \n",
    "\n",
    "    \n",
    "    rag_pretrained_model = \"facebook/rag-sequence-nq\"\n",
    "\n",
    "    # INSTANTIATE THE \"RAG\" TOKENIZER\n",
    "    tokenizer_for_retriever = RagTokenizer.from_pretrained(rag_pretrained_model) \n",
    "\n",
    "   # Tokenize the QUESTION\n",
    "    input_ids = tokenizer_for_retriever(question, return_tensors=\"pt\").input_ids\n",
    "\n",
    "\n",
    "    device = torch.device('cuda:0')\n",
    "\n",
    "    # INSTANTIATE THE \"ENCODER\" FOR THE QUESTION AND MOVE IT TO GPU\n",
    "    model_result_generation = RagTokenForGeneration.from_pretrained(rag_pretrained_model).to(device)   \n",
    "\n",
    " \n",
    "    # Get hidden states for the question USING \"question_encoder\" \n",
    "    # ALSO Disable gradient calculation for inference\n",
    "    with torch.no_grad():  \n",
    "        question_hidden_states = model_result_generation.question_encoder(input_ids.to(device))[0] \n",
    "\n",
    "\n",
    "    # Convert TENSOR to NumPy array BY MOVING IT to CPU and convert to NumPy array (cpu FRAMEWORK)\n",
    "    question_hidden_states_np = question_hidden_states.detach().cpu().numpy()  \n",
    "\n",
    "  \n",
    "    # INSTANTIATE THE RAG \"RETRIEVER\" COMPONENT\n",
    "    retriever = RagRetriever.from_pretrained(rag_pretrained_model,\n",
    "                                            index_name=\"custom\", output_retrieved=False,                                            \n",
    "                                            tokenizer = tokenizer_for_retriever,\n",
    "                                            n_docs= number_docs_retrieved,\n",
    "                                            passages_path= path_to_chunks_in_arrow_format, \n",
    "                                            index_path=path_to_indeces\n",
    "                                            )\n",
    "\n",
    "\n",
    "\n",
    "    #  Use the retriever to retrieve relevant documents\n",
    "    docs_dict = retriever(input_ids, question_hidden_states=question_hidden_states_np, return_tensors=\"pt\")\n",
    "\n",
    "    # RETRIEVE EMBEDDINGS OF DOCUMENTS (Assuming you have retrieved embeddings for the documents)\n",
    "    retrieved_doc_embeds = docs_dict['retrieved_doc_embeds']  # Shape (n_docs, embed_dim)\n",
    "\n",
    "    # GET CONTEXT INPUT IDs\n",
    "    n_docs =docs_dict['context_input_ids'].shape[0]\n",
    "    batch_size = input_ids.shape[0]\n",
    "    context_input_ids = docs_dict['context_input_ids'].reshape(batch_size * n_docs, -1)\n",
    "\n",
    "\n",
    "    if 'context_attention_mask' not in docs_dict:\n",
    "        docs_dict['context_attention_mask'] = torch.ones_like(context_input_ids)\n",
    "    #print(docs_dict['context_attention_mask'].shape)\n",
    "\n",
    "\n",
    "    return question_hidden_states, retrieved_doc_embeds, model_result_generation, input_ids, \\\n",
    "            context_input_ids, docs_dict, tokenizer_for_retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save_faiss_indeces= os.path.join(datasets_storage_on_disk, \"data\",\"disk_index.faiss\")\n",
    "output_folder_for_dataset = os.path.join(datasets_storage_on_disk,\"data\",\"passages_final_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
      "The class this function is called from is 'BartTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "# Example question\n",
    "#user_prompt = \"What is machine learning?\"\n",
    "#user_prompt = \"What is the capital France?\"    \n",
    "#user_prompt = \"what is Berlin?\"    \n",
    "user_prompt = \"Who won the world series in 2020?\"\n",
    "number_docs_to_retrieve = 5 \n",
    "\n",
    "question_hidden_states, retrieved_doc_embeds, \\\n",
    "model_result_generation, input_ids, context_input_ids, \\\n",
    "docs_dict, tokenizer_for_retriever = retriever_for_similar_passages( question = user_prompt,\n",
    "                                                                     number_docs_retrieved =  number_docs_to_retrieve,             \n",
    "                                                               path_to_chunks_in_arrow_format = output_folder_for_dataset, \n",
    "                                                               path_to_indeces = path_to_save_faiss_indeces\n",
    "                                                            )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_2_'></a>[IV- 2. THE RAG \"GENERATOR\"](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc2_2_2_1_'></a>[IV- 2.1 RESPONSE BASED ON COSINE SIMILARITY WITH RETRIEVED DOCUMENTS](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cosine_similarity_response():\n",
    "\n",
    "    #device = torch.device('cuda:0')\n",
    "    \n",
    "    # Expand question_hidden_states to [1, 5, 1, 768]\n",
    "    expanded_question_hidden_states = question_hidden_states.unsqueeze(1).unsqueeze(2).expand(-1, retrieved_doc_embeds.size(1), \\\n",
    "                                                                                              retrieved_doc_embeds.size(2), -1)\n",
    "\n",
    "    # Compute cosine similarity along the last dimension\n",
    "    doc_scores_cosine = F.cosine_similarity(expanded_question_hidden_states.to(device), retrieved_doc_embeds.to(device), dim=-1)\n",
    "\n",
    "    # Squeeze out any unnecessary singleton dimensions for further processing\n",
    "    doc_scores_cosine = doc_scores_cosine.squeeze(-1)  # Shape should now be [1, 5]\n",
    "\n",
    "\n",
    "    # Generate the answer conditioned on retrieved documents\n",
    "    outputs_cosine = model_result_generation.generate(\n",
    "        input_ids=input_ids.to(device),\n",
    "        context_input_ids=context_input_ids.to(device),\n",
    "        doc_scores=doc_scores_cosine,\n",
    "        context_attention_mask=docs_dict['context_attention_mask'].to(device)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Decode the generated output\n",
    "    generated_text_cosine = tokenizer_for_retriever.batch_decode(outputs_cosine, skip_special_tokens=True)[0]\n",
    "    #print(generated_text_cosine)\n",
    "\n",
    "\n",
    "    return generated_text_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n",
      "torch.Size([1, 5, 1, 768])\n",
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "print(question_hidden_states.shape)\n",
    "#retrieved_doc_embeds=retrieved_doc_embeds.squeeze(2)\n",
    "print(retrieved_doc_embeds.shape)  # Should be (batch_size, 60, embedding_dim)\n",
    "expanded_question_hidden_states = question_hidden_states.unsqueeze(1).expand(-1, retrieved_doc_embeds.size(1), -1)\n",
    "\n",
    "print(expanded_question_hidden_states.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the houston astros'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_cosine_similarity_response()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_3_'></a>[IV- 2.2. RESPONSE BASED ON EUCLIDEAN DISTANCE WITH RETRIEVED DOCUMENTS](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_euclidean_similarity_response():\n",
    "        \n",
    "    # Ensure that retrieved_doc_embeds is squeezed along the third dimension (singleton) before cdist\n",
    "    doc_scores_euclidean = torch.cdist(question_hidden_states.unsqueeze(1).to(device), \n",
    "                                       retrieved_doc_embeds.squeeze(2).to(device), p=2)  # Shape: [1, 5]\n",
    "\n",
    "    # Remove the extra singleton dimension if present to get shape [1, 5]\n",
    "    doc_scores_euclidean = doc_scores_euclidean.squeeze(1) \n",
    "\n",
    "\n",
    "\n",
    "    # Generate the answer conditioned on retrieved documents\n",
    "    outputs_euclidean = model_result_generation.generate(\n",
    "                            input_ids=input_ids.to(device), \n",
    "                            context_input_ids=context_input_ids.to(device), \n",
    "                            doc_scores=doc_scores_euclidean,\n",
    "                            context_attention_mask = docs_dict['context_attention_mask'].to(device)\n",
    "                            )\n",
    "\n",
    "\n",
    "    # Decode the generated output\n",
    "    generated_text_euclidean = tokenizer_for_retriever.batch_decode(outputs_euclidean, skip_special_tokens=True)[0]\n",
    "    #print(generated_text_euclidean)\n",
    "\n",
    "    return generated_text_euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the houston astros'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_euclidean_similarity_response()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_2_4_'></a>[IV- 2.3. RESPONSE BASED ON DOT PRODUCT WITH RETRIEVED DOCUMENTS](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dotproduct_similarity_response():\n",
    "\n",
    "    # Ensure the shapes are correct before batch matrix multiplication\n",
    "    question_hidden_states_unsq = question_hidden_states.unsqueeze(1) # Shape: (batch_size, 1, embed_dim)\n",
    "\n",
    "    retrieved_doc_embeds_squeezed = retrieved_doc_embeds.squeeze(2)  # Shape: (batch_size, n_docs, embed_dim)\n",
    "\n",
    "\n",
    "    # Perform batch matrix multiplication\n",
    "    doc_scores_dotprod = torch.bmm(question_hidden_states_unsq.to(device), \n",
    "                                   retrieved_doc_embeds_squeezed.transpose(1, 2).to(device)).squeeze(1)  # Shape: (batch_size, n_docs)\n",
    "\n",
    "\n",
    "\n",
    "    # Generate the answer conditioned on retrieved documents\n",
    "    outputs_dotprod = model_result_generation.generate(\n",
    "                            input_ids=input_ids.to(device), \n",
    "                            context_input_ids=context_input_ids.to(device), \n",
    "                            doc_scores=doc_scores_dotprod,\n",
    "                            context_attention_mask = docs_dict['context_attention_mask'].to(device)\n",
    "                            )\n",
    "\n",
    "\n",
    "    # Decode the generated output\n",
    "    generated_text_dotprod = tokenizer_for_retriever.batch_decode(outputs_dotprod, skip_special_tokens=True)[0]\n",
    "    #print(generated_text_dotprod)\n",
    "\n",
    "    return generated_text_dotprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the houston astros'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_dotproduct_similarity_response()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[X- APPENDIX](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_divisors(n):\n",
    "    divisors = []\n",
    "    for i in range(1, int(n**0.5) + 1):\n",
    "        if n % i == 0:  # Check if i is a divisor\n",
    "            divisors.append(i)\n",
    "            if i != n // i:  # Avoid adding the square root twice if n is a perfect square\n",
    "                divisors.append(n // i)\n",
    "    return sorted(divisors)  # Sort the list of divisors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divisors of 8825328 : [1, 2, 3, 4, 6, 8, 9, 12, 16, 18, 24, 27, 31, 36, 48, 54, 62, 72, 93, 108, 124, 144, 186, 216, 248, 279, 372, 432, 496, 558, 659, 744, 837, 1116, 1318, 1488, 1674, 1977, 2232, 2636, 3348, 3954, 4464, 5272, 5931, 6696, 7908, 10544, 11862, 13392, 15816, 17793, 20429, 23724, 31632, 35586, 40858, 47448, 61287, 71172, 81716, 94896, 122574, 142344, 163432, 183861, 245148, 284688, 326864, 367722, 490296, 551583, 735444, 980592, 1103166, 1470888, 2206332, 2941776, 4412664, 8825328]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "number = data_size\n",
    "divisors_of_number1 = find_divisors(number)\n",
    "print(\"Divisors of\", number, \":\", divisors_of_number1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Divisors of  1755520 :\n",
      " [1, 2, 4, 5, 8, 10, 13, 16, 20, 26, 32, 40, 52, 64, 65, 80, 104, 128, 130, 160, 208, 211, 260, 320, 416, 422, 520, 640, 832, 844, 1040, 1055, 1664, 1688, 2080, 2110, 2743, 3376, 4160, 4220, 5486, 6752, 8320, 8440, 10972, 13504, 13715, 16880, 21944, 27008, 27430, 33760, 43888, 54860, 67520, 87776, 109720, 135040, 175552, 219440, 351104, 438880, 877760, 1755520]\n"
     ]
    }
   ],
   "source": [
    "num_vectors = index.ntotal\n",
    "divisors_of_index_size= find_divisors(num_vectors)\n",
    "print(\"\\n\", \"Divisors of \", num_vectors, \":\\n\", divisors_of_index_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "versatile_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
